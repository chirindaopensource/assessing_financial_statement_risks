{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHvzp8Gj3TIz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `README.md`\n",
        "\n",
        "# Heuristic-Augmented Financial Risk Index (HAFRI): Assessing Financial Statement Risks among MCDM Techniques\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2512.04035v1-b31b1b.svg)](https://arxiv.org/abs/2512.04035v1)\n",
        "[![Journal](https://img.shields.io/badge/Journal-Economics%20(econ.TH)-003366)](https://arxiv.org/abs/2512.04035v1)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/assessing_financial_statement_risks)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Financial%20Risk%20Management%20%7C%20MCDM-00529B)](https://github.com/chirindaopensource/assessing_financial_statement_risks)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-Damascus%20Securities%20Exchange-lightgrey)](http://www.dse.sy/)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-Bloomberg%20Terminal%20(Fundamentals)-lightgrey)](https://www.bloomberg.com/professional/solution/bloomberg-terminal/)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-Company%20Annual%20Reports-lightgrey)](https://github.com/chirindaopensource/assessing_financial_statement_risks)\n",
        "[![Core Method](https://img.shields.io/badge/Method-Analytic%20Hierarchy%20Process%20(AHP)-orange)](https://github.com/chirindaopensource/assessing_financial_statement_risks)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Simple%20Additive%20Weighting%20(SAW)-red)](https://github.com/chirindaopensource/assessing_financial_statement_risks)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![PyYAML](https://img.shields.io/badge/PyYAML-gray?logo=yaml&logoColor=white)](https://pyyaml.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/assessing_financial_statement_risks`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Assessing Financial Statement Risks among MCDM Techniques\"** by:\n",
        "\n",
        "*   Marwa Abdullah\n",
        "*   Revzon Oksana Anatolyevna\n",
        "*   Duaa Abdullah\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous financial statement data validation and expert survey processing to hierarchical weight derivation via AHP, risk scoring via SAW, and comprehensive robustness analysis.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `execute_hafri_master_pipeline`](#key-callable-execute_hafri_master_pipeline)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the analytical framework presented in Abdullah et al. (2025). The core of this repository is the iPython Notebook `assessing_financial_statement_risks_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings. The pipeline is designed to be a generalizable toolkit for constructing a **Heuristic-Augmented Financial Risk Index (HAFRI)**, enabling the ranking of fiscal periods by their aggregate exposure to financial risk.\n",
        "\n",
        "The paper addresses the challenge of quantifying latent financial risk by integrating expert heuristics with objective financial ratios. This codebase operationalizes the paper's framework, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration via a single `study_configuration.yaml` file.\n",
        "-   Process raw expert pairwise comparison surveys to derive consensus weights for risk criteria.\n",
        "-   Implement the **Analytic Hierarchy Process (AHP)** to decompose risk into Capital Structure, Liquidity, Income, and Cash Flow domains.\n",
        "-   Apply **Simple Additive Weighting (SAW)** to aggregate normalized financial ratios into a composite risk score.\n",
        "-   Validate findings against the original study's results for Al-Ahliah Vegetable Oil Company (2008–2017).\n",
        "-   Conduct automated robustness checks to test the sensitivity of risk rankings to methodological assumptions.\n",
        "-   Automatically generate a comprehensive technical report and reproducibility package.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in Multi-Criteria Decision Making (MCDM) theory and financial statement analysis.\n",
        "\n",
        "**1. Analytic Hierarchy Process (AHP):**\n",
        "AHP is used to elicit and synthesize expert judgments into a coherent set of priority weights. The problem is decomposed into a hierarchy:\n",
        "-   **Goal:** Aggregate Financial Risk.\n",
        "-   **Main Criteria:** Capital Structure Risk (CSR), Liquidity Risk (LR), Income Risk (IR), Cash Flow Risk (CFR).\n",
        "-   **Sub-Criteria:** 34 financial ratios derived from financial statements.\n",
        "\n",
        "Weights are derived from pairwise comparison matrices $A$ where $A_{ij}$ represents the relative importance of criterion $i$ over $j$. The principal eigenvector $w$ is approximated using the Row Geometric Mean method, and consistency is validated using the Consistency Ratio ($CR < 0.10$).\n",
        "\n",
        "**2. Simple Additive Weighting (SAW):**\n",
        "SAW aggregates the performance of alternatives (fiscal years) across multiple criteria.\n",
        "-   **Normalization:** Raw ratio values $x_{ij}$ are transformed into commensurable utility scores $r_{ij} \\in [0, 1]$ using Min-Max normalization.\n",
        "    -   **Benefit Criteria (Max):** Higher values reduce risk (e.g., Cash Readiness).\n",
        "        $$ r_{ij} = \\frac{x_j^+ - x_{ij}}{x_j^+ - x_j^-} $$\n",
        "    -   **Cost Criteria (Min):** Higher values increase risk (e.g., Debt/Equity).\n",
        "        $$ r_{ij} = \\frac{x_{ij} - x_j^-}{x_j^+ - x_j^-} $$\n",
        "-   **Aggregation:** The composite risk score $V_i$ is the weighted sum of normalized scores:\n",
        "    $$ V_i = \\sum_{j=1}^{n} w_j r_{ij} $$\n",
        "\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`assessing_financial_statement_risks_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The entire pipeline is broken down into 28 distinct, modular tasks, each with its own orchestrator function.\n",
        "-   **Configuration-Driven Design:** All study parameters are managed in an external `study_configuration.yaml` file.\n",
        "-   **Rigorous Data Validation:** A multi-stage validation process checks the schema, content integrity, and temporal consistency of expert surveys and financial statements.\n",
        "-   **Advanced MCDM Implementation:** Integrates AHP consistency checking, hierarchical weight composition, and SAW normalization with explicit directionality handling.\n",
        "-   **Robustness Verification:** Includes automated sensitivity analysis scenarios (e.g., Strict vs. Relaxed Consistency thresholds) to validate the stability of risk rankings.\n",
        "-   **Reproducible Artifacts:** Generates structured dictionaries and serialized files (CSV, JSON) for every intermediate result, ensuring full auditability.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation & Preprocessing (Tasks 1-7):** Ingests raw data, validates schemas, enforces accounting identities (e.g., Assets = Liabilities + Equity), and handles missing values/zero denominators.\n",
        "2.  **AHP Analysis (Tasks 8-15):** Constructs the risk hierarchy, builds pairwise comparison matrices, computes local weights, filters inconsistent experts ($CR \\ge 0.10$), and aggregates global weights.\n",
        "3.  **Ratio Computation (Tasks 16-18):** Defines computational logic for 34 ratios, computes the raw decision matrix $X$, and validates it for outliers and zero variance.\n",
        "4.  **SAW Analysis (Tasks 19-23):** Configures criterion directionality (Benefit/Cost), normalizes the decision matrix to risk scores $R$, applies global weights to get $V$, and computes composite risk scores $V_t$.\n",
        "5.  **Ranking & Validation (Tasks 24-27):** Ranks fiscal years by aggregate risk, identifies extreme years, and cross-checks results against the published study values.\n",
        "6.  **Packaging (Task 28):** Generates a technical report and serializes all outputs into a reproducibility package.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `assessing_financial_statement_risks_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 28 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `execute_hafri_master_pipeline`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`execute_hafri_master_pipeline`:** This master orchestrator function, located in the final section of the notebook, runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project, managing data flow between all 28 sub-tasks, including robustness checks and report generation.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `pyyaml`, `scipy`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/assessing_financial_statement_risks.git\n",
        "    cd assessing_financial_statement_risks\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy pyyaml scipy\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires two primary DataFrames:\n",
        "1.  **`raw_expert_survey_df`**: A log of pairwise comparisons with columns: `expert_id`, `hierarchy_level`, `criterion_i`, `criterion_j`, `saaty_scale_value`, `comparison_type`.\n",
        "2.  **`raw_financial_statement_df`**: Audited financial line items with columns: `fiscal_year`, `total_assets`, `current_assets`, `net_operating_cash_flow`, etc. (27 fields total).\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `assessing_financial_statement_risks_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell of the notebook, which demonstrates how to use the top-level `execute_hafri_master_pipeline` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # 1. Load the master configuration from the YAML file.\n",
        "    with open('study_configuration.yaml', 'r') as f:\n",
        "        study_config = yaml.safe_load(f)\n",
        "    \n",
        "    # 2. Load raw datasets (Example using synthetic generator provided in the notebook)\n",
        "    # In production, load from CSV/Parquet: pd.read_csv(...)\n",
        "    raw_expert_survey_df = ...\n",
        "    raw_financial_statement_df = ...\n",
        "    \n",
        "    # 3. Execute the entire replication study.\n",
        "    results = execute_hafri_master_pipeline(\n",
        "        raw_expert_survey_df=raw_expert_survey_df,\n",
        "        raw_financial_statement_df=raw_financial_statement_df,\n",
        "        study_configuration=study_config\n",
        "    )\n",
        "    \n",
        "    # 4. Access results\n",
        "    print(f\"Most Risky Year: {results['baseline_results']['ranking'].index[0]}\")\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline returns a master dictionary containing all analytical artifacts:\n",
        "-   **`baseline_results`**: Contains `global_weights`, `decision_matrix`, `normalized_matrix`, `weighted_matrix`, `composite_scores`, `ranking`, and `comparison`.\n",
        "-   **`robustness_results`**: Contains `scenario_details` and `stability_summary` (rank statistics across scenarios).\n",
        "-   **`validation_results`**: Contains `weight_comparison`, `score_comparison`, and `summary_report` (discrepancy analysis).\n",
        "-   **`final_package`**: Contains serialized strings for `technical_report.md`, `README.md`, and CSVs of all matrices.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "assessing_financial_statement_risks/\n",
        "│\n",
        "├── assessing_financial_statement_risks_draft.ipynb  # Main implementation notebook\n",
        "├── study_configuration.yaml                         # Master configuration file\n",
        "├── requirements.txt                                 # Python package dependencies\n",
        "│\n",
        "├── LICENSE                                          # MIT Project License File\n",
        "└── README.md                                        # This file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `study_configuration.yaml` file. Users can modify study parameters such as:\n",
        "-   **Time Horizon:** `start_year`, `end_year`.\n",
        "-   **AHP Settings:** `consistency_threshold`, `saaty_scale_mapping`.\n",
        "-   **Ratio Definitions:** Numerator/denominator logic in `feature_engineering_logic`.\n",
        "-   **SAW Settings:** `criteria_directionality` (Benefit/Cost assignment).\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Fuzzy AHP:** Incorporating fuzzy logic to handle uncertainty in expert judgments.\n",
        "-   **TOPSIS Integration:** Adding Technique for Order of Preference by Similarity to Ideal Solution as an alternative ranking method.\n",
        "-   **Dynamic Weighting:** Allowing weights to evolve over time based on market conditions.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{abdullah2025assessing,\n",
        "  title={Assessing Financial Statement Risks among MCDM Techniques},\n",
        "  author={Abdullah, Marwa and Anatolyevna, Revzon Oksana and Abdullah, Duaa},\n",
        "  journal={arXiv preprint arXiv:2512.04035v1},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). Heuristic-Augmented Financial Risk Index (HAFRI): An Open Source Implementation.\n",
        "GitHub repository: https://github.com/chirindaopensource/assessing_financial_statement_risks\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Marwa Abdullah et al.** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, and SciPy**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `assessing_financial_statement_risks_draft.ipynb` notebook and follows best practices for research software documentation.*\n"
      ],
      "metadata": {
        "id": "OUJZMvBvplQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Assessing Financial Statement Risks among MCDM Techniques*\"\n",
        "\n",
        "Authors: Marwa Abdullah, Revzon Oksana Anatolyevna, Duaa Abdullah\n",
        "\n",
        "E-Journal Submission Date: 3 December 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2512.04035v1\n",
        "\n",
        "Revised Abstract:\n",
        "\n",
        "This paper introduces a Heuristic-Augmented Financial Risk Index (HAFRI) constructed through the systematic integration of two complementary multi-criteria decision-making (MCDM) techniques: the Analytic Hierarchy Process (AHP) for weight elicitation and Simple Additive Weighting (SAW) for multi-attribute utility aggregation. The proposed framework addresses the problem of ranking discrete fiscal periods by their aggregate exposure to financial risk, where risk is conceptualized as a latent construct manifested through observable financial ratios.\n",
        "\n",
        "**Methodological Architecture.** The risk assessment hierarchy comprises three levels: (1) a singular goal node representing aggregate financial risk; (2) four main criteria corresponding to distinct risk domains—Capital Structure Risk (CSR), Liquidity Risk (LR), Income Risk (IR), and Cash Flow Risk (CFR); and (3) 34 sub-criteria operationalized as financial ratios computed from the Statement of Financial Position, Income Statement, and Statement of Cash Flows. Criterion importance weights are derived from pairwise comparison matrices elicited from a panel of five domain experts using Saaty's 1–9 psychometric scale. Each matrix is validated against the consistency ratio threshold (CR < 0.10), with weights computed via the row geometric mean approximation to the principal eigenvector. Global sub-criterion weights are obtained through hierarchical weight cascade (multiplication of level-specific weights). The SAW phase transforms raw financial ratio values into commensurable utility scores via Min-Max normalization, with explicit encoding of criterion directionality: benefit criteria (higher values reduce risk) and cost criteria (higher values increase risk). Weighted normalized scores are aggregated via linear summation to produce a cardinal risk index for each fiscal period.\n",
        "\n",
        "**Empirical Application.** The framework is applied to Al-Ahliah Vegetable Oil Company, a publicly listed firm on the Damascus Securities Exchange, over the ten-year period 2008–2017. The ten fiscal years constitute the decision alternatives. Expert consensus assigns the highest aggregate weight to Cash Flow Risk (45.9%), followed by Liquidity Risk (24.4%), Income Risk (15.2%), and Capital Structure Risk (14.6%). At the sub-criterion level, the Cash Readiness Ratio (LR3) emerges as the most critical indicator (13.3% global weight), followed by the Quick Ratio (LR2, 9.4%) and Return on Equity (IR6, 4.5%). The integrated AHP-SAW analysis identifies fiscal year 2016 as exhibiting the highest aggregate risk exposure (relative score: 13.3%), attributable to elevated liquidity ratios signaling increased return volatility. Conversely, fiscal year 2009 demonstrates the lowest risk exposure (relative score: 8.0%).\n",
        "\n",
        "**Contributions.** The study contributes a replicable, modular, and customizable decision-support methodology for financial risk assessment that (i) synthesizes expert judgment with empirical financial data, (ii) provides full methodological transparency through explicit weight derivation and consistency validation, and (iii) enables comparative risk analysis across temporal or cross-sectional units.\n"
      ],
      "metadata": {
        "id": "jZWfcXDC3ZT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **Bibliographic Context and Research Objective**\n",
        "The paper proposes a quantitative framework for evaluating financial risk within an industrial firm. Unlike traditional econometric models that might rely on stochastic processes (e.g., Brownian motion for asset pricing) or regression analysis, this study employs **Operations Research** techniques.\n",
        "\n",
        "*   **Objective:** To quantify the relative importance of various financial risks and rank specific fiscal years based on risk exposure.\n",
        "*   **Subject:** Al-Ahlia Vegetable Oil Company (Syria).\n",
        "*   **Time Series:** 2008–2017.\n",
        "*   **Core Methodology:** A hybrid integration of the **Analytic Hierarchy Process (AHP)** for criteria weighting and **Simple Additive Weighting (SAW)** for alternative ranking.\n",
        "\n",
        "### **Theoretical Framework and Risk Taxonomy**\n",
        "The authors depart from standard risk classifications (e.g., systematic vs. unsystematic) and instead propose a taxonomy derived directly from the three primary accounting statements. This creates a hierarchical structure suitable for MCDM decomposition.\n",
        "\n",
        "The risk model is defined by four main criteria ($C_j$), subdivided into secondary criteria (financial ratios):\n",
        "1.  **Capital Structure Risk (CSR):** Derived from the Balance Sheet (e.g., Debt-to-Equity, Retained Earnings/Assets).\n",
        "2.  **Liquidity Risk (LR):** Derived from the Balance Sheet (e.g., Turnover ratio, Fast liquidity ratio).\n",
        "3.  **Income Risk (IR):** Derived from the Income Statement (e.g., Net Profit/Sales).\n",
        "4.  **Cash Flow Risk (CFR):** Derived from the Cash Flow Statement (e.g., Operating Cash Flow/Total Debt).\n",
        "\n",
        "### **Algorithmic Methodology**\n",
        "The research employs a two-phase computational approach.\n",
        "\n",
        "#### **Phase I: Weight Determination via AHP**\n",
        "The Analytic Hierarchy Process (Saaty, 1980) is used to extract expert heuristics and convert them into numerical weights.\n",
        "1.  **Pairwise Comparison:** A matrix $A$ is constructed where $A_{ij}$ represents the relative importance of criterion $i$ over $j$.\n",
        "2.  **Eigenvector Calculation:** The principal eigenvector is computed to determine the relative weights ($W$). The maximum eigenvalue ($\\lambda_{max}$) is derived using:\n",
        "    $$ \\lambda_{max} = \\sum_{i=1}^{m} EV_i \\times S_i $$\n",
        "    *(Where $EV$ is the eigenvector and $S$ is the column sum).*\n",
        "3.  **Consistency Check:** A Consistency Ratio ($CR$) is calculated ($CR = CI/RI$). The authors ensure $CR < 0.10$ to validate the logical consistency of the experts' judgments.\n",
        "\n",
        "#### **Ranking via SAW**\n",
        "Simple Additive Weighting is applied to rank the \"alternatives,\" which in this study are the **financial years** (2008–2017).\n",
        "1.  **Decision Matrix Construction:** A matrix $D$ of size $m \\times n$ (10 years $\\times$ 34 ratios).\n",
        "2.  **Normalization (Max-Min):** The data is normalized to a $[0,1]$ scale to handle incommensurable units.\n",
        "    *   **Benefit Criteria (Max):** $r_{ij} = (x_{ij} - x_j^-) / (x_j^+ - x_j^-)$\n",
        "    *   **Cost Criteria (Min):** $r_{ij} = (x_j^+ - x_{ij}) / (x_j^+ - x_j^-)$\n",
        "    *(Note: The paper explicitly identifies ratios like Total Debt/Equity as cost criteria and Cash Flow/Debt as benefit criteria).*\n",
        "3.  **Weighted Aggregation:** The final score $V_i$ for each year is the dot product of the weight vector and the normalized attribute vector:\n",
        "    $$ V_i = \\sum_{j=1}^{m} w_j r_{ij} $$\n",
        "\n",
        "### **Empirical Results**\n",
        "\n",
        "#### **Weighting Results (AHP)**\n",
        "Based on the input of five financial experts, the algorithm yielded the following hierarchy of risk importance:\n",
        "1.  **Cash Flow Risk (CFR):** **45.9%** (Highest Weight). The experts view the ability to generate cash as the primary determinant of risk.\n",
        "2.  **Liquidity Risk (LR):** **24.3%**.\n",
        "3.  **Income Risk (IR):** **15.1%**.\n",
        "4.  **Capital Structure Risk (CSR):** **14.5%** (Lowest Weight).\n",
        "\n",
        "*Observation:* The specific sub-criterion **\"Liquidity Readiness Ratio\" (LR3)** was deemed the single most critical metric (13.2% global weight), while **\"Retained Earnings to Assets\"** was the least critical.\n",
        "\n",
        "#### **Temporal Ranking (SAW)**\n",
        "The model generated a risk index for each year (where a higher score implies *better* performance/lower risk, though the paper's phrasing on \"risk exposure\" vs \"performance score\" requires careful interpretation of the SAW inversion).\n",
        "\n",
        "*   **Highest Risk Year:** **2016**.\n",
        "    *   *Reasoning:* Despite high liquidity ratios, the company suffered from poor cash flow generation relative to profits.\n",
        "*   **Lowest Risk Year:** **2009**.\n",
        "    *   *Reasoning:* High cash generation rates and stable financial ratios.\n",
        "\n",
        "### **Critical Assessment & Conclusion**\n",
        "\n",
        "**Strengths:**\n",
        "*   **Methodological Integration:** Successfully demonstrates the coupling of subjective expert intuition (AHP) with objective historical data (SAW).\n",
        "*   **Granularity:** The decomposition of risk into 34 distinct financial ratios provides a high-resolution view of the company's financial health.\n",
        "\n",
        "**Limitations (Academic Critique):**\n",
        "*   **Deterministic Nature:** The model treats financial ratios as static inputs. It lacks stochastic elements (e.g., Monte Carlo simulations) that would account for market volatility or uncertainty in the ratios themselves.\n",
        "*   **Sample Size:** The AHP relies on only five experts, which may introduce bias.\n",
        "*   **Interpretation of \"Risk\":** The paper conflates \"Performance\" with \"Risk.\" In SAW, a higher score usually indicates the \"best\" alternative. The paper concludes 2016 is the \"most dangerous\" but assigns it the highest score (Rank 1) in Table 12, while the text implies 2016 is the riskiest. There is a semantic inversion in the paper's conclusion versus the standard mathematical interpretation of SAW scores (usually High Score = Good). *Correction: Upon close inspection of the conclusion, the authors state 2016 had the highest \"financial risk,\" yet Table 12 ranks it #1. This suggests the authors may have modeled \"Risk\" as the benefit criterion, or there is a discrepancy in their interpretation of the SAW output.*\n",
        "\n",
        "**Final Verdict:** The paper provides a functional heuristic framework for internal audit and risk assessment, prioritizing **Cash Flow** analysis over traditional Balance Sheet leverage metrics."
      ],
      "metadata": {
        "id": "vTPWwxcMGBkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "cza7Wm1tPbLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  Heuristic-Augmented Financial Risk Index (HAFRI) Construction\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Assessing Financial Statement Risks among\n",
        "#  MCDM Techniques\" by Marwa Abdullah, Revzon Oksana Anatolyevna, and Duaa\n",
        "#  Abdullah (2025). It delivers a computationally tractable system for quantitative\n",
        "#  financial risk assessment, enabling robust, multi-criteria ranking of fiscal\n",
        "#  periods through the integration of expert heuristics and objective financial data.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Analytic Hierarchy Process (AHP) for subjective weight elicitation\n",
        "#  • Hierarchical decomposition of financial risk into Capital Structure, Liquidity,\n",
        "#    Income, and Cash Flow domains\n",
        "#  • Pairwise comparison matrix construction and consistency validation (CR < 0.10)\n",
        "#  • Principal eigenvector approximation via Row Average Method for weight derivation\n",
        "#  • Simple Additive Weighting (SAW) for multi-attribute utility aggregation\n",
        "#  • Min-Max normalization with explicit Benefit/Cost criterion directionality\n",
        "#  • Composite risk index calculation via linear weighted summation\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Robust data validation pipeline for expert surveys and financial statements\n",
        "#  • Automated enforcement of accounting identities and derived field computation\n",
        "#  • Vectorized matrix operations for efficient AHP weight calculation\n",
        "#  • Dynamic handling of zero-denominator anomalies and missing data policies\n",
        "#  • Comprehensive robustness analysis framework (sensitivity to weights/methods)\n",
        "#  • Automated generation of technical reports and reproducibility artifacts\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Abdullah, M., Anatolyevna, R. O., & Abdullah, D. (2025). Assessing Financial\n",
        "#  Statement Risks among MCDM Techniques. arXiv preprint arXiv:2512.04035v1.\n",
        "#  https://arxiv.org/abs/2512.04035v1\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Set, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "E2pStsEvPfMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "v7oS1WYFPUlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Discussion of the Inputs, Processes and Outputs of Key Pipeline Consituents**\n",
        "\n",
        "### **Task 1: Validate the `study_configuration` dictionary**\n",
        "\n",
        "**1. `validate_study_configuration` (Orchestrator)**\n",
        "*   **Inputs:** `study_configuration` (Dict).\n",
        "*   **Process:** Sequentially invokes sub-validators (`validate_top_level_keys`, `validate_metadata_scope`, `validate_schema_definitions`) to check structural integrity, metadata constraints (e.g., 2008–2017 timeframe), and schema completeness.\n",
        "*   **Outputs:** `bool` (True if valid, raises Exception otherwise).\n",
        "*   **Research Role:** Implements the **initialization phase** of the research pipeline. It ensures the computational environment matches the study's parameters: \"Applied to AL-Ahliah Vegetable Oil Company... from 2008 to 2017.\" It enforces the structural prerequisites for all subsequent MCDM calculations.\n",
        "\n",
        "### **Task 2: Validate the `raw_expert_survey_df` DataFrame schema**\n",
        "\n",
        "**2. `validate_raw_expert_survey` (Orchestrator)**\n",
        "*   **Inputs:** `raw_expert_survey_df` (DataFrame).\n",
        "*   **Process:** Checks for required columns (`expert_id`, `hierarchy_level`, etc.), validates categorical domains (e.g., exactly 5 experts), and enforces the Saaty scale constraint ($x \\in \\{1..9\\}$).\n",
        "*   **Outputs:** `bool` (True if valid).\n",
        "*   **Research Role:** Implements the **data quality assurance** for the AHP phase. It ensures the input data conforms to the \"expert questionnaire\" methodology described in Section 5, specifically validating the \"scale of the hierarchical analysis process\" (Table 4).\n",
        "\n",
        "### **Task 3: Validate combinatorial completeness**\n",
        "\n",
        "**3. `validate_combinatorial_completeness` (Orchestrator)**\n",
        "*   **Inputs:** `raw_expert_survey_df` (DataFrame).\n",
        "*   **Process:** Retrieves the canonical hierarchy structure. For each expert and hierarchy level, it calculates the required number of pairwise comparisons $N_h = \\binom{n_h}{2}$ and verifies that the input data contains exactly this many unique unordered pairs.\n",
        "*   **Outputs:** `bool` (True if complete).\n",
        "*   **Research Role:** Enforces the **completeness axiom** of the AHP methodology. It ensures that for every matrix $A^{(e)}_{(h)}$, all off-diagonal entries $A_{ij}$ ($i < j$) are provided, allowing for the construction of fully defined reciprocal matrices as required by Saaty's method.\n",
        "\n",
        "### **Task 4: Clean and standardize `raw_expert_survey_df`**\n",
        "\n",
        "**4. `clean_and_standardize_survey` (Orchestrator)**\n",
        "*   **Inputs:** `raw_expert_survey_df` (DataFrame).\n",
        "*   **Process:** Removes degenerate rows (self-comparisons where $i=j$), strips whitespace from string identifiers to ensure canonical matching, and re-verifies combinatorial completeness post-cleaning.\n",
        "*   **Outputs:** `cleaned_survey_df` (DataFrame).\n",
        "*   **Research Role:** Implements the **data preprocessing** step necessary to construct the \"pairwise comparison matrix\" described in Section 5. It ensures that the diagonal elements $A_{ii}=1$ are implicit and not duplicated in the input stream.\n",
        "\n",
        "### **Task 5: Validate the `raw_financial_statement_df` DataFrame schema**\n",
        "\n",
        "**5. `validate_financial_statements` (Orchestrator)**\n",
        "*   **Inputs:** `raw_financial_statement_df` (DataFrame).\n",
        "*   **Process:** Validates the presence of the `fiscal_year` index and 25 base financial columns. Enforces temporal continuity and uniqueness for the period 2008–2017.\n",
        "*   **Outputs:** `validated_financial_df` (DataFrame).\n",
        "*   **Research Role:** Implements the **data validation** for the \"financial statements\" analysis (Section 3). It ensures the dataset covers the exact \"ten years (2008–2017)\" specified in the Abstract and Case Study sections.\n",
        "\n",
        "### **Task 6: Validate financial identities and enforce derived equalities**\n",
        "\n",
        "**6. `enforce_financial_identities` (Orchestrator)**\n",
        "*   **Inputs:** `validated_financial_df` (DataFrame).\n",
        "*   **Process:** Validates the accounting identity $\\text{Total Debt} \\approx \\text{Short Term} + \\text{Long Term}$. Derives `net_working_capital` ($CA - CL$) and `net_profit_before_interest` ($\\text{Net Profit} + \\text{Interest}$).\n",
        "*   **Outputs:** `enriched_financial_df` (DataFrame).\n",
        "*   **Research Role:** Implements the **feature engineering** logic required to compute specific ratios defined in Tables 1 and 2. For example, it constructs the numerator for the ratio \"Net Profit Before Interest / net profit after interest\" (IR1).\n",
        "\n",
        "### **Task 7: Handle missing values and zero denominators**\n",
        "\n",
        "**7. `handle_missing_and_zero_values` (Orchestrator)**\n",
        "*   **Inputs:** `enriched_financial_df` (DataFrame), `study_configuration` (Dict).\n",
        "*   **Process:** Scans for critical missing data. Evaluates denominator expressions for all 34 ratios and identifies year-ratio pairs where the denominator is effectively zero. Returns a mask to enforce NaNs in these cases.\n",
        "*   **Outputs:** `diagnostic_report` (Dict), `zero_mask` (DataFrame).\n",
        "*   **Research Role:** Implements the **robustness checks** for ratio computation. It addresses the \"degree of uncertainty\" mentioned in the introduction by identifying data points where financial ratios (e.g., \"Net cash flows / net profit\") are mathematically undefined.\n",
        "\n",
        "### **Task 8: Define hierarchical structure**\n",
        "\n",
        "**8. `initialize_ahp_hierarchy` (Orchestrator)**\n",
        "*   **Inputs:** None (uses internal static definition).\n",
        "*   **Process:** Instantiates the `AHPHierarchy` class, which defines the 4 main criteria and 34 sub-criteria, maps sub-criteria to parents, and establishes the canonical global order.\n",
        "*   **Outputs:** `hierarchy` (AHPHierarchy object).\n",
        "*   **Research Role:** Implements the **hierarchical decomposition** of the problem (Figure 1). It structurally defines the \"goal representing the problem\" and the \"main criteria\" and \"secondary criteria\" as described in Section 4.\n",
        "\n",
        "### **Task 9: Build pairwise comparison matrices**\n",
        "\n",
        "**9. `build_ahp_matrices` (Orchestrator)**\n",
        "*   **Inputs:** `cleaned_survey_df` (DataFrame), `hierarchy` (Object).\n",
        "*   **Process:** Initializes $n \\times n$ matrices with 1s on the diagonal. Populates off-diagonal entries $A_{ij}$ from survey data. Enforces reciprocity by setting $A_{ji} = 1/A_{ij}$.\n",
        "*   **Outputs:** `matrices` (Nested Dict of np.ndarray).\n",
        "*   **Research Role:** Implements **Step 1(a)** of the AHP methodology (Section 5): \"Building a pairwise comparison matrix... $A_{ij} = 1/a_{ji}$\".\n",
        "\n",
        "### **Task 10: Compute local weights**\n",
        "\n",
        "**10. `compute_ahp_local_weights` (Orchestrator)**\n",
        "*   **Inputs:** `matrices` (Dict).\n",
        "*   **Process:** Computes column sums $S_j = \\sum_i A_{ij}$. Normalizes matrices $\\tilde{A}_{ij} = A_{ij}/S_j$. Computes row averages $w_i = \\frac{1}{n} \\sum_j \\tilde{A}_{ij}$ to derive local weights.\n",
        "*   **Outputs:** `local_weights` (Dict), `column_sums` (Dict).\n",
        "*   **Research Role:** Implements **Step 1(b) and 1(c)** of the AHP methodology: \"Derive the normalized comparison matrix\" and \"Calculate the relative weights for each row\".\n",
        "\n",
        "### **Task 11: Compute consistency metrics**\n",
        "\n",
        "**11. `compute_ahp_consistency_metrics` (Orchestrator)**\n",
        "*   **Inputs:** `local_weights`, `column_sums`, `hierarchy`, `ahp_parameters`.\n",
        "*   **Process:** Computes $\\lambda_{max} = \\sum w_i S_i$. Computes Consistency Index $CI = (\\lambda_{max} - n)/(n-1)$. Retrieves Random Index $RI(n)$.\n",
        "*   **Outputs:** `lambda_max` (Dict), `ci` (Dict), `ri` (Dict).\n",
        "*   **Research Role:** Implements **Step 1(d)** of the AHP methodology: \"Calculate the consistency ratio or consistency index... $\\lambda_{max} = \\sum EV_i * S_i$\".\n",
        "\n",
        "### **Task 12: Filter consistent matrices**\n",
        "\n",
        "**12. `filter_consistent_matrices` (Orchestrator)**\n",
        "*   **Inputs:** `ci`, `ri`, `ahp_parameters`.\n",
        "*   **Process:** Computes $CR = CI/RI$. Filters experts where $CR < 0.10$. Verifies at least one expert remains per level.\n",
        "*   **Outputs:** `cr_values` (Dict), `accepted_experts` (Dict).\n",
        "*   **Research Role:** Implements the **consistency validation** rule: \"The result is considered acceptable if the consistency ratio is less than 0.10.\"\n",
        "\n",
        "### **Task 13: Aggregate main criteria weights**\n",
        "\n",
        "**13. `aggregate_main_criteria_weights` (Orchestrator)**\n",
        "*   **Inputs:** `local_weights`, `accepted_experts`.\n",
        "*   **Process:** Selects weight vectors for \"Main_Criteria\" from accepted experts. Computes the arithmetic mean vector and normalizes it to sum to 1.\n",
        "*   **Outputs:** `main_weights` (np.ndarray).\n",
        "*   **Research Role:** Implements the **group decision making** aggregation to derive the consensus weights for CSR, LR, IR, and CFR (Table 9).\n",
        "\n",
        "### **Task 14: Aggregate sub-criteria weights**\n",
        "\n",
        "**14. `aggregate_all_sub_criteria_weights` (Orchestrator)**\n",
        "*   **Inputs:** `local_weights`, `accepted_experts`.\n",
        "*   **Process:** Iterates through sub-levels (Sub_CSR, etc.), aggregates accepted expert vectors via arithmetic mean, and maps them to their parent codes.\n",
        "*   **Outputs:** `sub_weights` (Dict of np.ndarray).\n",
        "*   **Research Role:** Implements the **group aggregation** for the secondary criteria levels, producing the \"Average weights of secondary criteria\" (Table 9).\n",
        "\n",
        "### **Task 15: Compute global weights**\n",
        "\n",
        "**15. `compute_global_weights` (Orchestrator)**\n",
        "*   **Inputs:** `main_weights`, `sub_weights`, `hierarchy`.\n",
        "*   **Process:** Computes global weights via hierarchical composition: $w_s^{global} = w_{parent(s)}^{main} \\times w_s^{local}$. Assembles them into a canonical vector and normalizes.\n",
        "*   **Outputs:** `global_weights` (np.ndarray).\n",
        "*   **Research Role:** Implements the **hierarchical synthesis** to produce the final \"Average weights of secondary standards relative to the target\" (Figure 3).\n",
        "\n",
        "### **Task 16: Prepare ratio logic**\n",
        "\n",
        "**16. `prepare_ratio_computation_logic` (Orchestrator)**\n",
        "*   **Inputs:** `hierarchy`, `study_configuration`.\n",
        "*   **Process:** Retrieves the canonical ratio order and extracts the numerator/denominator expressions for all 34 ratios from the configuration.\n",
        "*   **Outputs:** `canonical_order` (List), `ratio_specs` (Dict).\n",
        "*   **Research Role:** Prepares the **computational definitions** for the financial ratios listed in Tables 1, 2, and 3.\n",
        "\n",
        "### **Task 17: Compute decision matrix**\n",
        "\n",
        "**17. `compute_decision_matrix` (Orchestrator)**\n",
        "*   **Inputs:** `enriched_financial_df`, `canonical_order`, `ratio_specs`, `zero_mask`.\n",
        "*   **Process:** Evaluates ratio expressions against financial data for each year. Applies the zero-mask to force NaNs where denominators are zero.\n",
        "*   **Outputs:** `X` (DataFrame), `report` (Dict).\n",
        "*   **Research Role:** Computes the **raw decision matrix** $X$ where rows are years (alternatives) and columns are ratios (criteria), representing the \"actual historical values derived from the financial statements\".\n",
        "\n",
        "### **Task 18: Validate decision matrix**\n",
        "\n",
        "**18. `validate_and_freeze_decision_matrix` (Orchestrator)**\n",
        "*   **Inputs:** `X`.\n",
        "*   **Process:** Checks for zero-variance columns and statistical outliers. Freezes the matrix for SAW processing.\n",
        "*   **Outputs:** `X_final` (DataFrame), `valid_mask` (DataFrame), `diagnostics` (Dict).\n",
        "*   **Research Role:** Ensures the **data integrity** of the decision matrix before normalization, a prerequisite for the SAW method.\n",
        "\n",
        "### **Task 19: Configure SAW directionality**\n",
        "\n",
        "**19. `configure_saw_directionality` (Orchestrator)**\n",
        "*   **Inputs:** `saw_parameters`, `canonical_order`.\n",
        "*   **Process:** Validates that every ratio is assigned exactly one direction (Benefit or Cost) and builds a lookup map.\n",
        "*   **Outputs:** `directionality_map` (Dict).\n",
        "*   **Research Role:** Defines the **risk semantics** for each ratio (e.g., \"Total Debt / Equity\" is a cost criterion/Min, \"Cash Readiness\" is a benefit criterion/Max) as required for Step 2 of the SAW method.\n",
        "\n",
        "### **Task 20: Normalize decision matrix**\n",
        "\n",
        "**20. `normalize_decision_matrix` (Orchestrator)**\n",
        "*   **Inputs:** `X_final`, `directionality_map`.\n",
        "*   **Process:** Computes column extrema ($x_j^+, x_j^-$). Applies Min-Max normalization: $r_{tj} = \\frac{x_j^+ - x_{tj}}{x_j^+ - x_j^-}$ (Benefit) or $r_{tj} = \\frac{x_{tj} - x_j^-}{x_j^+ - x_j^-}$ (Cost).\n",
        "*   **Outputs:** `R` (DataFrame).\n",
        "*   **Research Role:** Implements **Step 2 of the SAW method**: \"Converting the Pairwise Comparison Matrix into a Normalized Matrix (Max/Min)\". Note: The paper's notation for cost criteria was inverted; this implementation uses the standard correct formula for risk (higher raw cost = higher risk score).\n",
        "\n",
        "### **Task 21: Compute weighted risk matrix**\n",
        "\n",
        "**21. `compute_weighted_risk_matrix` (Orchestrator)**\n",
        "*   **Inputs:** `R`, `global_weights`, `canonical_order`.\n",
        "*   **Process:** Aligns weights to columns and computes element-wise product $v_{tj} = w_j \\times r_{tj}$.\n",
        "*   **Outputs:** `V` (DataFrame), `report` (Dict).\n",
        "*   **Research Role:** Implements **Step 3 of the SAW method**: \"Building the Weighted Normalized Matrix V... multiplying the normalized matrix... by the vector of relative weights\".\n",
        "\n",
        "### **Task 22: Compute composite risk scores**\n",
        "\n",
        "**22. `compute_composite_risk_scores` (Orchestrator)**\n",
        "*   **Inputs:** `V`.\n",
        "*   **Process:** Sums the weighted risk scores across all criteria for each year: $V_t = \\sum_j v_{tj}$.\n",
        "*   **Outputs:** `V_t` (Series), `report` (Dict).\n",
        "*   **Research Role:** Implements **Step 4 of the SAW method**: \"Calculate the performance vector of alternatives... $V = \\sum w_j r_{ij}$\".\n",
        "\n",
        "### **Task 23: Compute relative risk indices**\n",
        "\n",
        "**23. `compute_relative_risk_indices` (Orchestrator)**\n",
        "*   **Inputs:** `V_t`.\n",
        "*   **Process:** Computes the total sum $S = \\sum V_t$ and divides each year's score by the sum: $A_t = V_t / S$.\n",
        "*   **Outputs:** `A_t` (Series), `report` (Dict).\n",
        "*   **Research Role:** Implements **Step 5 of the SAW method**: \"Rank the alternatives... $A_{ij} = V_{ij} / \\sum V_{ij}$\". This produces the relative risk share for each year.\n",
        "\n",
        "### **Task 24: Rank and compare years**\n",
        "\n",
        "**24. `rank_and_compare_years` (Orchestrator)**\n",
        "*   **Inputs:** `A_t`.\n",
        "*   **Process:** Ranks years by $A_t$ descending (Rank 1 = Highest Risk). Identifies max/min risk years. Compares rankings with the study's reported results (Table 12).\n",
        "*   **Outputs:** `ranking_df` (DataFrame), `extremes` (Dict), `comparison_df` (DataFrame).\n",
        "*   **Research Role:** Implements **Step 6 of the SAW method**: \"Determine the best alternative\" (or in this context, the riskiest year). It validates the finding that \"The year 2016 was the year in which the company was exposed to the highest level of financial risk\".\n",
        "\n",
        "### **Task 25: Main Pipeline Orchestrator**\n",
        "\n",
        "**25. `run_hafri_pipeline` (Orchestrator)**\n",
        "*   **Inputs:** Raw DataFrames, Configuration.\n",
        "*   **Process:** Sequentially executes Validation, AHP, Ratio, and SAW phases using the sub-orchestrators.\n",
        "*   **Outputs:** `baseline_results` (Dict).\n",
        "*   **Research Role:** Represents the **execution of the primary research methodology** to derive the HAFRI index.\n",
        "\n",
        "### **Task 26: Robustness Analysis Orchestrator**\n",
        "\n",
        "**26. `conduct_robustness_analysis` (Orchestrator)**\n",
        "*   **Inputs:** Configuration, Raw DataFrames.\n",
        "*   **Process:** Generates scenario configurations (Strict/Relaxed Consistency), executes the pipeline for each, and aggregates ranking statistics.\n",
        "*   **Outputs:** `scenario_results` (Dict), `robustness_summary` (DataFrame).\n",
        "*   **Research Role:** Implements a **sensitivity analysis** (implied by the need for rigorous validation) to test if the identification of 2016 as the riskiest year holds under varying assumptions.\n",
        "\n",
        "### **Task 27: Validation Orchestrator**\n",
        "\n",
        "**27. `validate_results_against_study` (Orchestrator)**\n",
        "*   **Inputs:** `global_weights`, `hierarchy`, `ranking_df`, `comparison_df`.\n",
        "*   **Process:** Compares computed weights and scores against hardcoded values from the paper. Generates a discrepancy report.\n",
        "*   **Outputs:** `validation_results` (Dict).\n",
        "*   **Research Role:** Performs the **verification** step, ensuring the implementation reproduces the published results (e.g., CFR weight $\\approx$ 45.9%).\n",
        "\n",
        "### **Task 28: Packaging Orchestrator**\n",
        "\n",
        "**28. `package_project_outputs` (Orchestrator)**\n",
        "*   **Inputs:** `results`, `study_configuration`.\n",
        "*   **Process:** Generates a Markdown technical report and serializes all data artifacts (CSVs, JSONs) into a file mapping.\n",
        "*   **Outputs:** `final_package` (Dict).\n",
        "*   **Research Role:** Produces the **final deliverables** and documentation required for reproducibility and external audit.\n",
        "\n",
        "<br><br>\n",
        "### **Usage Example**\n",
        "\n",
        "The following code snippet uses synthentic data to demonstrate, in a step by step fashion, how to run the Heuristic-Augmented Financial Risk Index (HAFRI) pipeline accurately:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "import itertools\n",
        "from typing import List, Dict\n",
        "\n",
        "# ==============================================================================\n",
        "# HAFRI Pipeline Usage Example\n",
        "# ==============================================================================\n",
        "# This script demonstrates the end-to-end execution of the Heuristic-Augmented\n",
        "# Financial Risk Index (HAFRI) pipeline using synthetic data that mirrors the\n",
        "# structure of the Al-Ahlia Vegetable Oil Company case study.\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 1: Synthetic Data Generation - Expert Survey\n",
        "# ------------------------------------------------------------------------------\n",
        "# We generate a synthetic dataset of expert pairwise comparisons.\n",
        "# The data must satisfy the combinatorial requirement: n*(n-1)/2 pairs per level.\n",
        "\n",
        "def generate_synthetic_survey() -> pd.DataFrame:\n",
        "    \"\"\"Generates a structurally valid synthetic expert survey DataFrame.\"\"\"\n",
        "    \n",
        "    experts = [f\"E{i}\" for i in range(1, 6)] # 5 Experts\n",
        "    \n",
        "    # Define hierarchy levels and their criteria (matching the config)\n",
        "    hierarchy_structure = {\n",
        "        \"Main_Criteria\": [\"CSR\", \"LR\", \"IR\", \"CFR\"],\n",
        "        \"Sub_CSR\": [f\"CSR{i}\" for i in range(1, 12)],\n",
        "        \"Sub_LR\": [f\"LR{i}\" for i in range(1, 4)],\n",
        "        \"Sub_IR\": [f\"IR{i}\" for i in range(1, 7)],\n",
        "        \"Sub_CFR\": [f\"CFR{i}\" for i in range(1, 15)]\n",
        "    }\n",
        "    \n",
        "    survey_rows = []\n",
        "    \n",
        "    for expert in experts:\n",
        "        for level, criteria in hierarchy_structure.items():\n",
        "            # Generate all unique unordered pairs (i, j) where i != j\n",
        "            # We use combinations to get n*(n-1)/2 pairs\n",
        "            pairs = list(itertools.combinations(criteria, 2))\n",
        "            \n",
        "            for crit_i, crit_j in pairs:\n",
        "                # Assign a random Saaty scale value {1..9}\n",
        "                # In a real scenario, these are elicited judgments.\n",
        "                # We use a fixed seed for reproducibility in this example context.\n",
        "                val = np.random.randint(1, 10)\n",
        "                \n",
        "                survey_rows.append({\n",
        "                    \"expert_id\": expert,\n",
        "                    \"hierarchy_level\": level,\n",
        "                    \"criterion_i\": crit_i,\n",
        "                    \"criterion_j\": crit_j,\n",
        "                    \"saaty_scale_value\": float(val),\n",
        "                    \"comparison_type\": \"Direct\"\n",
        "                })\n",
        "                \n",
        "    return pd.DataFrame(survey_rows)\n",
        "\n",
        "# Generate the survey dataframe\n",
        "print(\"Generating synthetic expert survey data...\")\n",
        "raw_expert_survey_df = generate_synthetic_survey()\n",
        "print(f\"Survey Data Generated: {raw_expert_survey_df.shape[0]} rows.\")\n",
        "print(raw_expert_survey_df.head())\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 2: Synthetic Data Generation - Financial Statements\n",
        "# ------------------------------------------------------------------------------\n",
        "# We generate synthetic financial statements for 2008-2017.\n",
        "# We ensure basic accounting identities hold to pass validation.\n",
        "\n",
        "def generate_synthetic_financials() -> pd.DataFrame:\n",
        "    \"\"\"Generates structurally valid synthetic financial statements.\"\"\"\n",
        "    years = list(range(2008, 2018))\n",
        "    n_years = len(years)\n",
        "    \n",
        "    data = {\"fiscal_year\": years}\n",
        "    \n",
        "    # Helper to generate positive floats\n",
        "    def gen_series(low, high):\n",
        "        return np.random.uniform(low, high, n_years)\n",
        "\n",
        "    # --- Balance Sheet ---\n",
        "    # Construct components first to ensure identities\n",
        "    data[\"current_assets\"] = gen_series(500, 1000)\n",
        "    data[\"net_fixed_assets\"] = gen_series(1000, 2000)\n",
        "    data[\"total_assets\"] = data[\"current_assets\"] + data[\"net_fixed_assets\"] # Identity\n",
        "    \n",
        "    data[\"current_liabilities\"] = gen_series(200, 500)\n",
        "    data[\"short_term_debt\"] = data[\"current_liabilities\"] * 0.8 # Assumption\n",
        "    data[\"long_term_debt\"] = gen_series(100, 300)\n",
        "    data[\"total_debt\"] = data[\"short_term_debt\"] + data[\"long_term_debt\"] # Identity\n",
        "    \n",
        "    # Equity = Assets - Liabilities (simplified)\n",
        "    # We treat total_debt + other_liabilities as total liabilities\n",
        "    # For simplicity in this mock, we define equity explicitly\n",
        "    data[\"shareholders_equity\"] = data[\"total_assets\"] - (data[\"current_liabilities\"] + data[\"long_term_debt\"])\n",
        "    \n",
        "    data[\"inventory\"] = data[\"current_assets\"] * 0.4\n",
        "    data[\"cash_and_equivalents\"] = data[\"current_assets\"] * 0.1\n",
        "    data[\"retained_earnings\"] = data[\"shareholders_equity\"] * 0.2\n",
        "    \n",
        "    # --- Income Statement ---\n",
        "    data[\"sales_revenue\"] = gen_series(800, 1500)\n",
        "    data[\"gross_profit\"] = data[\"sales_revenue\"] * 0.4\n",
        "    data[\"ebit\"] = data[\"gross_profit\"] * 0.5\n",
        "    data[\"interest_expense\"] = data[\"long_term_debt\"] * 0.05\n",
        "    data[\"net_profit\"] = data[\"ebit\"] - data[\"interest_expense\"]\n",
        "    \n",
        "    # --- Cash Flow Statement ---\n",
        "    data[\"net_operating_cash_flow\"] = gen_series(100, 300)\n",
        "    data[\"capital_expenditures\"] = gen_series(50, 100)\n",
        "    data[\"net_investing_cash_flow\"] = -data[\"capital_expenditures\"]\n",
        "    data[\"net_financing_cash_flow\"] = gen_series(-50, 50)\n",
        "    \n",
        "    # Derived CF fields\n",
        "    data[\"total_cash_flow_inv_fin\"] = data[\"net_investing_cash_flow\"] + data[\"net_financing_cash_flow\"]\n",
        "    data[\"cash_distributions\"] = gen_series(10, 50)\n",
        "    data[\"operating_cash_inflows\"] = data[\"sales_revenue\"] # Proxy\n",
        "    data[\"initial_cash_requirements\"] = gen_series(50, 100)\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Generate the financial dataframe\n",
        "print(\"Generating synthetic financial statement data...\")\n",
        "raw_financial_statement_df = generate_synthetic_financials()\n",
        "print(f\"Financial Data Generated: {raw_financial_statement_df.shape} (Rows, Cols).\")\n",
        "print(raw_financial_statement_df[[\"fiscal_year\", \"total_assets\", \"net_profit\"]].head())\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 3: Load Configuration\n",
        "# ------------------------------------------------------------------------------\n",
        "# We load the study parameters from the YAML file.\n",
        "\n",
        "print(\"Loading study configuration...\")\n",
        "try:\n",
        "    with open(\"study_configuration.yaml\", \"r\") as f:\n",
        "        study_configuration = yaml.safe_load(f)\n",
        "    print(\"Configuration loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'study_configuration.yaml' not found. Please ensure Task 28 output is saved.\")\n",
        "    # For the sake of the example running if file is missing, we would define it inline,\n",
        "    # but we assume the file exists as per instructions.\n",
        "    raise\n",
        "\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 4: Execute the HAFRI Master Pipeline\n",
        "# ------------------------------------------------------------------------------\n",
        "# We invoke the top-level orchestrator to run Validation, AHP, SAW, and Packaging.\n",
        "\n",
        "print(\"Executing HAFRI Master Pipeline...\")\n",
        "\n",
        "# Note: We assume 'execute_hafri_master_pipeline' is available in the environment.\n",
        "# In a real notebook, all previous cells defining the functions must be run first.\n",
        "\n",
        "try:\n",
        "    master_artifacts = execute_hafri_master_pipeline(\n",
        "        raw_expert_survey_df=raw_expert_survey_df,\n",
        "        raw_financial_statement_df=raw_financial_statement_df,\n",
        "        study_configuration=study_configuration\n",
        "    )\n",
        "    print(\"Pipeline execution completed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Pipeline failed: {e}\")\n",
        "    # In a real scenario, we would inspect the logs here.\n",
        "    raise\n",
        "\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 5: Inspect Outputs\n",
        "# ------------------------------------------------------------------------------\n",
        "# We explore the returned artifacts to understand the results.\n",
        "\n",
        "if 'master_artifacts' in locals():\n",
        "    baseline = master_artifacts[\"baseline_results\"]\n",
        "    \n",
        "    # 1. View Global Weights\n",
        "    print(\"\\n>>> Top 5 Global Risk Drivers (Weights):\")\n",
        "    weights = pd.Series(baseline[\"global_weights\"], index=baseline[\"decision_matrix\"].columns)\n",
        "    print(weights.sort_values(ascending=False).head(5))\n",
        "    \n",
        "    # 2. View Final Ranking\n",
        "    print(\"\\n>>> Financial Year Risk Ranking:\")\n",
        "    ranking = baseline[\"ranking\"][[\"A_t\", \"Rank\"]]\n",
        "    print(ranking.sort_values(\"Rank\"))\n",
        "    \n",
        "    # 3. View Robustness Summary\n",
        "    print(\"\\n>>> Robustness Analysis (Rank Stability):\")\n",
        "    robustness = master_artifacts[\"robustness_results\"][\"stability_summary\"]\n",
        "    print(robustness[[\"Mean_Rank\", \"Std_Rank\", \"Min_Rank\", \"Max_Rank\"]])\n",
        "    \n",
        "    # 4. View Validation Report\n",
        "    print(\"\\n>>> Validation against Original Study:\")\n",
        "    validation = master_artifacts[\"validation_results\"][\"summary_report\"]\n",
        "    print(f\"Rank Consistency: {validation['rank_consistency']['status']}\")\n",
        "    print(f\"Weight Consistency: {validation['weight_consistency']['status']}\")\n",
        "```\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "jlPxciPYPXtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 – Validate the `study_configuration` dictionary for structural completeness\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate and parse the study configuration dictionary\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 1:  Load the `study_parameters` dictionary and verify structural completeness.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_top_level_keys(study_configuration: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the presence of required top-level keys in the study configuration dictionary.\n",
        "\n",
        "    This function ensures that the configuration object contains all the necessary sections\n",
        "    required for the HAFRI pipeline, specifically metadata, schemas, AHP parameters,\n",
        "    feature engineering logic, and SAW parameters.\n",
        "\n",
        "    Args:\n",
        "        study_configuration (Dict[str, Any]): The main configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If `study_configuration` is not a dictionary.\n",
        "        ValueError: If any required top-level key is missing.\n",
        "    \"\"\"\n",
        "    # Validate that the input is a dictionary\n",
        "    if not isinstance(study_configuration, dict):\n",
        "        raise TypeError(f\"study_configuration must be a dict, got {type(study_configuration)}\")\n",
        "\n",
        "    # Define the set of required top-level keys based on the study specification\n",
        "    required_keys: Set[str] = {\n",
        "        \"metadata\",\n",
        "        \"input_data_schemas\",\n",
        "        \"ahp_parameters\",\n",
        "        \"feature_engineering_logic\",\n",
        "        \"saw_parameters\"\n",
        "    }\n",
        "\n",
        "    # Extract the actual keys present in the configuration\n",
        "    actual_keys: Set[str] = set(study_configuration.keys())\n",
        "\n",
        "    # Calculate missing keys by set difference\n",
        "    missing_keys: Set[str] = required_keys - actual_keys\n",
        "\n",
        "    # If there are missing keys, raise an error with a descriptive message\n",
        "    if missing_keys:\n",
        "        raise ValueError(f\"Missing required top-level keys in study_configuration: {missing_keys}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Validate numerical parameter ranges and types.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_metadata_scope(study_configuration: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the metadata section of the configuration, specifically temporal scope and expert panel size.\n",
        "\n",
        "    This function enforces the specific constraints of the study:\n",
        "    - Time horizon: 2008 to 2017 (10 periods).\n",
        "    - Expert panel: 5 experts.\n",
        "    It also verifies the arithmetic consistency of the time horizon.\n",
        "\n",
        "    Args:\n",
        "        study_configuration (Dict[str, Any]): The main configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If required nested keys are missing.\n",
        "        ValueError: If values are invalid, inconsistent, or do not match the study's requirements.\n",
        "        TypeError: If values cannot be cast to integers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access the metadata section\n",
        "        metadata = study_configuration[\"metadata\"]\n",
        "\n",
        "        # Access the time_horizon subsection\n",
        "        time_horizon = metadata[\"time_horizon\"]\n",
        "\n",
        "        # Extract and cast temporal parameters to integers\n",
        "        start_year: int = int(time_horizon[\"start_year\"])\n",
        "        end_year: int = int(time_horizon[\"end_year\"])\n",
        "        total_periods: int = int(time_horizon[\"total_periods\"])\n",
        "\n",
        "        # Access the expert_panel subsection\n",
        "        expert_panel = metadata[\"expert_panel\"]\n",
        "\n",
        "        # Extract and cast expert count\n",
        "        expert_count: int = int(expert_panel[\"count\"])\n",
        "\n",
        "    except KeyError as e:\n",
        "        # Raise error if a specific key path is missing\n",
        "        raise KeyError(f\"Missing required metadata key: {e}\")\n",
        "    except (ValueError, TypeError) as e:\n",
        "        # Raise error if values are not valid integers\n",
        "        raise TypeError(f\"Metadata values must be convertible to integers: {e}\")\n",
        "\n",
        "    # Verify the arithmetic identity: total_periods = end - start + 1\n",
        "    # Equation: T = Y_end - Y_start + 1\n",
        "    expected_periods: int = end_year - start_year + 1\n",
        "    if total_periods != expected_periods:\n",
        "        raise ValueError(\n",
        "            f\"Time horizon inconsistency: calculated periods ({expected_periods}) \"\n",
        "            f\"!= provided total_periods ({total_periods})\"\n",
        "        )\n",
        "\n",
        "    # Enforce specific study constraints for replication\n",
        "    # Constraint: Start Year = 2008\n",
        "    if start_year != 2008:\n",
        "        raise ValueError(f\"Study replication requires start_year=2008, got {start_year}\")\n",
        "\n",
        "    # Constraint: End Year = 2017\n",
        "    if end_year != 2017:\n",
        "        raise ValueError(f\"Study replication requires end_year=2017, got {end_year}\")\n",
        "\n",
        "    # Constraint: Expert Count = 5\n",
        "    if expert_count != 5:\n",
        "        raise ValueError(f\"Study replication requires expert_count=5, got {expert_count}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Validate string-based model identifiers and create a configuration snapshot.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_schema_definitions(study_configuration: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the input data schemas for expert surveys and financial statements.\n",
        "\n",
        "    Ensures that the configuration defines the correct column structures required\n",
        "    for downstream processing (AHP matrix construction and Ratio computation).\n",
        "\n",
        "    Args:\n",
        "        study_configuration (Dict[str, Any]): The main configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If schema sections are missing.\n",
        "        ValueError: If required columns are missing from the schema definitions.\n",
        "    \"\"\"\n",
        "    # Access the input_data_schemas section\n",
        "    schemas = study_configuration.get(\"input_data_schemas\", {})\n",
        "\n",
        "    # --- Validate Expert Survey Schema ---\n",
        "    survey_schema = schemas.get(\"expert_survey_schema\", {})\n",
        "    survey_columns_dict = survey_schema.get(\"columns\", {})\n",
        "\n",
        "    # Define required columns for the expert survey dataframe\n",
        "    required_survey_cols: Set[str] = {\n",
        "        \"expert_id\",\n",
        "        \"hierarchy_level\",\n",
        "        \"criterion_i\",\n",
        "        \"criterion_j\",\n",
        "        \"saaty_scale_value\",\n",
        "        \"comparison_type\"\n",
        "    }\n",
        "\n",
        "    # Check for missing survey columns\n",
        "    actual_survey_cols = set(survey_columns_dict.keys())\n",
        "    missing_survey_cols = required_survey_cols - actual_survey_cols\n",
        "\n",
        "    if missing_survey_cols:\n",
        "        raise ValueError(f\"expert_survey_schema missing required columns: {missing_survey_cols}\")\n",
        "\n",
        "    # --- Validate Financial Statement Schema ---\n",
        "    fin_schema = schemas.get(\"financial_statement_schema\", {})\n",
        "    fin_columns_dict = fin_schema.get(\"columns\", {})\n",
        "\n",
        "    # Define required columns for the financial statement dataframe\n",
        "    # These correspond to the 27 fields identified in the pre-code analysis\n",
        "    required_fin_cols: Set[str] = {\n",
        "        # Balance Sheet\n",
        "        \"total_assets\", \"current_assets\", \"inventory\", \"cash_and_equivalents\",\n",
        "        \"net_fixed_assets\", \"total_debt\", \"long_term_debt\", \"short_term_debt\",\n",
        "        \"current_liabilities\", \"shareholders_equity\", \"retained_earnings\",\n",
        "        \"net_working_capital\",\n",
        "\n",
        "        # Income Statement\n",
        "        \"sales_revenue\", \"gross_profit\", \"ebit\", \"interest_expense\",\n",
        "        \"net_profit\", \"net_profit_before_interest\",\n",
        "\n",
        "        # Cash Flow Statement\n",
        "        \"net_operating_cash_flow\", \"capital_expenditures\", \"net_investing_cash_flow\",\n",
        "        \"net_financing_cash_flow\", \"total_cash_flow_inv_fin\", \"cash_distributions\",\n",
        "        \"operating_cash_inflows\", \"initial_cash_requirements\"\n",
        "    }\n",
        "\n",
        "    # Check for missing financial columns\n",
        "    actual_fin_cols = set(fin_columns_dict.keys())\n",
        "    missing_fin_cols = required_fin_cols - actual_fin_cols\n",
        "\n",
        "    # Note: We allow net_working_capital and net_profit_before_interest to be missing\n",
        "    # from the *raw* dataframe (as they can be derived), but the *schema* must define them\n",
        "    # as expected fields. However, strictly following the prompt's schema definition,\n",
        "    # we enforce their presence in the schema keys.\n",
        "    if missing_fin_cols:\n",
        "        raise ValueError(f\"financial_statement_schema missing required columns: {missing_fin_cols}\")\n",
        "\n",
        "    # Validate index definition\n",
        "    if fin_schema.get(\"index\") != \"fiscal_year (Int64, 2008–2017)\":\n",
        "         # We check loosely for 'fiscal_year' in the string to be robust to description changes\n",
        "         if \"fiscal_year\" not in str(fin_schema.get(\"index\", \"\")):\n",
        "             raise ValueError(\"financial_statement_schema index must be defined as 'fiscal_year'\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_study_configuration(study_configuration: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 1: Validates the entire study configuration structure.\n",
        "\n",
        "    Sequentially calls sub-validators to ensure structural completeness, metadata accuracy,\n",
        "    and schema validity. This function acts as a gatekeeper before any data processing begins.\n",
        "\n",
        "    Args:\n",
        "        study_configuration (Dict[str, Any]): The main configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if validation passes.\n",
        "\n",
        "    Raises:\n",
        "        TypeError, ValueError, KeyError: If any validation step fails.\n",
        "    \"\"\"\n",
        "    # Step 1: Check top-level structure\n",
        "    validate_top_level_keys(study_configuration)\n",
        "\n",
        "    # Step 2: Check metadata and scope constraints\n",
        "    validate_metadata_scope(study_configuration)\n",
        "\n",
        "    # Step 3: Check schema definitions\n",
        "    validate_schema_definitions(study_configuration)\n",
        "\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "hj82dizXPaCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 – Validate the raw_expert_survey_df DataFrame schema\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate the `raw_expert_survey_df` DataFrame schema\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Check DataFrame type and column presence\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_survey_columns(raw_expert_survey_df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates that the input is a pandas DataFrame and contains all required columns\n",
        "    for the expert survey data.\n",
        "\n",
        "    This function enforces the schema defined in the study configuration, ensuring\n",
        "    that the DataFrame has the necessary structure for AHP processing.\n",
        "\n",
        "    Args:\n",
        "        raw_expert_survey_df (pd.DataFrame): The raw expert survey data.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input is not a pandas DataFrame.\n",
        "        ValueError: If any required column is missing.\n",
        "    \"\"\"\n",
        "    # Validate input type\n",
        "    # Check if the input object is an instance of pandas DataFrame\n",
        "    if not isinstance(raw_expert_survey_df, pd.DataFrame):\n",
        "        raise TypeError(f\"Input must be a pandas DataFrame, got {type(raw_expert_survey_df)}\")\n",
        "\n",
        "    # Define the set of required columns based on the schema\n",
        "    # These columns are essential for identifying experts, hierarchy levels, criteria, and judgments\n",
        "    required_columns: Set[str] = {\n",
        "        \"expert_id\",\n",
        "        \"hierarchy_level\",\n",
        "        \"criterion_i\",\n",
        "        \"criterion_j\",\n",
        "        \"saaty_scale_value\",\n",
        "        \"comparison_type\"\n",
        "    }\n",
        "\n",
        "    # Extract the actual columns from the DataFrame\n",
        "    actual_columns: Set[str] = set(raw_expert_survey_df.columns)\n",
        "\n",
        "    # Calculate missing columns\n",
        "    # Set difference operation: required - actual\n",
        "    missing_columns: Set[str] = required_columns - actual_columns\n",
        "\n",
        "    # If missing columns exist, raise an error\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"raw_expert_survey_df is missing required columns: {missing_columns}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Validate domain of categorical columns\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_survey_categorical_domains(raw_expert_survey_df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the domain constraints for categorical columns in the expert survey DataFrame.\n",
        "\n",
        "    Checks:\n",
        "    1. expert_id: Exactly 5 unique experts.\n",
        "    2. hierarchy_level: Values must belong to the defined set of AHP levels.\n",
        "    3. comparison_type: All values must be 'Direct'.\n",
        "\n",
        "    Args:\n",
        "        raw_expert_survey_df (pd.DataFrame): The raw expert survey data.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any domain constraint is violated.\n",
        "    \"\"\"\n",
        "    # --- Validate expert_id ---\n",
        "    # Extract unique expert IDs\n",
        "    unique_experts = raw_expert_survey_df[\"expert_id\"].unique()\n",
        "    num_experts = len(unique_experts)\n",
        "\n",
        "    # Constraint: There must be exactly 5 unique expert_id values\n",
        "    if num_experts != 5:\n",
        "        raise ValueError(f\"Expected exactly 5 unique experts, found {num_experts}: {unique_experts}\")\n",
        "\n",
        "    # --- Validate hierarchy_level ---\n",
        "    # Define the allowed hierarchy levels as per the study design\n",
        "    allowed_levels: Set[str] = {\n",
        "        \"Main_Criteria\",\n",
        "        \"Sub_CSR\",\n",
        "        \"Sub_LR\",\n",
        "        \"Sub_IR\",\n",
        "        \"Sub_CFR\"\n",
        "    }\n",
        "\n",
        "    # Check for invalid hierarchy levels\n",
        "    # We use .isin() to create a boolean mask, then invert it with ~ to find invalid rows\n",
        "    invalid_levels_mask = ~raw_expert_survey_df[\"hierarchy_level\"].isin(allowed_levels)\n",
        "\n",
        "    if invalid_levels_mask.any():\n",
        "        # Extract the specific invalid values for the error message\n",
        "        invalid_values = raw_expert_survey_df.loc[invalid_levels_mask, \"hierarchy_level\"].unique()\n",
        "        raise ValueError(f\"Invalid hierarchy_level values found: {invalid_values}. Allowed: {allowed_levels}\")\n",
        "\n",
        "    # --- Validate comparison_type ---\n",
        "    # Constraint: All entries must be 'Direct'\n",
        "    # We check if any value is NOT 'Direct'\n",
        "    invalid_type_mask = raw_expert_survey_df[\"comparison_type\"] != \"Direct\"\n",
        "\n",
        "    if invalid_type_mask.any():\n",
        "        invalid_types = raw_expert_survey_df.loc[invalid_type_mask, \"comparison_type\"].unique()\n",
        "        raise ValueError(f\"Invalid comparison_type values found: {invalid_types}. All must be 'Direct'.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Validate numeric range of Saaty scale values\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_saaty_scale_values(raw_expert_survey_df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates that the 'saaty_scale_value' column contains valid numeric values\n",
        "    conforming to Saaty's 1-9 scale.\n",
        "\n",
        "    Constraints:\n",
        "    - Values must be numeric.\n",
        "    - Values must be integers in the set {1, 2, 3, 4, 5, 6, 7, 8, 9}.\n",
        "    - Reciprocals (values < 1) are not allowed in the raw log.\n",
        "\n",
        "    Args:\n",
        "        raw_expert_survey_df (pd.DataFrame): The raw expert survey data.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If non-numeric values or values outside the 1-9 integer set are found.\n",
        "    \"\"\"\n",
        "    # Ensure the column is numeric, coercing errors to NaN to identify non-numbers\n",
        "    # This does not modify the original DataFrame in place, but returns a Series\n",
        "    numeric_values = pd.to_numeric(raw_expert_survey_df[\"saaty_scale_value\"], errors='coerce')\n",
        "\n",
        "    # Check for non-numeric values (NaNs after coercion)\n",
        "    if numeric_values.isna().any():\n",
        "        # Identify indices where values are non-numeric\n",
        "        invalid_indices = raw_expert_survey_df.index[numeric_values.isna()].tolist()\n",
        "        raise ValueError(f\"Non-numeric saaty_scale_value found at indices: {invalid_indices}\")\n",
        "\n",
        "    # Define the allowed Saaty scale set\n",
        "    allowed_saaty_values: Set[int] = {1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
        "\n",
        "    # Check if values are in the allowed set\n",
        "    # We check against the numeric series. We use np.isin for efficient checking.\n",
        "    # Note: This implicitly handles the check for integers, as 3.0 is in {1..9} but 3.5 is not.\n",
        "    is_valid = numeric_values.isin(allowed_saaty_values)\n",
        "\n",
        "    if not is_valid.all():\n",
        "        # Extract invalid values\n",
        "        invalid_values = raw_expert_survey_df.loc[~is_valid, \"saaty_scale_value\"].unique()\n",
        "        raise ValueError(\n",
        "            f\"Invalid saaty_scale_value entries found: {invalid_values}. \"\n",
        "            f\"Must be integers in {allowed_saaty_values}.\"\n",
        "        )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_raw_expert_survey(raw_expert_survey_df: pd.DataFrame) -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 2: Validates the schema and content domains of the raw expert survey DataFrame.\n",
        "\n",
        "    Sequentially executes column presence checks, categorical domain validation, and Saaty scale validation.\n",
        "\n",
        "    Args:\n",
        "        raw_expert_survey_df (pd.DataFrame): The raw expert survey data.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all validations pass.\n",
        "\n",
        "    Raises:\n",
        "        TypeError, ValueError: If any validation step fails.\n",
        "    \"\"\"\n",
        "    # Step 1: Check DataFrame type and column presence\n",
        "    validate_survey_columns(raw_expert_survey_df)\n",
        "\n",
        "    # Step 2: Validate domain of categorical columns\n",
        "    validate_survey_categorical_domains(raw_expert_survey_df)\n",
        "\n",
        "    # Step 3: Validate numeric range of Saaty scale values\n",
        "    validate_saaty_scale_values(raw_expert_survey_df)\n",
        "\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "rL0QAjUFUqNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 – Validate combinatorial completeness of expert pairwise comparisons\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Validate combinatorial completeness of expert pairwise comparisons\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Define canonical criterion sets per hierarchy level\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def get_canonical_hierarchy_criteria() -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Returns the canonical mapping of hierarchy levels to their ordered criterion sets.\n",
        "\n",
        "    This function defines the ground truth for the AHP structure as specified in the\n",
        "    LaTeX context. The order of criteria in these lists is fixed and determines the\n",
        "    row/column indices for the AHP matrices.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, List[str]]: A dictionary where keys are hierarchy levels and values\n",
        "        are lists of criterion labels.\n",
        "    \"\"\"\n",
        "    # Define the hierarchy structure based on the study's taxonomy\n",
        "    # Level 1: Main Criteria (4 items)\n",
        "    # Level 2: Sub-criteria for each main criterion\n",
        "    hierarchy: Dict[str, List[str]] = {\n",
        "        \"Main_Criteria\": [\"CSR\", \"LR\", \"IR\", \"CFR\"],\n",
        "        \"Sub_CSR\": [\n",
        "            \"CSR1\", \"CSR2\", \"CSR3\", \"CSR4\", \"CSR5\",\n",
        "            \"CSR6\", \"CSR7\", \"CSR8\", \"CSR9\", \"CSR10\", \"CSR11\"\n",
        "        ],\n",
        "        \"Sub_LR\": [\"LR1\", \"LR2\", \"LR3\"],\n",
        "        \"Sub_IR\": [\"IR1\", \"IR2\", \"IR3\", \"IR4\", \"IR5\", \"IR6\"],\n",
        "        \"Sub_CFR\": [\n",
        "            \"CFR1\", \"CFR2\", \"CFR3\", \"CFR4\", \"CFR5\",\n",
        "            \"CFR6\", \"CFR7\", \"CFR8\", \"CFR9\", \"CFR10\",\n",
        "            \"CFR11\", \"CFR12\", \"CFR13\", \"CFR14\"\n",
        "        ]\n",
        "    }\n",
        "    return hierarchy\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Validate criterion labels against canonical sets\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_criterion_labels(raw_expert_survey_df: pd.DataFrame, hierarchy: Dict[str, List[str]]) -> None:\n",
        "    \"\"\"\n",
        "    Validates that every criterion label in the survey DataFrame belongs to the\n",
        "    canonical set defined for its hierarchy level.\n",
        "\n",
        "    This ensures that there are no typos, misclassified criteria, or unknown labels\n",
        "    in the input data.\n",
        "\n",
        "    Args:\n",
        "        raw_expert_survey_df (pd.DataFrame): The raw expert survey data.\n",
        "        hierarchy (Dict[str, List[str]]): The canonical hierarchy mapping.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a criterion label is invalid for its hierarchy level.\n",
        "    \"\"\"\n",
        "    # Iterate over each hierarchy level present in the canonical definition\n",
        "    # We filter the dataframe by level to check validity in batches\n",
        "    for level, valid_criteria in hierarchy.items():\n",
        "        # Convert valid criteria list to a set for O(1) lookup\n",
        "        valid_set: Set[str] = set(valid_criteria)\n",
        "\n",
        "        # Filter dataframe for the current level\n",
        "        # We assume the dataframe has been cleaned/normalized (Task 4) or we check raw values\n",
        "        # Here we check raw values, so we must match the case exactly or rely on prior cleaning.\n",
        "        # The prompt implies we are validating the *raw* df, but strict validation implies\n",
        "        # we expect correct inputs. We will check exact matches.\n",
        "        level_mask = raw_expert_survey_df[\"hierarchy_level\"] == level\n",
        "        subset = raw_expert_survey_df[level_mask]\n",
        "\n",
        "        if subset.empty:\n",
        "            continue\n",
        "\n",
        "        # Check criterion_i column\n",
        "        # Identify rows where criterion_i is NOT in the valid set\n",
        "        invalid_i = ~subset[\"criterion_i\"].isin(valid_set)\n",
        "        if invalid_i.any():\n",
        "            invalid_labels = subset.loc[invalid_i, \"criterion_i\"].unique()\n",
        "            raise ValueError(\n",
        "                f\"Invalid 'criterion_i' labels found in level '{level}': {invalid_labels}. \"\n",
        "                f\"Expected one of {valid_criteria}.\"\n",
        "            )\n",
        "\n",
        "        # Check criterion_j column\n",
        "        # Identify rows where criterion_j is NOT in the valid set\n",
        "        invalid_j = ~subset[\"criterion_j\"].isin(valid_set)\n",
        "        if invalid_j.any():\n",
        "            invalid_labels = subset.loc[invalid_j, \"criterion_j\"].unique()\n",
        "            raise ValueError(\n",
        "                f\"Invalid 'criterion_j' labels found in level '{level}': {invalid_labels}. \"\n",
        "                f\"Expected one of {valid_criteria}.\"\n",
        "            )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Check cardinality of pairwise comparisons per (expert, level)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_pairwise_cardinality(raw_expert_survey_df: pd.DataFrame, hierarchy: Dict[str, List[str]]) -> None:\n",
        "    \"\"\"\n",
        "    Validates that for every expert and hierarchy level, the number of unique pairwise\n",
        "    comparisons matches exactly the combinatorial requirement n*(n-1)/2.\n",
        "\n",
        "    This ensures the survey data is complete (no missing comparisons) and minimal\n",
        "    (no duplicates or self-comparisons).\n",
        "\n",
        "    Args:\n",
        "        raw_expert_survey_df (pd.DataFrame): The raw expert survey data.\n",
        "        hierarchy (Dict[str, List[str]]): The canonical hierarchy mapping.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the count of unique pairs is incorrect for any expert/level.\n",
        "    \"\"\"\n",
        "    # Group data by expert and hierarchy level\n",
        "    grouped = raw_expert_survey_df.groupby([\"expert_id\", \"hierarchy_level\"])\n",
        "\n",
        "    # Iterate through each group to verify cardinality\n",
        "    for (expert_id, level), group in grouped:\n",
        "        # Retrieve the canonical criteria for this level\n",
        "        if level not in hierarchy:\n",
        "            # This should have been caught by domain validation, but as a safeguard:\n",
        "            raise ValueError(f\"Unknown hierarchy level '{level}' found for expert '{expert_id}'\")\n",
        "\n",
        "        criteria_list = hierarchy[level]\n",
        "        n = len(criteria_list)\n",
        "\n",
        "        # Calculate expected number of unique unordered pairs\n",
        "        # Equation: N = n * (n - 1) / 2\n",
        "        expected_pairs_count = (n * (n - 1)) // 2\n",
        "\n",
        "        # Extract pairs from the group\n",
        "        # We canonicalize pairs to handle (A, B) vs (B, A) equivalence by sorting\n",
        "        # We use a set comprehension to automatically filter exact duplicates\n",
        "        actual_pairs: Set[Tuple[str, str]] = {\n",
        "            tuple(sorted((row.criterion_i, row.criterion_j)))\n",
        "            for row in group.itertuples(index=False)\n",
        "            if row.criterion_i != row.criterion_j # Exclude self-comparisons if any slipped through\n",
        "        }\n",
        "\n",
        "        actual_count = len(actual_pairs)\n",
        "\n",
        "        # Check for completeness\n",
        "        if actual_count != expected_pairs_count:\n",
        "            raise ValueError(\n",
        "                f\"Combinatorial incompleteness for Expert '{expert_id}' at Level '{level}'. \"\n",
        "                f\"Expected {expected_pairs_count} unique pairs, found {actual_count}. \"\n",
        "                f\"Criteria count n={n}.\"\n",
        "            )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_combinatorial_completeness(raw_expert_survey_df: pd.DataFrame) -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 3: Validates the combinatorial integrity of the expert survey.\n",
        "\n",
        "    1. Retrieves the canonical hierarchy definition.\n",
        "    2. Validates that all criterion labels match the canonical definitions.\n",
        "    3. Validates that every expert has provided exactly the required number of unique pairwise comparisons.\n",
        "\n",
        "    Args:\n",
        "        raw_expert_survey_df (pd.DataFrame): The raw expert survey data.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all validations pass.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any validation step fails.\n",
        "    \"\"\"\n",
        "    # Step 1: Define canonical criterion sets\n",
        "    hierarchy = get_canonical_hierarchy_criteria()\n",
        "\n",
        "    # Step 2: Validate criterion labels against canonical sets\n",
        "    validate_criterion_labels(raw_expert_survey_df, hierarchy)\n",
        "\n",
        "    # Step 3: Check cardinality of pairwise comparisons\n",
        "    validate_pairwise_cardinality(raw_expert_survey_df, hierarchy)\n",
        "\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "GjsK6SapVnhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 – Clean and standardize raw_expert_survey_df\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Clean and standardize `raw_expert_survey_df`\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Remove invalid or degenerate rows\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def remove_degenerate_survey_rows(raw_expert_survey_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Removes degenerate rows from the expert survey DataFrame.\n",
        "\n",
        "    Specifically:\n",
        "    1. Removes rows where criterion_i == criterion_j (self-comparisons), as the\n",
        "       diagonal of AHP matrices is implicitly 1.\n",
        "    2. Validates that no rows have invalid saaty_scale_values (though Task 2\n",
        "       should have caught this, we enforce it here as a fatal error rather than\n",
        "       dropping, to preserve combinatorial integrity).\n",
        "\n",
        "    Args:\n",
        "        raw_expert_survey_df (pd.DataFrame): The raw expert survey data.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The cleaned DataFrame with self-comparisons removed.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If invalid saaty_scale_values are detected (data corruption).\n",
        "    \"\"\"\n",
        "    df = raw_expert_survey_df.copy()\n",
        "\n",
        "    # Identify self-comparisons\n",
        "    # Logic: AHP matrices have 1s on the diagonal. Explicit rows for i==j are redundant.\n",
        "    self_comparison_mask = df[\"criterion_i\"] == df[\"criterion_j\"]\n",
        "\n",
        "    if self_comparison_mask.any():\n",
        "        # Log or print could be added here in a real system\n",
        "        # Drop these rows\n",
        "        df = df[~self_comparison_mask].copy()\n",
        "\n",
        "    # Check for invalid Saaty values (1-9 integers)\n",
        "    # We assume Task 2 validation passed, but we double check for safety before processing\n",
        "    valid_saaty_values = {1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
        "    # Coerce to numeric to handle potential object types, errors='coerce' turns non-numeric to NaN\n",
        "    numeric_scales = pd.to_numeric(df[\"saaty_scale_value\"], errors='coerce')\n",
        "\n",
        "    # Check for NaNs (non-numeric) or values not in set\n",
        "    is_valid_scale = numeric_scales.isin(valid_saaty_values)\n",
        "\n",
        "    if not is_valid_scale.all():\n",
        "        invalid_rows = df[~is_valid_scale]\n",
        "        raise ValueError(\n",
        "            f\"Found {len(invalid_rows)} rows with invalid 'saaty_scale_value'. \"\n",
        "            f\"Values must be integers 1-9. Invalid values: {invalid_rows['saaty_scale_value'].unique()}\"\n",
        "        )\n",
        "\n",
        "    # Reset index after dropping rows to maintain clean indexing\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Normalize string fields to canonical case and strip whitespace\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_survey_strings(cleaned_survey_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalizes string columns in the survey DataFrame.\n",
        "\n",
        "    Operations:\n",
        "    1. Strips leading/trailing whitespace from 'expert_id', 'hierarchy_level',\n",
        "       'criterion_i', 'criterion_j', 'comparison_type'.\n",
        "    2. Does NOT force uppercase, as the canonical hierarchy keys (Task 3) are mixed case\n",
        "       (e.g., 'Main_Criteria', 'Sub_CSR'). We preserve case to ensure matching.\n",
        "\n",
        "    Args:\n",
        "        cleaned_survey_df (pd.DataFrame): The DataFrame from Step 1.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with normalized string columns.\n",
        "    \"\"\"\n",
        "    df = cleaned_survey_df.copy()\n",
        "\n",
        "    # List of string columns to normalize\n",
        "    string_cols = [\"expert_id\", \"hierarchy_level\", \"criterion_i\", \"criterion_j\", \"comparison_type\"]\n",
        "\n",
        "    for col in string_cols:\n",
        "        if col in df.columns:\n",
        "            # Ensure column is string type before stripping\n",
        "            # .astype(str) handles potential mixed types or numbers-as-strings\n",
        "            df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "            # Note: We explicitly do NOT .upper() here because 'Main_Criteria' != 'MAIN_CRITERIA'\n",
        "            # and we must match the canonical keys defined in Task 3.\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Re-verify completeness after cleaning\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def verify_post_cleaning_integrity(cleaned_survey_df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Re-runs the combinatorial completeness validation on the cleaned DataFrame.\n",
        "\n",
        "    This ensures that removing degenerate rows (like self-comparisons) did not\n",
        "    accidentally create gaps in the required pairwise comparison set.\n",
        "\n",
        "    Args:\n",
        "        cleaned_survey_df (pd.DataFrame): The fully cleaned and standardized DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the cleaned data fails combinatorial validation.\n",
        "    \"\"\"\n",
        "    # We reuse the orchestrator from Task 3.\n",
        "    # Since we are in the same environment, we assume validate_combinatorial_completeness is available.\n",
        "    # If this were a separate module, we would import it.\n",
        "\n",
        "    # Call Task 3 orchestrator\n",
        "    # This will raise ValueError if integrity is compromised\n",
        "    validate_combinatorial_completeness(cleaned_survey_df)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def clean_and_standardize_survey(raw_expert_survey_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 4: Cleans and standardizes the expert survey data.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Remove degenerate rows (self-comparisons).\n",
        "    2. Normalize string formats (strip whitespace).\n",
        "    3. Re-verify combinatorial completeness to ensure data integrity.\n",
        "\n",
        "    Args:\n",
        "        raw_expert_survey_df (pd.DataFrame): The raw input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The cleaned, standardized, and validated DataFrame ready for AHP matrix construction.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If data integrity checks fail.\n",
        "    \"\"\"\n",
        "    # Step 1: Remove degenerate rows\n",
        "    df_no_degenerate = remove_degenerate_survey_rows(raw_expert_survey_df)\n",
        "\n",
        "    # Step 2: Normalize string fields\n",
        "    df_normalized = normalize_survey_strings(df_no_degenerate)\n",
        "\n",
        "    # Step 3: Re-verify completeness\n",
        "    # This acts as a gatekeeper; if cleaning broke the data, we fail here.\n",
        "    verify_post_cleaning_integrity(df_normalized)\n",
        "\n",
        "    return df_normalized\n"
      ],
      "metadata": {
        "id": "RvXbW0NJWk0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5 – Validate the raw_financial_statement_df DataFrame schema\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Validate the `raw_financial_statement_df` DataFrame schema\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Check DataFrame type and index\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_financial_structure(raw_financial_statement_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validates the structural integrity of the financial statement DataFrame.\n",
        "\n",
        "    Checks:\n",
        "    1. Input is a pandas DataFrame.\n",
        "    2. 'fiscal_year' exists (either as column or index).\n",
        "    3. Coerces 'fiscal_year' to Int64 and sets it as a column (resetting index if needed).\n",
        "    4. Sorts the DataFrame by 'fiscal_year' ascending.\n",
        "\n",
        "    Args:\n",
        "        raw_financial_statement_df (pd.DataFrame): The raw financial data.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The structurally validated DataFrame sorted by fiscal year.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If input is not a DataFrame.\n",
        "        ValueError: If 'fiscal_year' is missing.\n",
        "    \"\"\"\n",
        "    # Check type\n",
        "    if not isinstance(raw_financial_statement_df, pd.DataFrame):\n",
        "        raise TypeError(f\"Input must be a pandas DataFrame, got {type(raw_financial_statement_df)}\")\n",
        "\n",
        "    df = raw_financial_statement_df.copy()\n",
        "\n",
        "    # Handle fiscal_year index vs column\n",
        "    if \"fiscal_year\" not in df.columns:\n",
        "        if df.index.name == \"fiscal_year\":\n",
        "            df.reset_index(inplace=True)\n",
        "        else:\n",
        "            raise ValueError(\"DataFrame must have a 'fiscal_year' column or index.\")\n",
        "\n",
        "    # Coerce fiscal_year to integer\n",
        "    try:\n",
        "        df[\"fiscal_year\"] = df[\"fiscal_year\"].astype(\"Int64\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Could not convert 'fiscal_year' to integer: {e}\")\n",
        "\n",
        "    # Sort by year for time-series consistency\n",
        "    df.sort_values(\"fiscal_year\", ascending=True, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 2: Validate column presence against schema\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_financial_columns(raw_financial_statement_df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates that the DataFrame contains all required base financial line items.\n",
        "\n",
        "    Note: Derived fields 'net_working_capital' and 'net_profit_before_interest'\n",
        "    are NOT enforced here, as they are computed in Task 6. However, their\n",
        "    source components MUST be present.\n",
        "\n",
        "    Args:\n",
        "        raw_financial_statement_df (pd.DataFrame): The financial data.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any required base column is missing.\n",
        "    \"\"\"\n",
        "    # Define required base columns (25 items)\n",
        "    # These are the raw inputs needed to compute the 34 ratios\n",
        "    required_base_columns: Set[str] = {\n",
        "        # Balance Sheet\n",
        "        \"total_assets\", \"current_assets\", \"inventory\", \"cash_and_equivalents\",\n",
        "        \"net_fixed_assets\", \"total_debt\", \"long_term_debt\", \"short_term_debt\",\n",
        "        \"current_liabilities\", \"shareholders_equity\", \"retained_earnings\",\n",
        "\n",
        "        # Income Statement\n",
        "        \"sales_revenue\", \"gross_profit\", \"ebit\", \"interest_expense\", \"net_profit\",\n",
        "\n",
        "        # Cash Flow Statement\n",
        "        \"net_operating_cash_flow\", \"capital_expenditures\", \"net_investing_cash_flow\",\n",
        "        \"net_financing_cash_flow\", \"total_cash_flow_inv_fin\", \"cash_distributions\",\n",
        "        \"operating_cash_inflows\", \"initial_cash_requirements\"\n",
        "    }\n",
        "\n",
        "    actual_columns = set(raw_financial_statement_df.columns)\n",
        "    missing_columns = required_base_columns - actual_columns\n",
        "\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"Missing required financial base columns: {missing_columns}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 3: Validate temporal coverage and uniqueness\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_temporal_coverage(raw_financial_statement_df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates that the financial data covers exactly the required time horizon\n",
        "    (2008-2017) with no duplicates.\n",
        "\n",
        "    Args:\n",
        "        raw_financial_statement_df (pd.DataFrame): The financial data.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If years are missing, extra years exist, or duplicates are found.\n",
        "    \"\"\"\n",
        "    # Define expected years\n",
        "    expected_years = set(range(2008, 2018)) # 2008 to 2017 inclusive\n",
        "\n",
        "    # Extract actual years\n",
        "    actual_years = set(raw_financial_statement_df[\"fiscal_year\"].dropna().unique())\n",
        "\n",
        "    # Check for missing years\n",
        "    missing_years = expected_years - actual_years\n",
        "    if missing_years:\n",
        "        raise ValueError(f\"Missing fiscal years: {missing_years}. Expected 2008-2017.\")\n",
        "\n",
        "    # Check for extra years (strict replication mode)\n",
        "    extra_years = actual_years - expected_years\n",
        "    if extra_years:\n",
        "        raise ValueError(f\"Unexpected extra fiscal years found: {extra_years}. Expected only 2008-2017.\")\n",
        "\n",
        "    # Check for duplicates\n",
        "    # We check the length of the dataframe against the number of unique years\n",
        "    # Since we already filtered to expected years (implicitly via the set check),\n",
        "    # if len(df) > len(actual_years), we have duplicates.\n",
        "    if len(raw_financial_statement_df) != len(actual_years):\n",
        "        # Find specific duplicates\n",
        "        duplicates = raw_financial_statement_df[raw_financial_statement_df.duplicated(\"fiscal_year\")]\n",
        "        raise ValueError(f\"Duplicate entries found for fiscal years: {duplicates['fiscal_year'].unique()}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_financial_statements(raw_financial_statement_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 5: Validates the schema and temporal integrity of financial statements.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Structural validation (type, index, sorting).\n",
        "    2. Column presence validation (base fields).\n",
        "    3. Temporal coverage validation (2008-2017 uniqueness).\n",
        "\n",
        "    Args:\n",
        "        raw_financial_statement_df (pd.DataFrame): The raw financial data.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The validated, sorted DataFrame with 'fiscal_year' as a column.\n",
        "\n",
        "    Raises:\n",
        "        ValueError, TypeError: If validation fails.\n",
        "    \"\"\"\n",
        "    # Step 1: Structure and Sort\n",
        "    df_structured = validate_financial_structure(raw_financial_statement_df)\n",
        "\n",
        "    # Step 2: Column Presence\n",
        "    validate_financial_columns(df_structured)\n",
        "\n",
        "    # Step 3: Temporal Integrity\n",
        "    validate_temporal_coverage(df_structured)\n",
        "\n",
        "    return df_structured\n"
      ],
      "metadata": {
        "id": "V1zsdmf7XmhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6 – Validate financial identities and enforce derived equalities\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Validate financial identities and enforce derived equalities\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 1: Check accounting identity for total debt\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_total_debt_identity(raw_financial_statement_df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the accounting identity: Total Debt = Short Term Debt + Long Term Debt.\n",
        "\n",
        "    This ensures that the debt figures reported in the financial statements are\n",
        "    internally consistent before being used for ratio calculation.\n",
        "\n",
        "    Args:\n",
        "        raw_financial_statement_df (pd.DataFrame): The financial data.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the identity is violated for any fiscal year.\n",
        "    \"\"\"\n",
        "    df = raw_financial_statement_df\n",
        "\n",
        "    # Extract relevant columns\n",
        "    total_debt = df[\"total_debt\"]\n",
        "    short_term = df[\"short_term_debt\"]\n",
        "    long_term = df[\"long_term_debt\"]\n",
        "\n",
        "    # Calculate expected total debt\n",
        "    # We fill NaNs with 0 for the check if one component is missing but total exists?\n",
        "    # No, strict validation implies if data is present it must match.\n",
        "    # If components are NaN, the sum is NaN.\n",
        "    calculated_total = short_term + long_term\n",
        "\n",
        "    # Check for mismatches where all values are present\n",
        "    # We use a small epsilon for floating point comparisons\n",
        "    # Mask for rows where all three are non-NaN\n",
        "    valid_mask = total_debt.notna() & short_term.notna() & long_term.notna()\n",
        "\n",
        "    if not valid_mask.any():\n",
        "        # If no rows have full debt data, we can't validate, but we don't fail unless required.\n",
        "        # However, Task 5 ensured columns exist.\n",
        "        return\n",
        "\n",
        "    # Compare\n",
        "    # We use np.isclose for robust float comparison\n",
        "    matches = np.isclose(total_debt[valid_mask], calculated_total[valid_mask], rtol=1e-5, atol=1e-5)\n",
        "\n",
        "    if not matches.all():\n",
        "        mismatched_indices = df.loc[valid_mask][~matches].index\n",
        "        mismatched_years = df.loc[mismatched_indices, \"fiscal_year\"].tolist()\n",
        "        raise ValueError(\n",
        "            f\"Total Debt identity violated for years: {mismatched_years}. \"\n",
        "            f\"Total Debt must equal Short Term + Long Term Debt.\"\n",
        "        )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 2: Derive net working capital if not provided\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def derive_net_working_capital(raw_financial_statement_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Derives or enforces 'net_working_capital' based on Current Assets and Current Liabilities.\n",
        "\n",
        "    Formula: Net Working Capital = Current Assets - Current Liabilities.\n",
        "\n",
        "    If the column exists, it is overwritten to ensure consistency with the components\n",
        "    used in other ratios (LR2, etc.).\n",
        "\n",
        "    Args:\n",
        "        raw_financial_statement_df (pd.DataFrame): The financial data.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with the 'net_working_capital' column enforced.\n",
        "    \"\"\"\n",
        "    df = raw_financial_statement_df.copy()\n",
        "\n",
        "    # Calculate NWC\n",
        "    # Equation: NWC = CA - CL\n",
        "    computed_nwc = df[\"current_assets\"] - df[\"current_liabilities\"]\n",
        "\n",
        "    # Overwrite or create the column\n",
        "    # We do this unconditionally to guarantee algebraic consistency\n",
        "    df[\"net_working_capital\"] = computed_nwc\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 3: Derive net profit before interest\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def derive_net_profit_before_interest(raw_financial_statement_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Derives or enforces 'net_profit_before_interest'.\n",
        "\n",
        "    Formula: Net Profit Before Interest = Net Profit + Interest Expense.\n",
        "\n",
        "    This specific definition is required for Income Risk ratios IR1 and IR5\n",
        "    as per the study's methodology.\n",
        "\n",
        "    Args:\n",
        "        raw_financial_statement_df (pd.DataFrame): The financial data.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with the 'net_profit_before_interest' column enforced.\n",
        "    \"\"\"\n",
        "    df = raw_financial_statement_df.copy()\n",
        "\n",
        "    # Calculate NPBI\n",
        "    # Equation: NPBI = Net Profit + Interest Expense\n",
        "    # Note: This adds back interest to get the pre-interest profit\n",
        "    computed_npbi = df[\"net_profit\"] + df[\"interest_expense\"]\n",
        "\n",
        "    # Overwrite or create the column\n",
        "    df[\"net_profit_before_interest\"] = computed_npbi\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def enforce_financial_identities(raw_financial_statement_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 6: Validates identities and enforces derived fields.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Validates Total Debt = Short + Long Term Debt.\n",
        "    2. Derives/Enforces Net Working Capital.\n",
        "    3. Derives/Enforces Net Profit Before Interest.\n",
        "\n",
        "    Args:\n",
        "        raw_financial_statement_df (pd.DataFrame): The validated financial data from Task 5.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The financial data with consistent derived columns.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If accounting identities are violated.\n",
        "    \"\"\"\n",
        "    # Step 1: Validate Debt Identity\n",
        "    # This is a check; it does not modify the dataframe\n",
        "    validate_total_debt_identity(raw_financial_statement_df)\n",
        "\n",
        "    # Step 2: Derive Net Working Capital\n",
        "    # This modifies the dataframe\n",
        "    df_nwc = derive_net_working_capital(raw_financial_statement_df)\n",
        "\n",
        "    # Step 3: Derive Net Profit Before Interest\n",
        "    # This modifies the dataframe further\n",
        "    df_final = derive_net_profit_before_interest(df_nwc)\n",
        "\n",
        "    return df_final\n"
      ],
      "metadata": {
        "id": "alpCwRABYhSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7 – Handle missing values and zero denominators in financial data\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Handle missing values and zero denominators in financial data\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 1: Inspect for missing or anomalous values\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def inspect_financial_anomalies(raw_financial_statement_df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Inspects the financial DataFrame for missing values (NaNs) and anomalous negative values\n",
        "    in fields that are strictly non-negative.\n",
        "\n",
        "    Critical fields (Total Assets, Shareholders' Equity) are checked strictly; missing values\n",
        "    there trigger a ValueError. Other anomalies are logged in the return dictionary.\n",
        "\n",
        "    Args:\n",
        "        raw_financial_statement_df (pd.DataFrame): The financial data.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A report containing counts of NaNs and negative values per column.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If critical fields contain NaNs or invalid negative values (e.g., negative Total Assets).\n",
        "    \"\"\"\n",
        "    df = raw_financial_statement_df\n",
        "    report = {\"nan_counts\": {}, \"negative_counts\": {}}\n",
        "\n",
        "    # Define fields that must strictly be non-negative\n",
        "    # Note: Equity can be negative in distress, but Assets cannot.\n",
        "    strictly_positive_fields = {\n",
        "        \"total_assets\", \"current_assets\", \"net_fixed_assets\",\n",
        "        \"total_debt\", \"long_term_debt\", \"short_term_debt\", \"current_liabilities\"\n",
        "    }\n",
        "\n",
        "    # Define critical fields where missing data is fatal for the model\n",
        "    critical_fields = {\"total_assets\", \"shareholders_equity\", \"net_operating_cash_flow\"}\n",
        "\n",
        "    for col in df.columns:\n",
        "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "            continue\n",
        "\n",
        "        # Check NaNs\n",
        "        nan_count = df[col].isna().sum()\n",
        "        if nan_count > 0:\n",
        "            report[\"nan_counts\"][col] = int(nan_count)\n",
        "            if col in critical_fields:\n",
        "                raise ValueError(f\"Critical field '{col}' has {nan_count} missing values.\")\n",
        "\n",
        "        # Check Negatives\n",
        "        if col in strictly_positive_fields:\n",
        "            neg_count = (df[col] < 0).sum()\n",
        "            if neg_count > 0:\n",
        "                report[\"negative_counts\"][col] = int(neg_count)\n",
        "                # Negative assets are physically impossible and indicate data error\n",
        "                if col == \"total_assets\":\n",
        "                    raise ValueError(f\"Total Assets contains {neg_count} negative values.\")\n",
        "\n",
        "    return report\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 2: Identify potential zero denominators for ratio computation\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def identify_zero_denominators(\n",
        "    raw_financial_statement_df: pd.DataFrame,\n",
        "    feature_engineering_logic: Dict[str, Dict[str, str]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Identifies (Year, Ratio) pairs where the denominator is effectively zero,\n",
        "    which would lead to division-by-zero errors or infinite ratios.\n",
        "\n",
        "    It evaluates the denominator expression for each ratio and checks against a\n",
        "    small epsilon tolerance.\n",
        "\n",
        "    Args:\n",
        "        raw_financial_statement_df (pd.DataFrame): The financial data.\n",
        "        feature_engineering_logic (Dict): Configuration mapping ratio IDs to numerator/denominator expressions.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A boolean DataFrame (index=fiscal_year, columns=ratio_ids)\n",
        "                      where True indicates a zero denominator.\n",
        "    \"\"\"\n",
        "    df = raw_financial_statement_df.set_index(\"fiscal_year\")\n",
        "    ratio_ids = list(feature_engineering_logic.keys())\n",
        "\n",
        "    # Initialize mask with False\n",
        "    zero_mask = pd.DataFrame(False, index=df.index, columns=ratio_ids)\n",
        "\n",
        "    epsilon = 1e-9\n",
        "\n",
        "    for ratio_id, logic in feature_engineering_logic.items():\n",
        "        den_expr = logic[\"denominator\"]\n",
        "\n",
        "        # Evaluate denominator expression\n",
        "        # We use pd.eval for safe evaluation of expressions like \"current_assets - inventory\"\n",
        "        # The local context is the dataframe columns\n",
        "        try:\n",
        "            # pd.eval works on the dataframe context\n",
        "            den_values = df.eval(den_expr)\n",
        "        except Exception as e:\n",
        "            # Fallback: if eval fails (e.g., simple column name with special chars?), try direct access\n",
        "            if den_expr in df.columns:\n",
        "                den_values = df[den_expr]\n",
        "            else:\n",
        "                raise ValueError(f\"Could not evaluate denominator '{den_expr}' for ratio '{ratio_id}': {e}\")\n",
        "\n",
        "        # Check for near-zero values\n",
        "        # We use abs() to handle negative denominators (which are valid but not if 0)\n",
        "        is_zero = den_values.abs() < epsilon\n",
        "\n",
        "        if is_zero.any():\n",
        "            zero_mask[ratio_id] = is_zero\n",
        "\n",
        "    return zero_mask\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 3: Decide on policy for zero-denominator ratios\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def apply_zero_denominator_policy(zero_mask: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Formalizes the policy for handling zero denominators.\n",
        "\n",
        "    Policy:\n",
        "    - If the denominator is zero, the ratio value is undefined (NaN).\n",
        "    - We do NOT substitute epsilon, as this distorts the magnitude.\n",
        "    - We do NOT set to zero, as that implies low risk/value which might be false.\n",
        "\n",
        "    This function returns the mask to be used by the ratio computer to force NaNs.\n",
        "    It serves as a documentation and enforcement point for this policy.\n",
        "\n",
        "    Args:\n",
        "        zero_mask (pd.DataFrame): The boolean mask from Step 2.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The validated mask to be used for NaN enforcement.\n",
        "    \"\"\"\n",
        "    # The policy is strictly to treat these as NaNs.\n",
        "    # We return the mask as-is, but this function exists to make the decision explicit\n",
        "    # in the pipeline architecture.\n",
        "    return zero_mask\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def handle_missing_and_zero_values(\n",
        "    raw_financial_statement_df: pd.DataFrame,\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Tuple[Dict[str, Any], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 7: Detects data anomalies and zero-denominator conditions.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Inspects for missing/negative values in critical fields.\n",
        "    2. Identifies ratios where the denominator evaluates to zero.\n",
        "    3. Returns a diagnostic report and a mask of zero-denominator entries.\n",
        "\n",
        "    Args:\n",
        "        raw_financial_statement_df (pd.DataFrame): The financial data.\n",
        "        study_configuration (Dict): The full study configuration containing feature logic.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict, pd.DataFrame]:\n",
        "            - Diagnostic report (dict).\n",
        "            - Zero denominator mask (DataFrame, True = zero/undefined).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If critical data quality issues are found.\n",
        "    \"\"\"\n",
        "    # Step 1: Inspect Anomalies\n",
        "    report = inspect_financial_anomalies(raw_financial_statement_df)\n",
        "\n",
        "    # Step 2: Identify Zero Denominators\n",
        "    feature_logic = study_configuration[\"feature_engineering_logic\"]\n",
        "    zero_mask_raw = identify_zero_denominators(raw_financial_statement_df, feature_logic)\n",
        "\n",
        "    # Step 3: Apply Policy\n",
        "    zero_mask_final = apply_zero_denominator_policy(zero_mask_raw)\n",
        "\n",
        "    return report, zero_mask_final\n"
      ],
      "metadata": {
        "id": "v_mJqg7wZcnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8 – Define the hierarchical structure and criterion sets for AHP\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Define the hierarchical structure and criterion sets for AHP\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 1, 2, 3: Define Hierarchy Class and Structures\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class AHPHierarchy:\n",
        "    \"\"\"\n",
        "    Encapsulates the hierarchical structure of the Analytic Hierarchy Process (AHP) model\n",
        "    for Financial Risk Assessment.\n",
        "\n",
        "    This class serves as the immutable, single source of truth for the structural definitions\n",
        "    required by the AHP methodology. It defines the decomposition of the financial risk\n",
        "    problem into a goal, main criteria, and sub-criteria (ratios), and provides O(1)\n",
        "    access to structural relationships.\n",
        "\n",
        "    Attributes:\n",
        "        levels (Dict[str, List[str]]):\n",
        "            A dictionary mapping hierarchy level identifiers (e.g., \"Main_Criteria\", \"Sub_CSR\")\n",
        "            to their corresponding ordered lists of criterion labels. The order defined here\n",
        "            is canonical and determines the row/column indices for pairwise comparison matrices.\n",
        "\n",
        "        parent_mapping (Dict[str, str]):\n",
        "            A dictionary mapping each secondary criterion label (e.g., \"CSR1\") to its\n",
        "            parent main criterion label (e.g., \"CSR\"). This is essential for computing\n",
        "            global weights via hierarchical composition:\n",
        "            w_global = w_main * w_local.\n",
        "\n",
        "        canonical_ratio_order (List[str]):\n",
        "            The fixed, global ordered list of all 34 secondary criteria (ratios). This order\n",
        "            is used to align the columns of the decision matrix X with the global weight vector w.\n",
        "\n",
        "        label_to_index (Dict[str, Dict[str, int]]):\n",
        "            A nested dictionary mapping hierarchy levels to a mapping of criterion labels\n",
        "            to their integer indices (0 to n-1). Used for efficient matrix population.\n",
        "    \"\"\"\n",
        "\n",
        "    # Default factory defines the static structure derived from the study's taxonomy.\n",
        "    levels: Dict[str, List[str]] = field(default_factory=lambda: {\n",
        "        \"Main_Criteria\": [\"CSR\", \"LR\", \"IR\", \"CFR\"],\n",
        "        \"Sub_CSR\": [\n",
        "            \"CSR1\", \"CSR2\", \"CSR3\", \"CSR4\", \"CSR5\",\n",
        "            \"CSR6\", \"CSR7\", \"CSR8\", \"CSR9\", \"CSR10\", \"CSR11\"\n",
        "        ],\n",
        "        \"Sub_LR\": [\"LR1\", \"LR2\", \"LR3\"],\n",
        "        \"Sub_IR\": [\"IR1\", \"IR2\", \"IR3\", \"IR4\", \"IR5\", \"IR6\"],\n",
        "        \"Sub_CFR\": [\n",
        "            \"CFR1\", \"CFR2\", \"CFR3\", \"CFR4\", \"CFR5\",\n",
        "            \"CFR6\", \"CFR7\", \"CFR8\", \"CFR9\", \"CFR10\",\n",
        "            \"CFR11\", \"CFR12\", \"CFR13\", \"CFR14\"\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    # Fields initialized in __post_init__\n",
        "    parent_mapping: Dict[str, str] = field(init=False)\n",
        "    canonical_ratio_order: List[str] = field(init=False)\n",
        "    label_to_index: Dict[str, Dict[str, int]] = field(init=False)\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        \"\"\"\n",
        "        Initializes derived structural attributes (`parent_mapping`, `canonical_ratio_order`,\n",
        "        `label_to_index`) based on the `levels` definition.\n",
        "\n",
        "        This method constructs the reverse mappings and global orderings required for\n",
        "        efficient lookups during the AHP and SAW calculation phases.\n",
        "        \"\"\"\n",
        "        # 1. Build Parent Mapping and Global Order\n",
        "        # We map the sub-level keys to the main criterion codes they represent.\n",
        "        # This relationship undergirds the hierarchical weight composition.\n",
        "        level_to_parent_code: Dict[str, str] = {\n",
        "            \"Sub_CSR\": \"CSR\",\n",
        "            \"Sub_LR\": \"LR\",\n",
        "            \"Sub_IR\": \"IR\",\n",
        "            \"Sub_CFR\": \"CFR\"\n",
        "        }\n",
        "\n",
        "        mapping: Dict[str, str] = {}\n",
        "        ratio_order: List[str] = []\n",
        "\n",
        "        # Iterate through sub-levels in a deterministic order to build the global list.\n",
        "        # The order of keys here determines the order of columns in the final decision matrix.\n",
        "        sub_levels: List[str] = [\"Sub_CSR\", \"Sub_LR\", \"Sub_IR\", \"Sub_CFR\"]\n",
        "\n",
        "        for sub_level in sub_levels:\n",
        "            # Identify the parent main criterion for this group\n",
        "            parent_code = level_to_parent_code[sub_level]\n",
        "\n",
        "            # Retrieve the list of secondary criteria for this group\n",
        "            criteria = self.levels[sub_level]\n",
        "\n",
        "            # Extend the global canonical order list\n",
        "            ratio_order.extend(criteria)\n",
        "\n",
        "            # Populate the parent mapping for each criterion in the group\n",
        "            for criterion in criteria:\n",
        "                mapping[criterion] = parent_code\n",
        "\n",
        "        # Use object.__setattr__ to bypass frozen dataclass immutability constraints during initialization\n",
        "        object.__setattr__(self, 'parent_mapping', mapping)\n",
        "        object.__setattr__(self, 'canonical_ratio_order', ratio_order)\n",
        "\n",
        "        # 2. Build Label to Index Mapping for each level\n",
        "        # This allows O(1) lookup of the matrix index (row/col) for a given criterion label.\n",
        "        # Structure: {level_name: {criterion_label: index}}\n",
        "        l_to_i: Dict[str, Dict[str, int]] = {}\n",
        "\n",
        "        for level, criteria in self.levels.items():\n",
        "            l_to_i[level] = {label: idx for idx, label in enumerate(criteria)}\n",
        "\n",
        "        object.__setattr__(self, 'label_to_index', l_to_i)\n",
        "\n",
        "    def get_criteria(self, level: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Retrieves the ordered list of criteria labels for a specific hierarchy level.\n",
        "\n",
        "        This list defines the canonical order of rows and columns for the pairwise\n",
        "        comparison matrix associated with the specified level.\n",
        "\n",
        "        Args:\n",
        "            level (str): The hierarchy level identifier (e.g., \"Main_Criteria\", \"Sub_CSR\").\n",
        "\n",
        "        Returns:\n",
        "            List[str]: The ordered list of criterion labels.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the provided `level` key does not exist in the hierarchy definition.\n",
        "        \"\"\"\n",
        "        # Validate that the requested level exists in the hierarchy\n",
        "        if level not in self.levels:\n",
        "            raise ValueError(f\"Unknown hierarchy level: '{level}'. Available levels: {list(self.levels.keys())}\")\n",
        "\n",
        "        return self.levels[level]\n",
        "\n",
        "    def get_matrix_size(self, level: str) -> int:\n",
        "        \"\"\"\n",
        "        Returns the dimension (n) of the pairwise comparison matrix for a given hierarchy level.\n",
        "\n",
        "        This corresponds to the number of criteria being compared at that specific node\n",
        "        of the hierarchy.\n",
        "\n",
        "        Args:\n",
        "            level (str): The hierarchy level identifier.\n",
        "\n",
        "        Returns:\n",
        "            int: The number of criteria at that level (n).\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the provided `level` key does not exist (propagated from get_criteria).\n",
        "        \"\"\"\n",
        "        # Retrieve criteria list (validation happens inside get_criteria)\n",
        "        criteria_list = self.get_criteria(level)\n",
        "\n",
        "        # Return the length of the list\n",
        "        return len(criteria_list)\n",
        "\n",
        "    def get_parent(self, secondary_criterion: str) -> str:\n",
        "        \"\"\"\n",
        "        Returns the parent main criterion label for a given secondary criterion (ratio).\n",
        "\n",
        "        This lookup is used during the calculation of global weights, where the local weight\n",
        "        of a secondary criterion is multiplied by the weight of its parent main criterion.\n",
        "\n",
        "        Args:\n",
        "            secondary_criterion (str): The secondary criterion label (e.g., \"CSR1\").\n",
        "\n",
        "        Returns:\n",
        "            str: The label of the parent main criterion (e.g., \"CSR\").\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the provided criterion label is not a known secondary criterion.\n",
        "        \"\"\"\n",
        "        # Validate that the criterion exists in the parent mapping\n",
        "        if secondary_criterion not in self.parent_mapping:\n",
        "            raise ValueError(f\"Unknown secondary criterion: '{secondary_criterion}'. Cannot determine parent.\")\n",
        "\n",
        "        return self.parent_mapping[secondary_criterion]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def initialize_ahp_hierarchy() -> AHPHierarchy:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 8: Instantiates and returns the AHP Hierarchy configuration object.\n",
        "\n",
        "    This object contains all structural definitions required for AHP matrix construction\n",
        "    and weight aggregation.\n",
        "\n",
        "    Returns:\n",
        "        AHPHierarchy: The configured hierarchy object.\n",
        "    \"\"\"\n",
        "    return AHPHierarchy()\n"
      ],
      "metadata": {
        "id": "LRhhOTAjaX16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9 – Build pairwise comparison matrices for each (expert, hierarchy_level) combination\n",
        "\n",
        "# ==========================================================================================\n",
        "# Task 9: Build pairwise comparison matrices for each (expert, hierarchy_level) combination\n",
        "# ==========================================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 1: Initialize square matrices with diagonal ones\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def initialize_matrices(\n",
        "    cleaned_survey_df: pd.DataFrame,\n",
        "    hierarchy: 'AHPHierarchy'\n",
        ") -> Dict[str, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Initializes empty pairwise comparison matrices for every expert and hierarchy level\n",
        "    found in the survey data.\n",
        "\n",
        "    Matrices are initialized with NaNs off-diagonal and 1.0 on the diagonal.\n",
        "\n",
        "    Args:\n",
        "        cleaned_survey_df (pd.DataFrame): The validated expert survey data.\n",
        "        hierarchy (AHPHierarchy): The configuration object containing structure definitions.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, np.ndarray]]: A nested dictionary structure:\n",
        "            matrices[expert_id][hierarchy_level] -> np.ndarray (n x n)\n",
        "    \"\"\"\n",
        "    matrices: Dict[str, Dict[str, np.ndarray]] = {}\n",
        "\n",
        "    # Identify all unique (expert, level) combinations\n",
        "    groups = cleaned_survey_df.groupby([\"expert_id\", \"hierarchy_level\"])\n",
        "\n",
        "    for (expert_id, level), _ in groups:\n",
        "        # Ensure the expert dict exists\n",
        "        if expert_id not in matrices:\n",
        "            matrices[expert_id] = {}\n",
        "\n",
        "        # Get matrix dimension n for this level\n",
        "        # We use the hierarchy object to get the canonical size\n",
        "        try:\n",
        "            n = hierarchy.get_matrix_size(level)\n",
        "        except ValueError as e:\n",
        "            raise ValueError(f\"Unknown hierarchy level '{level}' in survey data: {e}\")\n",
        "\n",
        "        # Initialize n x n matrix with NaNs\n",
        "        # We use float64 for precision\n",
        "        mat = np.full((n, n), np.nan, dtype=np.float64)\n",
        "\n",
        "        # Set diagonal to 1.0\n",
        "        np.fill_diagonal(mat, 1.0)\n",
        "\n",
        "        matrices[expert_id][level] = mat\n",
        "\n",
        "    return matrices\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 2: Populate direct entries from `raw_expert_survey_df`\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def populate_direct_entries(\n",
        "    matrices: Dict[str, Dict[str, np.ndarray]],\n",
        "    cleaned_survey_df: pd.DataFrame,\n",
        "    hierarchy: 'AHPHierarchy'\n",
        ") -> Dict[str, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Populates the initialized matrices with the direct pairwise judgments from the survey.\n",
        "\n",
        "    Maps criterion labels to integer indices using the hierarchy configuration.\n",
        "\n",
        "    Args:\n",
        "        matrices (Dict): The initialized nested dictionary of matrices.\n",
        "        cleaned_survey_df (pd.DataFrame): The survey data.\n",
        "        hierarchy (AHPHierarchy): The configuration object for label-to-index mapping.\n",
        "\n",
        "    Returns:\n",
        "        Dict: The matrices with direct entries filled.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a criterion label is not found in the hierarchy or if overwriting occurs.\n",
        "    \"\"\"\n",
        "    # Iterate over each row in the survey\n",
        "    # Using itertuples for performance, though dataset is small\n",
        "    for row in cleaned_survey_df.itertuples(index=False):\n",
        "        expert_id = row.expert_id\n",
        "        level = row.hierarchy_level\n",
        "        crit_i = row.criterion_i\n",
        "        crit_j = row.criterion_j\n",
        "        value = float(row.saaty_scale_value)\n",
        "\n",
        "        # Retrieve the specific matrix\n",
        "        # Note: initialize_matrices ensures existence, but we check for safety\n",
        "        if expert_id not in matrices or level not in matrices[expert_id]:\n",
        "            # This implies the grouping logic in step 1 missed something or df changed\n",
        "            raise ValueError(f\"Matrix not initialized for Expert '{expert_id}', Level '{level}'\")\n",
        "\n",
        "        mat = matrices[expert_id][level]\n",
        "\n",
        "        # Map labels to indices\n",
        "        # hierarchy.label_to_index is Dict[level, Dict[label, int]]\n",
        "        try:\n",
        "            idx_i = hierarchy.label_to_index[level][crit_i]\n",
        "            idx_j = hierarchy.label_to_index[level][crit_j]\n",
        "        except KeyError as e:\n",
        "            raise ValueError(f\"Criterion label mapping failed for '{e}' in level '{level}'\")\n",
        "\n",
        "        # Assign value\n",
        "        # Check for overwrite (should be caught by validation, but defensive coding)\n",
        "        if not np.isnan(mat[idx_i, idx_j]) and mat[idx_i, idx_j] != value:\n",
        "             # If it's the diagonal, it must be 1.0. If input says otherwise, it's an error.\n",
        "             # But we filtered self-comparisons in Task 4.\n",
        "             # So this implies duplicate rows for (i, j).\n",
        "             raise ValueError(\n",
        "                 f\"Duplicate conflicting entry for Expert '{expert_id}', Level '{level}', \"\n",
        "                 f\"Pair ({crit_i}, {crit_j}). Existing: {mat[idx_i, idx_j]}, New: {value}\"\n",
        "             )\n",
        "\n",
        "        mat[idx_i, idx_j] = value\n",
        "\n",
        "    return matrices\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 3: Enforce reciprocity to fill the lower (or upper) triangle\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def enforce_reciprocity(matrices: Dict[str, Dict[str, np.ndarray]]) -> Dict[str, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Completes the pairwise comparison matrices by enforcing the reciprocity property:\n",
        "    A_ji = 1 / A_ij.\n",
        "\n",
        "    Also validates that the resulting matrices are fully populated (no NaNs).\n",
        "\n",
        "    Args:\n",
        "        matrices (Dict): The partially populated matrices.\n",
        "\n",
        "    Returns:\n",
        "        Dict: The fully populated, reciprocal matrices.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a matrix cannot be fully populated (missing pairs) or contains conflicts.\n",
        "    \"\"\"\n",
        "    for expert_id, levels_dict in matrices.items():\n",
        "        for level, mat in levels_dict.items():\n",
        "            n = mat.shape[0]\n",
        "\n",
        "            # Iterate over the upper triangle (excluding diagonal)\n",
        "            # We assume inputs might be in upper or lower, so we check both\n",
        "            for i in range(n):\n",
        "                for j in range(i + 1, n):\n",
        "                    val_ij = mat[i, j]\n",
        "                    val_ji = mat[j, i]\n",
        "\n",
        "                    has_ij = not np.isnan(val_ij)\n",
        "                    has_ji = not np.isnan(val_ji)\n",
        "\n",
        "                    if has_ij and not has_ji:\n",
        "                        # Fill reciprocal\n",
        "                        mat[j, i] = 1.0 / val_ij\n",
        "                    elif not has_ij and has_ji:\n",
        "                        # Fill reciprocal\n",
        "                        mat[i, j] = 1.0 / val_ji\n",
        "                    elif has_ij and has_ji:\n",
        "                        # Both set: check consistency\n",
        "                        # Reciprocity check: val_ij * val_ji should be approx 1.0\n",
        "                        if not np.isclose(val_ij * val_ji, 1.0, rtol=1e-5):\n",
        "                            raise ValueError(\n",
        "                                f\"Reciprocity violation for Expert '{expert_id}', Level '{level}', \"\n",
        "                                f\"Indices ({i}, {j}). Values: {val_ij}, {val_ji}\"\n",
        "                            )\n",
        "                    else:\n",
        "                        # Neither set: Missing pair\n",
        "                        # We raise error after the loop to catch all, or here immediately.\n",
        "                        # Immediate failure is clearer for debugging specific pairs.\n",
        "                        raise ValueError(\n",
        "                            f\"Missing pairwise comparison for Expert '{expert_id}', Level '{level}', \"\n",
        "                            f\"Indices ({i}, {j}). Matrix is incomplete.\"\n",
        "                        )\n",
        "\n",
        "            # Final check for any remaining NaNs (should be covered above, but as safeguard)\n",
        "            if np.isnan(mat).any():\n",
        "                raise ValueError(f\"Matrix for Expert '{expert_id}', Level '{level}' contains NaNs after reciprocity enforcement.\")\n",
        "\n",
        "    return matrices\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_ahp_matrices(\n",
        "    cleaned_survey_df: pd.DataFrame,\n",
        "    hierarchy: 'AHPHierarchy'\n",
        ") -> Dict[str, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 9: Constructs full AHP pairwise comparison matrices.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Initialize n x n matrices for all expert/level combinations.\n",
        "    2. Populate matrices with direct survey judgments.\n",
        "    3. Enforce reciprocity (A_ji = 1/A_ij) to fill missing triangles and validate completeness.\n",
        "\n",
        "    Args:\n",
        "        cleaned_survey_df (pd.DataFrame): The validated and cleaned survey data.\n",
        "        hierarchy (AHPHierarchy): The hierarchy configuration object.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, np.ndarray]]: A nested dictionary of fully constructed AHP matrices.\n",
        "            Structure: result[expert_id][hierarchy_level] -> np.ndarray\n",
        "    \"\"\"\n",
        "    # Step 1: Initialize\n",
        "    matrices_init = initialize_matrices(cleaned_survey_df, hierarchy)\n",
        "\n",
        "    # Step 2: Populate Direct\n",
        "    matrices_populated = populate_direct_entries(matrices_init, cleaned_survey_df, hierarchy)\n",
        "\n",
        "    # Step 3: Enforce Reciprocity\n",
        "    matrices_final = enforce_reciprocity(matrices_populated)\n",
        "\n",
        "    return matrices_final\n"
      ],
      "metadata": {
        "id": "7oMCFI3gbl10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10 – Compute column normalization and local weight vectors\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Compute column normalization and local weight vectors\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 1: Compute column sums for each matrix\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_column_sums(matrices: Dict[str, Dict[str, np.ndarray]]) -> Dict[str, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Computes the column sums for each pairwise comparison matrix.\n",
        "\n",
        "    This is the first step of the AHP Row Average Method. It also validates that\n",
        "    matrices contain no NaNs and that column sums are positive (valid positive reciprocal matrices).\n",
        "\n",
        "    Args:\n",
        "        matrices (Dict): Nested dictionary of AHP matrices [expert][level] -> np.ndarray.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Nested dictionary of column sum vectors [expert][level] -> np.ndarray (1D).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a matrix contains NaNs or has non-positive column sums.\n",
        "    \"\"\"\n",
        "    column_sums: Dict[str, Dict[str, np.ndarray]] = {}\n",
        "\n",
        "    for expert_id, levels in matrices.items():\n",
        "        column_sums[expert_id] = {}\n",
        "        for level, mat in levels.items():\n",
        "            # Check for NaNs (defensive, though Task 9 should have cleared them)\n",
        "            if np.isnan(mat).any():\n",
        "                raise ValueError(f\"Matrix for Expert '{expert_id}', Level '{level}' contains NaNs.\")\n",
        "\n",
        "            # Compute column sums\n",
        "            # axis=0 sums down the rows (collapsing to columns)\n",
        "            sums = np.sum(mat, axis=0)\n",
        "\n",
        "            # Validate positivity\n",
        "            if np.any(sums <= 0):\n",
        "                raise ValueError(f\"Non-positive column sums found for Expert '{expert_id}', Level '{level}'.\")\n",
        "\n",
        "            column_sums[expert_id][level] = sums\n",
        "\n",
        "    return column_sums\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 2: Normalize each column\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_matrices(\n",
        "    matrices: Dict[str, Dict[str, np.ndarray]],\n",
        "    column_sums: Dict[str, Dict[str, np.ndarray]]\n",
        ") -> Dict[str, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Normalizes each pairwise comparison matrix by dividing each element by its column sum.\n",
        "\n",
        "    After this step, the sum of each column in the normalized matrix should be exactly 1.0.\n",
        "\n",
        "    Args:\n",
        "        matrices (Dict): The original AHP matrices.\n",
        "        column_sums (Dict): The column sums computed in Step 1.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Nested dictionary of normalized matrices.\n",
        "    \"\"\"\n",
        "    normalized_matrices: Dict[str, Dict[str, np.ndarray]] = {}\n",
        "\n",
        "    for expert_id, levels in matrices.items():\n",
        "        normalized_matrices[expert_id] = {}\n",
        "        for level, mat in levels.items():\n",
        "            sums = column_sums[expert_id][level]\n",
        "\n",
        "            # Broadcasting division: (n, n) / (n,) divides each column j by sums[j]\n",
        "            norm_mat = mat / sums\n",
        "\n",
        "            # Verify normalization (sum of columns should be 1)\n",
        "            # We use a small epsilon for float comparison\n",
        "            check_sums = np.sum(norm_mat, axis=0)\n",
        "            if not np.allclose(check_sums, 1.0, atol=1e-10):\n",
        "                raise ValueError(f\"Normalization failed for Expert '{expert_id}', Level '{level}'. Column sums != 1.\")\n",
        "\n",
        "            normalized_matrices[expert_id][level] = norm_mat\n",
        "\n",
        "    return normalized_matrices\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 3: Compute local weight vector via row averaging\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_row_averages(normalized_matrices: Dict[str, Dict[str, np.ndarray]]) -> Dict[str, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Computes the local weight vector (eigenvector approximation) by averaging the rows\n",
        "    of the normalized matrix.\n",
        "\n",
        "    The resulting vector is re-normalized to ensure it sums exactly to 1.0.\n",
        "\n",
        "    Args:\n",
        "        normalized_matrices (Dict): The normalized matrices from Step 2.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Nested dictionary of local weight vectors [expert][level] -> np.ndarray (1D).\n",
        "    \"\"\"\n",
        "    local_weights: Dict[str, Dict[str, np.ndarray]] = {}\n",
        "\n",
        "    for expert_id, levels in normalized_matrices.items():\n",
        "        local_weights[expert_id] = {}\n",
        "        for level, norm_mat in levels.items():\n",
        "            # Compute row means\n",
        "            # axis=1 sums across columns (collapsing to rows)\n",
        "            weights = np.mean(norm_mat, axis=1)\n",
        "\n",
        "            # Re-normalize to ensure sum is exactly 1.0\n",
        "            # (Row averaging usually sums to 1, but float precision might drift)\n",
        "            total_weight = np.sum(weights)\n",
        "            if total_weight == 0:\n",
        "                 raise ValueError(f\"Zero weight vector for Expert '{expert_id}', Level '{level}'.\")\n",
        "\n",
        "            weights = weights / total_weight\n",
        "\n",
        "            local_weights[expert_id][level] = weights\n",
        "\n",
        "    return local_weights\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_ahp_local_weights(\n",
        "    matrices: Dict[str, Dict[str, np.ndarray]]\n",
        ") -> Tuple[Dict[str, Dict[str, np.ndarray]], Dict[str, Dict[str, np.ndarray]]]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 10: Computes local AHP weights using the Row Average Method.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Compute column sums of the original matrices.\n",
        "    2. Normalize matrices by column sums.\n",
        "    3. Compute row averages of normalized matrices to get local weights.\n",
        "\n",
        "    Args:\n",
        "        matrices (Dict): The fully constructed pairwise comparison matrices.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict, Dict]:\n",
        "            - local_weights: The computed weight vectors [expert][level] -> np.ndarray.\n",
        "            - column_sums: The column sums of original matrices (needed for Lambda_max calculation in Task 11).\n",
        "    \"\"\"\n",
        "    # Step 1: Column Sums\n",
        "    # We return these because Task 11 needs them for Lambda_max = dot(weights, column_sums)\n",
        "    col_sums = compute_column_sums(matrices)\n",
        "\n",
        "    # Step 2: Normalize\n",
        "    norm_matrices = normalize_matrices(matrices, col_sums)\n",
        "\n",
        "    # Step 3: Row Averages (Weights)\n",
        "    weights = compute_row_averages(norm_matrices)\n",
        "\n",
        "    return weights, col_sums\n"
      ],
      "metadata": {
        "id": "ypsArCTxdtll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11 – Compute maximum eigenvalue and consistency index (CI) for each matrix.\n",
        "\n",
        "import numpy as np\n",
        "from typing import Dict, Any, Tuple\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Compute maximum eigenvalue and consistency index (CI) for each matrix\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 1: Compute approximate maximum eigenvalue (Lambda_max)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_lambda_max(\n",
        "    local_weights: Dict[str, Dict[str, np.ndarray]],\n",
        "    column_sums: Dict[str, Dict[str, np.ndarray]]\n",
        ") -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Computes the approximate maximum eigenvalue (Lambda_max) for each AHP matrix.\n",
        "\n",
        "    Formula: Lambda_max = sum(w_i * S_i)\n",
        "    Where w_i is the local weight for criterion i, and S_i is the column sum for criterion i.\n",
        "\n",
        "    Args:\n",
        "        local_weights (Dict): Nested dictionary of local weight vectors.\n",
        "        column_sums (Dict): Nested dictionary of column sum vectors.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Nested dictionary of Lambda_max scalars [expert][level] -> float.\n",
        "    \"\"\"\n",
        "    lambda_max_values: Dict[str, Dict[str, float]] = {}\n",
        "\n",
        "    for expert_id, levels in local_weights.items():\n",
        "        lambda_max_values[expert_id] = {}\n",
        "        for level, weights in levels.items():\n",
        "            # Retrieve corresponding column sums\n",
        "            if expert_id not in column_sums or level not in column_sums[expert_id]:\n",
        "                raise ValueError(f\"Missing column sums for Expert '{expert_id}', Level '{level}'.\")\n",
        "\n",
        "            sums = column_sums[expert_id][level]\n",
        "\n",
        "            # Validate dimensions\n",
        "            if weights.shape != sums.shape:\n",
        "                raise ValueError(\n",
        "                    f\"Dimension mismatch for Expert '{expert_id}', Level '{level}': \"\n",
        "                    f\"weights {weights.shape} vs sums {sums.shape}.\"\n",
        "                )\n",
        "\n",
        "            # Compute dot product\n",
        "            l_max = np.dot(weights, sums)\n",
        "\n",
        "            lambda_max_values[expert_id][level] = float(l_max)\n",
        "\n",
        "    return lambda_max_values\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 2: Compute Consistency Index (CI)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_consistency_index(\n",
        "    lambda_max_values: Dict[str, Dict[str, float]],\n",
        "    hierarchy: 'AHPHierarchy'\n",
        ") -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Computes the Consistency Index (CI) for each AHP matrix.\n",
        "\n",
        "    Formula: CI = (Lambda_max - n) / (n - 1)\n",
        "\n",
        "    Args:\n",
        "        lambda_max_values (Dict): The computed Lambda_max values.\n",
        "        hierarchy (AHPHierarchy): The hierarchy configuration to retrieve matrix size n.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Nested dictionary of CI scalars [expert][level] -> float.\n",
        "    \"\"\"\n",
        "    ci_values: Dict[str, Dict[str, float]] = {}\n",
        "\n",
        "    for expert_id, levels in lambda_max_values.items():\n",
        "        ci_values[expert_id] = {}\n",
        "        for level, l_max in levels.items():\n",
        "            # Get matrix size n\n",
        "            n = hierarchy.get_matrix_size(level)\n",
        "\n",
        "            if n <= 1:\n",
        "                # CI is 0 for 1x1 matrices (perfectly consistent)\n",
        "                ci = 0.0\n",
        "            else:\n",
        "                ci = (l_max - n) / (n - 1)\n",
        "\n",
        "            # Theoretical lower bound check (CI >= 0 for positive reciprocal matrices)\n",
        "            # Allow small epsilon for float noise\n",
        "            if ci < -1e-9:\n",
        "                 # This indicates a calculation error or invalid matrix properties\n",
        "                 raise ValueError(f\"Negative CI ({ci}) computed for Expert '{expert_id}', Level '{level}'.\")\n",
        "\n",
        "            ci_values[expert_id][level] = max(0.0, ci) # Clamp negative epsilon to 0\n",
        "\n",
        "    return ci_values\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 3: Retrieve Random Index (RI) from lookup table\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def retrieve_random_indices(\n",
        "    ci_values: Dict[str, Dict[str, float]],\n",
        "    hierarchy: 'AHPHierarchy',\n",
        "    ri_lookup: Dict[int, float]\n",
        ") -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Retrieves the Random Index (RI) for each matrix based on its order n.\n",
        "\n",
        "    Args:\n",
        "        ci_values (Dict): The CI values structure (used to iterate keys).\n",
        "        hierarchy (AHPHierarchy): The hierarchy configuration for matrix size n.\n",
        "        ri_lookup (Dict[int, float]): The Saaty RI lookup table {n: RI}.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Nested dictionary of RI scalars [expert][level] -> float.\n",
        "    \"\"\"\n",
        "    ri_values: Dict[str, Dict[str, float]] = {}\n",
        "\n",
        "    # Retrieve the Random Index (RI) for each matrix based on its order n.\n",
        "    for expert_id, levels in ci_values.items():\n",
        "        ri_values[expert_id] = {}\n",
        "        for level in levels:\n",
        "            n = hierarchy.get_matrix_size(level)\n",
        "\n",
        "            if n not in ri_lookup:\n",
        "                raise ValueError(f\"Matrix size n={n} not found in RI lookup table for Level '{level}'.\")\n",
        "\n",
        "            ri_values[expert_id][level] = ri_lookup[n]\n",
        "\n",
        "    return ri_values\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_ahp_consistency_metrics(\n",
        "    local_weights: Dict[str, Dict[str, np.ndarray]],\n",
        "    column_sums: Dict[str, Dict[str, np.ndarray]],\n",
        "    hierarchy: 'AHPHierarchy',\n",
        "    ahp_parameters: Dict[str, Any]\n",
        ") -> Tuple[Dict[str, Dict[str, float]], Dict[str, Dict[str, float]], Dict[str, Dict[str, float]]]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 11: Computes AHP consistency metrics (Lambda_max, CI, RI).\n",
        "\n",
        "    Pipeline:\n",
        "    1. Compute Lambda_max using weights and column sums.\n",
        "    2. Compute CI using Lambda_max and matrix size n.\n",
        "    3. Retrieve RI using matrix size n and the provided lookup table.\n",
        "\n",
        "    Args:\n",
        "        local_weights (Dict): Local weight vectors.\n",
        "        column_sums (Dict): Column sum vectors.\n",
        "        hierarchy (AHPHierarchy): Hierarchy configuration.\n",
        "        ahp_parameters (Dict): Configuration containing the 'random_index_lookup' table.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict, Dict, Dict]:\n",
        "            - lambda_max_values: [expert][level] -> float\n",
        "            - ci_values: [expert][level] -> float\n",
        "            - ri_values: [expert][level] -> float\n",
        "    \"\"\"\n",
        "    # Extract RI lookup table\n",
        "    ri_lookup = ahp_parameters.get(\"random_index_lookup\")\n",
        "    if not ri_lookup:\n",
        "        raise ValueError(\"ahp_parameters missing 'random_index_lookup'.\")\n",
        "\n",
        "    # Ensure keys are integers (JSON keys might be strings)\n",
        "    ri_lookup_int = {int(k): float(v) for k, v in ri_lookup.items()}\n",
        "\n",
        "    # Step 1: Lambda Max\n",
        "    lambda_max = compute_lambda_max(local_weights, column_sums)\n",
        "\n",
        "    # Step 2: Consistency Index\n",
        "    ci = compute_consistency_index(lambda_max, hierarchy)\n",
        "\n",
        "    # Step 3: Random Index\n",
        "    ri = retrieve_random_indices(ci, hierarchy, ri_lookup_int)\n",
        "\n",
        "    return lambda_max, ci, ri\n"
      ],
      "metadata": {
        "id": "GXXeN7Beey73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12 – Compute Consistency Ratio (CR) and filter acceptable matrices\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Compute Consistency Ratio (CR) and filter acceptable matrices\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 1: Compute Consistency Ratio (CR)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_consistency_ratio(\n",
        "    ci_values: Dict[str, Dict[str, float]],\n",
        "    ri_values: Dict[str, Dict[str, float]]\n",
        ") -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Computes the Consistency Ratio (CR) for each AHP matrix.\n",
        "\n",
        "    Formula: CR = CI / RI\n",
        "    If RI is 0 (for n=1, 2), CR is defined as 0.\n",
        "\n",
        "    Args:\n",
        "        ci_values (Dict): Nested dictionary of Consistency Indices [expert][level] -> float.\n",
        "        ri_values (Dict): Nested dictionary of Random Indices [expert][level] -> float.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Nested dictionary of Consistency Ratios [expert][level] -> float.\n",
        "    \"\"\"\n",
        "    cr_values: Dict[str, Dict[str, float]] = {}\n",
        "\n",
        "    # Compute the Consistency Ratio (CR) for each AHP matrix\n",
        "    for expert_id, levels in ci_values.items():\n",
        "        cr_values[expert_id] = {}\n",
        "        for level, ci in levels.items():\n",
        "            if expert_id not in ri_values or level not in ri_values[expert_id]:\n",
        "                raise ValueError(f\"Missing RI value for Expert '{expert_id}', Level '{level}'.\")\n",
        "\n",
        "            ri = ri_values[expert_id][level]\n",
        "\n",
        "            if ri == 0:\n",
        "                # For n=1 or n=2, RI is 0. Perfect consistency is assumed/enforced.\n",
        "                cr = 0.0\n",
        "            else:\n",
        "                cr = ci / ri\n",
        "\n",
        "            cr_values[expert_id][level] = cr\n",
        "\n",
        "    return cr_values\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 2: Apply consistency threshold and classify matrices\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def classify_matrices(\n",
        "    cr_values: Dict[str, Dict[str, float]],\n",
        "    threshold: float\n",
        ") -> Dict[str, Set[str]]:\n",
        "    \"\"\"\n",
        "    Classifies matrices as accepted or rejected based on the Consistency Ratio threshold.\n",
        "\n",
        "    Args:\n",
        "        cr_values (Dict): Nested dictionary of CR values.\n",
        "        threshold (float): The maximum allowed CR (typically 0.10).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Set[str]]: A dictionary mapping hierarchy levels to sets of accepted expert IDs.\n",
        "            Structure: {level: {expert_id1, expert_id2, ...}}\n",
        "    \"\"\"\n",
        "    accepted_experts: Dict[str, Set[str]] = {}\n",
        "\n",
        "    # Initialize sets for all levels found in the input\n",
        "    # We iterate to find all unique levels first\n",
        "    all_levels = set()\n",
        "    for levels in cr_values.values():\n",
        "        all_levels.update(levels.keys())\n",
        "\n",
        "    for level in all_levels:\n",
        "        accepted_experts[level] = set()\n",
        "\n",
        "    for expert_id, levels in cr_values.items():\n",
        "        for level, cr in levels.items():\n",
        "            # Strict inequality: CR < 0.10 is accepted\n",
        "            if cr < threshold:\n",
        "                accepted_experts[level].add(expert_id)\n",
        "            # Else: rejected (implicitly not added)\n",
        "\n",
        "    return accepted_experts\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 3: Verify that at least some experts remain per level\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def verify_accepted_experts(accepted_experts: Dict[str, Set[str]]) -> None:\n",
        "    \"\"\"\n",
        "    Verifies that at least one expert has been accepted for every hierarchy level.\n",
        "\n",
        "    If any level has zero accepted experts, the AHP aggregation cannot proceed.\n",
        "\n",
        "    Args:\n",
        "        accepted_experts (Dict): The mapping of levels to accepted expert sets.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any level has no accepted experts.\n",
        "    \"\"\"\n",
        "    for level, experts in accepted_experts.items():\n",
        "        if not experts:\n",
        "            raise ValueError(\n",
        "                f\"No experts passed the consistency check for Level '{level}'. \"\n",
        "                \"Cannot proceed with aggregation. Review expert judgments.\"\n",
        "            )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def filter_consistent_matrices(\n",
        "    ci_values: Dict[str, Dict[str, float]],\n",
        "    ri_values: Dict[str, Dict[str, float]],\n",
        "    ahp_parameters: Dict[str, Any]\n",
        ") -> Tuple[Dict[str, Dict[str, float]], Dict[str, Set[str]]]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 12: Computes CR and filters matrices based on consistency.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Compute CR = CI / RI.\n",
        "    2. Classify matrices as accepted/rejected based on threshold (0.10).\n",
        "    3. Verify that every level has at least one accepted expert.\n",
        "\n",
        "    Args:\n",
        "        ci_values (Dict): Consistency Indices.\n",
        "        ri_values (Dict): Random Indices.\n",
        "        ahp_parameters (Dict): Configuration containing 'consistency_threshold'.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict, Dict]:\n",
        "            - cr_values: [expert][level] -> float\n",
        "            - accepted_experts: [level] -> Set[expert_id]\n",
        "    \"\"\"\n",
        "    # Extract threshold\n",
        "    threshold_config = ahp_parameters.get(\"consistency_threshold\", {})\n",
        "    threshold = threshold_config.get(\"max_allowed_value\", 0.10)\n",
        "\n",
        "    # Step 1: Compute CR\n",
        "    cr_values = compute_consistency_ratio(ci_values, ri_values)\n",
        "\n",
        "    # Step 2: Classify\n",
        "    accepted_experts = classify_matrices(cr_values, threshold)\n",
        "\n",
        "    # Step 3: Verify\n",
        "    verify_accepted_experts(accepted_experts)\n",
        "\n",
        "    return cr_values, accepted_experts\n"
      ],
      "metadata": {
        "id": "mwnsmiQaf8Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13 – Aggregate accepted expert weights for the main criteria level\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Aggregate accepted expert weights for the main criteria level\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 1: Collect accepted weight vectors for main criteria\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def collect_main_weights(\n",
        "    local_weights: Dict[str, Dict[str, np.ndarray]],\n",
        "    accepted_experts: Dict[str, Set[str]]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Collects the local weight vectors for the 'Main_Criteria' level from all accepted experts.\n",
        "\n",
        "    Args:\n",
        "        local_weights (Dict): Nested dictionary of local weights [expert][level] -> np.ndarray.\n",
        "        accepted_experts (Dict): Dictionary mapping levels to sets of accepted expert IDs.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 2D array of shape (num_accepted_experts, 4) containing the weight vectors.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If no experts are accepted for 'Main_Criteria'.\n",
        "    \"\"\"\n",
        "    level = \"Main_Criteria\"\n",
        "\n",
        "    if level not in accepted_experts or not accepted_experts[level]:\n",
        "        raise ValueError(f\"No accepted experts found for level '{level}'. Cannot aggregate.\")\n",
        "\n",
        "    accepted_ids = accepted_experts[level]\n",
        "    weight_vectors: List[np.ndarray] = []\n",
        "\n",
        "    # Collect the local weight vectors for the 'Main_Criteria' level from all accepted expert\n",
        "    for expert_id in accepted_ids:\n",
        "        if expert_id not in local_weights or level not in local_weights[expert_id]:\n",
        "            raise ValueError(f\"Missing weights for accepted expert '{expert_id}' at level '{level}'.\")\n",
        "\n",
        "        w = local_weights[expert_id][level]\n",
        "        weight_vectors.append(w)\n",
        "\n",
        "    # Stack into (K, 4) matrix\n",
        "    return np.stack(weight_vectors, axis=0)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 2: Compute arithmetic mean of weight vectors\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_mean_weights(weight_matrix: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the arithmetic mean of the weight vectors across experts.\n",
        "\n",
        "    Args:\n",
        "        weight_matrix (np.ndarray): 2D array of weights (experts x criteria).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: 1D array of aggregated weights.\n",
        "    \"\"\"\n",
        "    # Compute mean along axis 0 (across experts)\n",
        "    mean_weights = np.mean(weight_matrix, axis=0)\n",
        "    return mean_weights\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 3: Normalize aggregated weights\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_aggregated_vector(weights: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Normalizes the aggregated weight vector to ensure it sums exactly to 1.0.\n",
        "\n",
        "    Args:\n",
        "        weights (np.ndarray): The raw aggregated weight vector.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The normalized weight vector.\n",
        "    \"\"\"\n",
        "    total = np.sum(weights)\n",
        "\n",
        "    if total == 0:\n",
        "        raise ValueError(\"Aggregated weight vector sums to zero.\")\n",
        "\n",
        "    return weights / total\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def aggregate_main_criteria_weights(\n",
        "    local_weights: Dict[str, Dict[str, np.ndarray]],\n",
        "    accepted_experts: Dict[str, Set[str]]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 13: Aggregates expert weights for the Main Criteria level.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Collect weight vectors from accepted experts.\n",
        "    2. Compute arithmetic mean.\n",
        "    3. Normalize to sum to 1.\n",
        "\n",
        "    Args:\n",
        "        local_weights (Dict): Local weights from Task 10.\n",
        "        accepted_experts (Dict): Accepted experts from Task 12.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The final aggregated global weight vector for Main Criteria (CSR, LR, IR, CFR).\n",
        "    \"\"\"\n",
        "    # Step 1: Collect\n",
        "    weight_matrix = collect_main_weights(local_weights, accepted_experts)\n",
        "\n",
        "    # Step 2: Mean\n",
        "    mean_weights = compute_mean_weights(weight_matrix)\n",
        "\n",
        "    # Step 3: Normalize\n",
        "    final_weights = normalize_aggregated_vector(mean_weights)\n",
        "\n",
        "    return final_weights\n"
      ],
      "metadata": {
        "id": "mzJlUYY6hG7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14 – Aggregate accepted expert weights for each sub-criteria level\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Aggregate accepted expert weights for each sub-criteria level\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 1 & 2: Generic Aggregation Function for Sub-Levels\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def aggregate_level_weights(\n",
        "    level: str,\n",
        "    local_weights: Dict[str, Dict[str, np.ndarray]],\n",
        "    accepted_experts: Dict[str, Set[str]]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Aggregates the local weight vectors for a specific hierarchy level from all accepted experts.\n",
        "\n",
        "    This function implements the arithmetic mean aggregation followed by normalization,\n",
        "    consistent with the AHP methodology for group decision making.\n",
        "\n",
        "    Args:\n",
        "        level (str): The hierarchy level to aggregate (e.g., \"Sub_CSR\").\n",
        "        local_weights (Dict): Nested dictionary of local weights [expert][level] -> np.ndarray.\n",
        "        accepted_experts (Dict): Dictionary mapping levels to sets of accepted expert IDs.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The aggregated, normalized weight vector for the specified level.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If no experts are accepted for the level or weights are missing.\n",
        "    \"\"\"\n",
        "    if level not in accepted_experts or not accepted_experts[level]:\n",
        "        raise ValueError(f\"No accepted experts found for level '{level}'. Cannot aggregate.\")\n",
        "\n",
        "    accepted_ids = accepted_experts[level]\n",
        "    weight_vectors: List[np.ndarray] = []\n",
        "\n",
        "    for expert_id in accepted_ids:\n",
        "        if expert_id not in local_weights or level not in local_weights[expert_id]:\n",
        "            raise ValueError(f\"Missing weights for accepted expert '{expert_id}' at level '{level}'.\")\n",
        "\n",
        "        w = local_weights[expert_id][level]\n",
        "        weight_vectors.append(w)\n",
        "\n",
        "    # Stack into (K, n) matrix\n",
        "    weight_matrix = np.stack(weight_vectors, axis=0)\n",
        "\n",
        "    # Compute arithmetic mean across experts\n",
        "    mean_weights = np.mean(weight_matrix, axis=0)\n",
        "\n",
        "    # Normalize to sum to 1.0\n",
        "    total = np.sum(mean_weights)\n",
        "    if total == 0:\n",
        "        raise ValueError(f\"Aggregated weight vector for level '{level}' sums to zero.\")\n",
        "\n",
        "    return mean_weights / total\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 3: Store aggregated local weights for all sub-criteria\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def map_sub_weights_to_main_groups(\n",
        "    aggregated_vectors: Dict[str, np.ndarray]\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Reorganizes the aggregated sub-level weight vectors by their parent main criterion code.\n",
        "\n",
        "    Mapping:\n",
        "    - \"Sub_CSR\" -> \"CSR\"\n",
        "    - \"Sub_LR\"  -> \"LR\"\n",
        "    - \"Sub_IR\"  -> \"IR\"\n",
        "    - \"Sub_CFR\" -> \"CFR\"\n",
        "\n",
        "    Args:\n",
        "        aggregated_vectors (Dict): Dictionary mapping hierarchy level names to weight vectors.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, np.ndarray]: Dictionary mapping main criterion codes to sub-criteria weight vectors.\n",
        "    \"\"\"\n",
        "    level_to_parent = {\n",
        "        \"Sub_CSR\": \"CSR\",\n",
        "        \"Sub_LR\": \"LR\",\n",
        "        \"Sub_IR\": \"IR\",\n",
        "        \"Sub_CFR\": \"CFR\"\n",
        "    }\n",
        "\n",
        "    mapped_weights: Dict[str, np.ndarray] = {}\n",
        "\n",
        "    # Reorganize the aggregated sub-level weight vectors by their parent main criterion code\n",
        "    for level, parent_code in level_to_parent.items():\n",
        "        if level not in aggregated_vectors:\n",
        "            raise ValueError(f\"Missing aggregated weights for level '{level}'.\")\n",
        "\n",
        "        mapped_weights[parent_code] = aggregated_vectors[level]\n",
        "\n",
        "    return mapped_weights\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def aggregate_all_sub_criteria_weights(\n",
        "    local_weights: Dict[str, Dict[str, np.ndarray]],\n",
        "    accepted_experts: Dict[str, Set[str]]\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 14: Aggregates expert weights for all sub-criteria levels.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Iterates through all sub-levels (Sub_CSR, Sub_LR, Sub_IR, Sub_CFR).\n",
        "    2. Aggregates weights for each level using accepted experts.\n",
        "    3. Maps the results to their parent main criterion codes.\n",
        "\n",
        "    Args:\n",
        "        local_weights (Dict): Local weights from Task 10.\n",
        "        accepted_experts (Dict): Accepted experts from Task 12.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, np.ndarray]: A dictionary mapping main criterion codes (CSR, LR, IR, CFR)\n",
        "                               to their aggregated sub-criteria weight vectors.\n",
        "    \"\"\"\n",
        "    sub_levels = [\"Sub_CSR\", \"Sub_LR\", \"Sub_IR\", \"Sub_CFR\"]\n",
        "    aggregated_vectors: Dict[str, np.ndarray] = {}\n",
        "\n",
        "    # Step 1 & 2: Aggregate per level\n",
        "    for level in sub_levels:\n",
        "        aggregated_vectors[level] = aggregate_level_weights(level, local_weights, accepted_experts)\n",
        "\n",
        "    # Step 3: Map to structure\n",
        "    final_sub_weights = map_sub_weights_to_main_groups(aggregated_vectors)\n",
        "\n",
        "    return final_sub_weights\n"
      ],
      "metadata": {
        "id": "g2CjEpPEiWa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15 – Compute global weights for all 34 financial ratios\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Compute global weights for all 34 financial ratios\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 1: Apply hierarchical weight composition\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def calculate_raw_global_weights(\n",
        "    main_weights: np.ndarray,\n",
        "    sub_weights: Dict[str, np.ndarray],\n",
        "    hierarchy: 'AHPHierarchy'\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Computes the raw global weight for each secondary criterion (ratio) by multiplying\n",
        "    its local weight by the weight of its parent main criterion.\n",
        "\n",
        "    Formula: w_global(s) = w_main(parent(s)) * w_local(s)\n",
        "\n",
        "    Args:\n",
        "        main_weights (np.ndarray): The aggregated weight vector for main criteria (CSR, LR, IR, CFR).\n",
        "        sub_weights (Dict): Dictionary mapping main criterion codes to aggregated sub-criteria weight vectors.\n",
        "        hierarchy (AHPHierarchy): The hierarchy configuration object.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: A dictionary mapping ratio labels to their raw global weights.\n",
        "    \"\"\"\n",
        "    # Map main criterion labels to their indices in the main_weights vector\n",
        "    # The order is defined in hierarchy.levels[\"Main_Criteria\"]\n",
        "    main_criteria_order = hierarchy.get_criteria(\"Main_Criteria\")\n",
        "    main_weight_map = {label: main_weights[i] for i, label in enumerate(main_criteria_order)}\n",
        "\n",
        "    raw_global_weights: Dict[str, float] = {}\n",
        "\n",
        "    # Iterate through all sub-levels to compute global weights\n",
        "    # We use the hierarchy to get the list of sub-criteria for each main group\n",
        "    # Note: sub_weights keys are main codes (CSR, LR, etc.)\n",
        "\n",
        "    # We need to know which sub-level corresponds to which main code to get the correct label order\n",
        "    # Mapping: CSR -> Sub_CSR, etc.\n",
        "    parent_to_level = {\n",
        "        \"CSR\": \"Sub_CSR\",\n",
        "        \"LR\": \"Sub_LR\",\n",
        "        \"IR\": \"Sub_IR\",\n",
        "        \"CFR\": \"Sub_CFR\"\n",
        "    }\n",
        "\n",
        "    for parent_code, local_vector in sub_weights.items():\n",
        "        # Get the weight of the parent main criterion\n",
        "        if parent_code not in main_weight_map:\n",
        "             raise ValueError(f\"Main criterion '{parent_code}' not found in main weights.\")\n",
        "\n",
        "        parent_weight = main_weight_map[parent_code]\n",
        "\n",
        "        # Get the ordered list of sub-criteria labels for this group\n",
        "        level_name = parent_to_level[parent_code]\n",
        "        sub_criteria_labels = hierarchy.get_criteria(level_name)\n",
        "\n",
        "        # Validate vector length\n",
        "        if len(local_vector) != len(sub_criteria_labels):\n",
        "            raise ValueError(\n",
        "                f\"Dimension mismatch for '{parent_code}': \"\n",
        "                f\"Vector len {len(local_vector)} != Labels len {len(sub_criteria_labels)}\"\n",
        "            )\n",
        "\n",
        "        # Compute global weights\n",
        "        for i, ratio_label in enumerate(sub_criteria_labels):\n",
        "            local_w = local_vector[i]\n",
        "            global_w = parent_weight * local_w\n",
        "            raw_global_weights[ratio_label] = global_w\n",
        "\n",
        "    return raw_global_weights\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 2: Assemble the global weight vector\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def assemble_global_vector(\n",
        "    raw_weights_map: Dict[str, float],\n",
        "    hierarchy: 'AHPHierarchy'\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Assembles the global weights into a single 1D array ordered according to the\n",
        "    canonical global ratio order defined in the hierarchy.\n",
        "\n",
        "    Args:\n",
        "        raw_weights_map (Dict): Dictionary of ratio labels to weights.\n",
        "        hierarchy (AHPHierarchy): Hierarchy configuration containing canonical_ratio_order.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The ordered global weight vector (length 34).\n",
        "    \"\"\"\n",
        "    canonical_order = hierarchy.canonical_ratio_order\n",
        "    vector_list = []\n",
        "\n",
        "    # Assemble the global weights into a single 1D array\n",
        "    for ratio_label in canonical_order:\n",
        "        if ratio_label not in raw_weights_map:\n",
        "            raise ValueError(f\"Missing global weight for ratio '{ratio_label}'.\")\n",
        "\n",
        "        vector_list.append(raw_weights_map[ratio_label])\n",
        "\n",
        "    return np.array(vector_list, dtype=np.float64)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 3: Normalize the global weight vector\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_final_global_vector(weights: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Normalizes the final global weight vector to ensure it sums exactly to 1.0.\n",
        "\n",
        "    Args:\n",
        "        weights (np.ndarray): The raw global weight vector.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The normalized global weight vector.\n",
        "    \"\"\"\n",
        "    total = np.sum(weights)\n",
        "\n",
        "    if total == 0:\n",
        "        raise ValueError(\"Global weight vector sums to zero.\")\n",
        "\n",
        "    return weights / total\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_global_weights(\n",
        "    main_weights: np.ndarray,\n",
        "    sub_weights: Dict[str, np.ndarray],\n",
        "    hierarchy: 'AHPHierarchy'\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 15: Computes the final global AHP weights for all 34 ratios.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Calculate raw global weights via hierarchical composition (Main * Local).\n",
        "    2. Assemble weights into a canonical vector.\n",
        "    3. Normalize the vector to sum to 1.\n",
        "\n",
        "    Args:\n",
        "        main_weights (np.ndarray): Aggregated main criteria weights.\n",
        "        sub_weights (Dict): Aggregated sub-criteria weights.\n",
        "        hierarchy (AHPHierarchy): Hierarchy configuration.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The final global weight vector (length 34), summing to 1.\n",
        "    \"\"\"\n",
        "    # Step 1: Hierarchical Composition\n",
        "    raw_map = calculate_raw_global_weights(main_weights, sub_weights, hierarchy)\n",
        "\n",
        "    # Step 2: Assemble Vector\n",
        "    raw_vector = assemble_global_vector(raw_map, hierarchy)\n",
        "\n",
        "    # Step 3: Normalize\n",
        "    final_vector = normalize_final_global_vector(raw_vector)\n",
        "\n",
        "    return final_vector\n"
      ],
      "metadata": {
        "id": "yQFgGq8Kkg4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16 – Define the computational logic for all 34 financial ratios\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Define the computational logic for all 34 financial ratios\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 1: Create a deterministic index ordering for the 34 ratios\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def get_canonical_ratio_order(hierarchy: 'AHPHierarchy') -> List[str]:\n",
        "    \"\"\"\n",
        "    Retrieves the deterministic order of the 34 financial ratios from the hierarchy object.\n",
        "\n",
        "    This order is used to align the columns of the decision matrix X with the global weight vector w.\n",
        "\n",
        "    Args:\n",
        "        hierarchy (AHPHierarchy): The hierarchy configuration object.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: The ordered list of 34 ratio identifiers (e.g., ['CSR1', ..., 'CFR14']).\n",
        "    \"\"\"\n",
        "    return hierarchy.canonical_ratio_order\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 2: Extract numerator and denominator variable names\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def extract_ratio_logic(\n",
        "    feature_engineering_logic: Dict[str, Dict[str, str]],\n",
        "    canonical_order: List[str]\n",
        ") -> Dict[str, Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Extracts and validates the numerator and denominator expressions for all ratios.\n",
        "\n",
        "    Args:\n",
        "        feature_engineering_logic (Dict): Configuration dictionary mapping ratio IDs to logic.\n",
        "        canonical_order (List[str]): The expected list of ratio IDs.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Tuple[str, str]]: A mapping of ratio ID -> (numerator_expr, denominator_expr).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If logic is missing for any ratio in the canonical order.\n",
        "    \"\"\"\n",
        "    ratio_specs: Dict[str, Tuple[str, str]] = {}\n",
        "\n",
        "    # Extract and validate the numerator and denominator expressions for all ratios\n",
        "    for ratio_id in canonical_order:\n",
        "        if ratio_id not in feature_engineering_logic:\n",
        "            raise ValueError(f\"Missing feature engineering logic for ratio '{ratio_id}'.\")\n",
        "\n",
        "        logic = feature_engineering_logic[ratio_id]\n",
        "\n",
        "        if \"numerator\" not in logic or \"denominator\" not in logic:\n",
        "             raise ValueError(f\"Incomplete logic for ratio '{ratio_id}': must have 'numerator' and 'denominator'.\")\n",
        "\n",
        "        ratio_specs[ratio_id] = (logic[\"numerator\"], logic[\"denominator\"])\n",
        "\n",
        "    return ratio_specs\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 3: Define a parser for compound numerator expressions\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def evaluate_expression(\n",
        "    expression: str,\n",
        "    df: pd.DataFrame\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Evaluates a string arithmetic expression against the columns of a DataFrame.\n",
        "\n",
        "    Supports basic arithmetic operators (+, -) and column references.\n",
        "    Uses pandas.eval for efficient vectorized evaluation.\n",
        "\n",
        "    Args:\n",
        "        expression (str): The arithmetic expression (e.g., \"current_assets - inventory\").\n",
        "        df (pd.DataFrame): The DataFrame containing the financial data.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: The result of the evaluation for each row.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the expression cannot be evaluated (e.g., invalid column name).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # pd.eval evaluates the string expression using columns in df\n",
        "        # engine='numexpr' is fast for large data, 'python' is safer for complex logic\n",
        "        # Given simple arithmetic, default engine is fine.\n",
        "        result = df.eval(expression)\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        # Fallback: check if it's a direct column name that failed eval for some reason (e.g. special chars)\n",
        "        if expression in df.columns:\n",
        "            return df[expression]\n",
        "\n",
        "        raise ValueError(f\"Failed to evaluate expression '{expression}': {e}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_ratio_computation_logic(\n",
        "    hierarchy: 'AHPHierarchy',\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Tuple[List[str], Dict[str, Tuple[str, str]]]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 16: Prepares the structural and logical definitions for ratio computation.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Retrieve canonical ratio order.\n",
        "    2. Extract and validate numerator/denominator logic for all ratios.\n",
        "    3. (The evaluator function is returned implicitly as a callable utility for the next task).\n",
        "\n",
        "    Args:\n",
        "        hierarchy (AHPHierarchy): The hierarchy configuration.\n",
        "        study_configuration (Dict): The full study configuration.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[str], Dict[str, Tuple[str, str]]]:\n",
        "            - canonical_order: List of 34 ratio IDs.\n",
        "            - ratio_specs: Mapping of ID -> (num_expr, den_expr).\n",
        "    \"\"\"\n",
        "    # Step 1: Order\n",
        "    canonical_order = get_canonical_ratio_order(hierarchy)\n",
        "\n",
        "    # Step 2: Logic Extraction\n",
        "    feature_logic = study_configuration[\"feature_engineering_logic\"]\n",
        "    ratio_specs = extract_ratio_logic(feature_logic, canonical_order)\n",
        "\n",
        "    return canonical_order, ratio_specs\n"
      ],
      "metadata": {
        "id": "q67XV2vpl6_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17 – Compute the raw decision matrix X of all ratios for all years\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Compute the raw decision matrix X of all ratios for all years\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 1: Initialize the decision matrix X\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def initialize_decision_matrix(\n",
        "    financial_df: pd.DataFrame,\n",
        "    canonical_order: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Initializes an empty decision matrix X with dimensions (T x n).\n",
        "\n",
        "    Rows correspond to fiscal years (sorted) and columns to the 34 financial ratios.\n",
        "    Initialized with NaNs.\n",
        "\n",
        "    Args:\n",
        "        financial_df (pd.DataFrame): The validated financial statements (source of index).\n",
        "        canonical_order (List[str]): The ordered list of 34 ratio identifiers.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: An empty DataFrame indexed by fiscal_year with ratio columns.\n",
        "    \"\"\"\n",
        "    # Ensure index is set to fiscal_year if not already\n",
        "    if \"fiscal_year\" in financial_df.columns:\n",
        "        years = financial_df[\"fiscal_year\"].unique()\n",
        "    else:\n",
        "        years = financial_df.index.unique()\n",
        "\n",
        "    # Sort years to ensure chronological order (2008 -> 2017)\n",
        "    sorted_years = sorted(years)\n",
        "\n",
        "    # Create DataFrame\n",
        "    X = pd.DataFrame(\n",
        "        np.nan,\n",
        "        index=sorted_years,\n",
        "        columns=canonical_order,\n",
        "        dtype=np.float64\n",
        "    )\n",
        "\n",
        "    X.index.name = \"fiscal_year\"\n",
        "    return X\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 2: Compute each ratio value x_tj\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def populate_decision_matrix(\n",
        "    X: pd.DataFrame,\n",
        "    financial_df: pd.DataFrame,\n",
        "    ratio_specs: Dict[str, Tuple[str, str]],\n",
        "    zero_mask: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the values for all financial ratios and populates the decision matrix.\n",
        "\n",
        "    For each ratio:\n",
        "    1. Evaluates the numerator and denominator expressions using the financial data.\n",
        "    2. Checks the zero-denominator mask.\n",
        "    3. Computes the ratio (Numerator / Denominator) where valid, else sets to NaN.\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): The initialized decision matrix.\n",
        "        financial_df (pd.DataFrame): The financial data.\n",
        "        ratio_specs (Dict): Mapping of ratio ID -> (numerator_expr, denominator_expr).\n",
        "        zero_mask (pd.DataFrame): Boolean mask where True indicates a zero denominator.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The populated decision matrix X.\n",
        "    \"\"\"\n",
        "    # Ensure financial_df is indexed by fiscal_year for alignment\n",
        "    if \"fiscal_year\" in financial_df.columns:\n",
        "        df_aligned = financial_df.set_index(\"fiscal_year\").sort_index()\n",
        "    else:\n",
        "        df_aligned = financial_df.sort_index()\n",
        "\n",
        "    # Iterate over each ratio in the canonical order (columns of X)\n",
        "    for ratio_id in X.columns:\n",
        "        if ratio_id not in ratio_specs:\n",
        "            raise ValueError(f\"Missing logic for ratio '{ratio_id}'\")\n",
        "\n",
        "        num_expr, den_expr = ratio_specs[ratio_id]\n",
        "\n",
        "        # Evaluate expressions\n",
        "        # We use the helper from Task 16 (assumed available in environment)\n",
        "        # Note: evaluate_expression was defined in Task 16.\n",
        "        numerator = evaluate_expression(num_expr, df_aligned)\n",
        "        denominator = evaluate_expression(den_expr, df_aligned)\n",
        "\n",
        "        # Compute ratio\n",
        "        # We align everything by index (fiscal_year)\n",
        "        ratio_values = numerator / denominator\n",
        "\n",
        "        # Apply zero denominator policy (force NaN)\n",
        "        # zero_mask is indexed by fiscal_year and has columns for ratios\n",
        "        if ratio_id in zero_mask.columns:\n",
        "            mask = zero_mask[ratio_id]\n",
        "            # Where mask is True, set to NaN\n",
        "            ratio_values.loc[mask] = np.nan\n",
        "\n",
        "        # Assign to matrix\n",
        "        X[ratio_id] = ratio_values\n",
        "\n",
        "    return X\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 3: Log and validate the decision matrix\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_decision_matrix(X: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Validates the populated decision matrix and generates summary statistics.\n",
        "\n",
        "    Checks for columns that are entirely NaN (which would break SAW normalization).\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): The populated decision matrix.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A summary dictionary containing NaN counts and descriptive stats.\n",
        "    \"\"\"\n",
        "    nan_counts = X.isna().sum()\n",
        "    all_nan_cols = nan_counts[nan_counts == len(X)].index.tolist()\n",
        "\n",
        "    if all_nan_cols:\n",
        "        # This is a warning condition in a real system, but we log it.\n",
        "        # In strict mode, we might want to halt, but for now we report.\n",
        "        pass\n",
        "\n",
        "    stats = X.describe().to_dict()\n",
        "\n",
        "    return {\n",
        "        \"nan_counts\": nan_counts.to_dict(),\n",
        "        \"all_nan_columns\": all_nan_cols,\n",
        "        \"statistics\": stats\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_decision_matrix(\n",
        "    financial_df: pd.DataFrame,\n",
        "    canonical_order: List[str],\n",
        "    ratio_specs: Dict[str, Tuple[str, str]],\n",
        "    zero_mask: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 17: Computes the raw decision matrix X.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Initialize empty matrix X.\n",
        "    2. Populate X by evaluating ratio logic against financial data.\n",
        "    3. Validate and summarize X.\n",
        "\n",
        "    Args:\n",
        "        financial_df (pd.DataFrame): Financial data.\n",
        "        canonical_order (List[str]): Ordered list of ratio IDs.\n",
        "        ratio_specs (Dict): Logic for ratios.\n",
        "        zero_mask (pd.DataFrame): Zero denominator mask.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict]: The decision matrix X and its validation report.\n",
        "    \"\"\"\n",
        "    # Step 1: Initialize\n",
        "    X_init = initialize_decision_matrix(financial_df, canonical_order)\n",
        "\n",
        "    # Step 2: Populate\n",
        "    X_populated = populate_decision_matrix(X_init, financial_df, ratio_specs, zero_mask)\n",
        "\n",
        "    # Step 3: Validate\n",
        "    report = validate_decision_matrix(X_populated)\n",
        "\n",
        "    return X_populated, report\n"
      ],
      "metadata": {
        "id": "oT_QDw8inKDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18 – Validate the decision matrix and enforce minimum variance constraints\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Validate the decision matrix and enforce minimum variance constraints\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 1: Check for zero-variance criteria\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def check_zero_variance(X: pd.DataFrame) -> List[str]:\n",
        "    \"\"\"\n",
        "    Identifies criteria (columns) in the decision matrix that have zero variance across years.\n",
        "\n",
        "    Zero-variance criteria provide no discriminatory power in SAW and would cause\n",
        "    division-by-zero errors during normalization (max - min = 0).\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): The decision matrix.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of column names with zero variance.\n",
        "    \"\"\"\n",
        "    zero_variance_cols = []\n",
        "\n",
        "    for col in X.columns:\n",
        "        # Skip columns that are all NaN\n",
        "        if X[col].isna().all():\n",
        "            continue\n",
        "\n",
        "        min_val = X[col].min()\n",
        "        max_val = X[col].max()\n",
        "\n",
        "        # Check if min is approximately equal to max\n",
        "        if np.isclose(min_val, max_val, atol=1e-9):\n",
        "            zero_variance_cols.append(col)\n",
        "\n",
        "    return zero_variance_cols\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 2: Inspect for outliers or unrealistic ratio values\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def detect_outliers(X: pd.DataFrame, threshold: float = 5.0) -> Dict[str, List[int]]:\n",
        "    \"\"\"\n",
        "    Detects statistical outliers in the decision matrix using the Z-score method.\n",
        "\n",
        "    Outliers are defined as values deviating from the mean by more than `threshold`\n",
        "    standard deviations. This is a diagnostic step; values are not modified.\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): The decision matrix.\n",
        "        threshold (float): The Z-score threshold for flagging outliers.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, List[int]]: A dictionary mapping column names to lists of fiscal years\n",
        "                              where outliers were detected.\n",
        "    \"\"\"\n",
        "    outliers = {}\n",
        "\n",
        "    # Detect statistical outliers in the decision matrix using the Z-score method\n",
        "    for col in X.columns:\n",
        "        series = X[col].dropna()\n",
        "        if len(series) < 2:\n",
        "            continue\n",
        "\n",
        "        mean = series.mean()\n",
        "        std = series.std()\n",
        "\n",
        "        if std == 0:\n",
        "            continue\n",
        "\n",
        "        z_scores = (series - mean) / std\n",
        "        outlier_mask = z_scores.abs() > threshold\n",
        "\n",
        "        if outlier_mask.any():\n",
        "            # Get the fiscal years (index) corresponding to outliers\n",
        "            outlier_years = series[outlier_mask].index.tolist()\n",
        "            outliers[col] = outlier_years\n",
        "\n",
        "    return outliers\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 3: Freeze the validated decision matrix\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def freeze_matrix(X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Finalizes the decision matrix for SAW processing and creates a validity mask.\n",
        "\n",
        "    The validity mask indicates which entries are non-NaN and thus valid for\n",
        "    normalization and weighting.\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): The validated decision matrix.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "            - The frozen decision matrix X (unchanged).\n",
        "            - A boolean DataFrame mask (True where data is valid/non-NaN).\n",
        "    \"\"\"\n",
        "    # Create validity mask (True where not NaN)\n",
        "    valid_mask = X.notna()\n",
        "\n",
        "    # Return X and the mask\n",
        "    # We return a copy of X to ensure immutability of the input if needed,\n",
        "    # though here we just pass it through as the \"frozen\" version.\n",
        "    return X.copy(), valid_mask\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_and_freeze_decision_matrix(X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 18: Validates and finalizes the decision matrix.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Check for zero-variance columns.\n",
        "    2. Detect statistical outliers.\n",
        "    3. Freeze the matrix and generate a validity mask.\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): The raw decision matrix from Task 17.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
        "            - Frozen decision matrix X.\n",
        "            - Validity mask.\n",
        "            - Diagnostic report (zero variance cols, outliers).\n",
        "    \"\"\"\n",
        "    # Step 1: Zero Variance\n",
        "    zero_var_cols = check_zero_variance(X)\n",
        "\n",
        "    # Step 2: Outliers\n",
        "    outliers = detect_outliers(X)\n",
        "\n",
        "    # Step 3: Freeze\n",
        "    X_final, valid_mask = freeze_matrix(X)\n",
        "\n",
        "    report = {\n",
        "        \"zero_variance_columns\": zero_var_cols,\n",
        "        \"outliers\": outliers\n",
        "    }\n",
        "\n",
        "    return X_final, valid_mask, report\n"
      ],
      "metadata": {
        "id": "ckV-LWvjojWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19 – Assign benefit/cost directionality to each criterion for SAW normalization\n",
        "\n",
        "# ====================================================================================\n",
        "# Task 19: Assign benefit/cost directionality to each criterion for SAW normalization\n",
        "# ====================================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 1: Retrieve directionality lists from configuration\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def retrieve_directionality_sets(\n",
        "    saw_parameters: Dict[str, Any]\n",
        ") -> Tuple[Set[str], Set[str]]:\n",
        "    \"\"\"\n",
        "    Retrieves the sets of benefit and cost criteria from the SAW configuration.\n",
        "\n",
        "    Args:\n",
        "        saw_parameters (Dict): The SAW parameters dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Set[str], Set[str]]:\n",
        "            - benefit_criteria: Set of ratio IDs where higher is better (lower risk).\n",
        "            - cost_criteria: Set of ratio IDs where higher is worse (higher risk).\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If required keys are missing from configuration.\n",
        "    \"\"\"\n",
        "    directionality = saw_parameters.get(\"criteria_directionality\", {})\n",
        "\n",
        "    if \"benefit_criteria_max\" not in directionality:\n",
        "        raise KeyError(\"Missing 'benefit_criteria_max' in SAW configuration.\")\n",
        "    if \"cost_criteria_min\" not in directionality:\n",
        "        raise KeyError(\"Missing 'cost_criteria_min' in SAW configuration.\")\n",
        "\n",
        "    benefit_set = set(directionality[\"benefit_criteria_max\"])\n",
        "    cost_set = set(directionality[\"cost_criteria_min\"])\n",
        "\n",
        "    return benefit_set, cost_set\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 2: Verify completeness and non-overlap\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_directionality_completeness(\n",
        "    benefit_set: Set[str],\n",
        "    cost_set: Set[str],\n",
        "    canonical_order: List[str]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates that the directionality sets form a partition of the canonical ratio set.\n",
        "\n",
        "    Checks:\n",
        "    1. Completeness: Union of sets equals the set of all ratios.\n",
        "    2. Non-overlap: Intersection of sets is empty.\n",
        "\n",
        "    Args:\n",
        "        benefit_set (Set[str]): Benefit criteria.\n",
        "        cost_set (Set[str]): Cost criteria.\n",
        "        canonical_order (List[str]): The list of all 34 ratio IDs.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If sets overlap or do not cover all ratios.\n",
        "    \"\"\"\n",
        "    all_ratios = set(canonical_order)\n",
        "    union_set = benefit_set.union(cost_set)\n",
        "    intersection_set = benefit_set.intersection(cost_set)\n",
        "\n",
        "    # Check completeness\n",
        "    missing = all_ratios - union_set\n",
        "    extra = union_set - all_ratios\n",
        "\n",
        "    if missing:\n",
        "        raise ValueError(f\"Directionality configuration missing ratios: {missing}\")\n",
        "    if extra:\n",
        "        raise ValueError(f\"Directionality configuration contains unknown ratios: {extra}\")\n",
        "\n",
        "    # Check non-overlap\n",
        "    if intersection_set:\n",
        "        raise ValueError(f\"Ratios assigned to both benefit and cost sets: {intersection_set}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 3: Construct a directionality lookup\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_directionality_map(\n",
        "    benefit_set: Set[str],\n",
        "    cost_set: Set[str]\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Constructs a lookup dictionary mapping each ratio ID to its directionality type.\n",
        "\n",
        "    Args:\n",
        "        benefit_set (Set[str]): Benefit criteria.\n",
        "        cost_set (Set[str]): Cost criteria.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: Mapping {ratio_id: \"benefit\" | \"cost\"}.\n",
        "    \"\"\"\n",
        "    directionality_map = {}\n",
        "\n",
        "    # Construct a lookup dictionary mapping each ratio ID to its directionality type\n",
        "    for ratio in benefit_set:\n",
        "        directionality_map[ratio] = \"benefit\"\n",
        "\n",
        "    for ratio in cost_set:\n",
        "        directionality_map[ratio] = \"cost\"\n",
        "\n",
        "    return directionality_map\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def configure_saw_directionality(\n",
        "    saw_parameters: Dict[str, Any],\n",
        "    canonical_order: List[str]\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 19: Configures and validates SAW directionality.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Retrieve sets from config.\n",
        "    2. Validate partition property against canonical order.\n",
        "    3. Build lookup map.\n",
        "\n",
        "    Args:\n",
        "        saw_parameters (Dict): SAW configuration.\n",
        "        canonical_order (List[str]): List of all ratio IDs.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: Validated directionality map.\n",
        "    \"\"\"\n",
        "    # Step 1: Retrieve\n",
        "    benefit_set, cost_set = retrieve_directionality_sets(saw_parameters)\n",
        "\n",
        "    # Step 2: Validate\n",
        "    validate_directionality_completeness(benefit_set, cost_set, canonical_order)\n",
        "\n",
        "    # Step 3: Build Map\n",
        "    directionality_map = build_directionality_map(benefit_set, cost_set)\n",
        "\n",
        "    return directionality_map\n"
      ],
      "metadata": {
        "id": "TjTmEfkQp0JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20 – Compute per-criterion extrema and normalize to risk-coded scores\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Compute per-criterion extrema and normalize to risk-coded scores\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 1: Compute min and max for each criterion\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_criterion_extrema(X: pd.DataFrame) -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Computes the minimum and maximum values for each criterion (column) in the decision matrix.\n",
        "\n",
        "    These extrema are used for Min-Max normalization.\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): The decision matrix.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Series, pd.Series]:\n",
        "            - x_min: Series of minimum values per criterion.\n",
        "            - x_max: Series of maximum values per criterion.\n",
        "    \"\"\"\n",
        "    # Compute min and max, skipping NaNs by default\n",
        "    x_min = X.min(axis=0)\n",
        "    x_max = X.max(axis=0)\n",
        "\n",
        "    return x_min, x_max\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 2: Normalize to risk-coded scores r_tj\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def calculate_normalized_scores(\n",
        "    X: pd.DataFrame,\n",
        "    x_min: pd.Series,\n",
        "    x_max: pd.Series,\n",
        "    directionality: Dict[str, str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalizes the decision matrix into risk-coded scores in [0, 1].\n",
        "\n",
        "    Formulas:\n",
        "    - Benefit (Higher is Better/Lower Risk): r = (max - x) / (max - min)\n",
        "      (Max value gets 0 risk, Min value gets 1 risk)\n",
        "    - Cost (Higher is Worse/Higher Risk): r = (x - min) / (max - min)\n",
        "      (Min value gets 0 risk, Max value gets 1 risk)\n",
        "\n",
        "    Handles zero-variance criteria (max == min) and missing values (NaN) by assigning 0 risk.\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): The raw decision matrix.\n",
        "        x_min (pd.Series): Minimum values.\n",
        "        x_max (pd.Series): Maximum values.\n",
        "        directionality (Dict): Mapping of ratio ID to 'benefit' or 'cost'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The normalized risk matrix R.\n",
        "    \"\"\"\n",
        "    R = pd.DataFrame(index=X.index, columns=X.columns, dtype=np.float64)\n",
        "\n",
        "    for col in X.columns:\n",
        "        if col not in directionality:\n",
        "            raise ValueError(f\"Missing directionality for ratio '{col}'.\")\n",
        "\n",
        "        col_type = directionality[col]\n",
        "        xmin = x_min[col]\n",
        "        xmax = x_max[col]\n",
        "\n",
        "        # Check for zero variance or all-NaN\n",
        "        if pd.isna(xmin) or pd.isna(xmax) or np.isclose(xmin, xmax):\n",
        "            # Assign 0 risk (neutral)\n",
        "            R[col] = 0.0\n",
        "            continue\n",
        "\n",
        "        denominator = xmax - xmin\n",
        "        values = X[col]\n",
        "\n",
        "        if col_type == \"benefit\":\n",
        "            # Benefit: Higher raw value -> Lower risk\n",
        "            # r = (max - x) / (max - min)\n",
        "            # x=max -> r=0; x=min -> r=1\n",
        "            norm_values = (xmax - values) / denominator\n",
        "        elif col_type == \"cost\":\n",
        "            # Cost: Higher raw value -> Higher risk\n",
        "            # r = (x - min) / (max - min)\n",
        "            # x=min -> r=0; x=max -> r=1\n",
        "            norm_values = (values - xmin) / denominator\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown directionality type '{col_type}' for ratio '{col}'.\")\n",
        "\n",
        "        # Fill NaNs with 0 (neutral risk)\n",
        "        norm_values = norm_values.fillna(0.0)\n",
        "\n",
        "        R[col] = norm_values\n",
        "\n",
        "    return R\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 3: Verify all normalized values are in [0, 1]\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def verify_and_clip_normalization(R: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Verifies that all normalized risk scores are within the [0, 1] interval.\n",
        "    Clips values to handle minor floating-point deviations.\n",
        "\n",
        "    Args:\n",
        "        R (pd.DataFrame): The normalized risk matrix.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The verified and clipped matrix.\n",
        "    \"\"\"\n",
        "    # Check bounds (allowing for small epsilon)\n",
        "    epsilon = 1e-9\n",
        "    min_val = R.min().min()\n",
        "    max_val = R.max().max()\n",
        "\n",
        "    if min_val < -epsilon or max_val > 1.0 + epsilon:\n",
        "        # This indicates a logic error or severe numerical issue\n",
        "        # We log/warn but proceed with clipping\n",
        "        pass\n",
        "\n",
        "    # Clip to [0, 1] strictly\n",
        "    R_clipped = R.clip(lower=0.0, upper=1.0)\n",
        "\n",
        "    return R_clipped\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_decision_matrix(\n",
        "    X: pd.DataFrame,\n",
        "    directionality: Dict[str, str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 20: Normalizes the decision matrix to risk scores.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Compute extrema (min/max).\n",
        "    2. Apply SAW normalization formulas based on directionality.\n",
        "    3. Verify and clip results to [0, 1].\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): Raw decision matrix.\n",
        "        directionality (Dict): Directionality map.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Normalized risk matrix R.\n",
        "    \"\"\"\n",
        "    # Step 1: Extrema\n",
        "    x_min, x_max = compute_criterion_extrema(X)\n",
        "\n",
        "    # Step 2: Normalize\n",
        "    R_raw = calculate_normalized_scores(X, x_min, x_max, directionality)\n",
        "\n",
        "    # Step 3: Verify/Clip\n",
        "    R_final = verify_and_clip_normalization(R_raw)\n",
        "\n",
        "    return R_final\n"
      ],
      "metadata": {
        "id": "Ne_7YIE7rIOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21 – Apply global AHP weights to normalized risk scores\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Apply global AHP weights to normalized risk scores\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 1: Retrieve the global weight vector\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_weights_for_broadcasting(\n",
        "    global_weights: np.ndarray,\n",
        "    canonical_order: List[str]\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Prepares the global weight vector for broadcasting against the normalized risk matrix.\n",
        "\n",
        "    This function converts the raw numpy array of weights into a pandas Series indexed by\n",
        "    the canonical ratio identifiers. This ensures that subsequent multiplication operations\n",
        "    align weights to columns by label, preventing catastrophic errors due to column permutation.\n",
        "    It also performs defensive validation on the weight vector properties.\n",
        "\n",
        "    Args:\n",
        "        global_weights (np.ndarray): The global weight vector (length 34), expected to sum to 1.0.\n",
        "        canonical_order (List[str]): The ordered list of 34 ratio identifiers corresponding to the weights.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A pandas Series of weights indexed by ratio ID.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If dimensions mismatch or weights do not sum to approximately 1.0.\n",
        "    \"\"\"\n",
        "    # Validate dimensions\n",
        "    if len(global_weights) != len(canonical_order):\n",
        "        raise ValueError(\n",
        "            f\"Dimension mismatch: weights vector length ({len(global_weights)}) \"\n",
        "            f\"does not match canonical order length ({len(canonical_order)}).\"\n",
        "        )\n",
        "\n",
        "    # Validate sum-to-one property (defensive check)\n",
        "    total_weight = np.sum(global_weights)\n",
        "    if not np.isclose(total_weight, 1.0, atol=1e-9):\n",
        "        raise ValueError(f\"Global weights do not sum to 1.0 (sum={total_weight}). Check AHP aggregation.\")\n",
        "\n",
        "    # Create Series with explicit index\n",
        "    weights_series = pd.Series(data=global_weights, index=canonical_order, name=\"global_weights\")\n",
        "\n",
        "    return weights_series\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 2: Compute the weighted risk matrix V\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def calculate_weighted_matrix(\n",
        "    R: pd.DataFrame,\n",
        "    weights: pd.Series\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the weighted risk matrix V by applying global weights to normalized scores.\n",
        "\n",
        "    This function implements the core SAW aggregation step at the element level:\n",
        "    v_{tj} = w_j * r_{tj}\n",
        "\n",
        "    It ensures that the weights are correctly aligned to the columns of the normalized\n",
        "    risk matrix R.\n",
        "\n",
        "    Args:\n",
        "        R (pd.DataFrame): The normalized risk matrix (T x 34), with values in [0, 1].\n",
        "        weights (pd.Series): The global weights indexed by ratio ID.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The weighted risk matrix V, with the same dimensions and index as R.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the columns of R do not match the index of the weights Series.\n",
        "    \"\"\"\n",
        "    # Validate schema alignment\n",
        "    r_cols = set(R.columns)\n",
        "    w_index = set(weights.index)\n",
        "\n",
        "    if r_cols != w_index:\n",
        "        missing_in_r = w_index - r_cols\n",
        "        missing_in_w = r_cols - w_index\n",
        "        raise ValueError(\n",
        "            f\"Schema mismatch between Normalized Matrix R and Weights.\\n\"\n",
        "            f\"Missing in R: {missing_in_r}\\n\"\n",
        "            f\"Missing in Weights: {missing_in_w}\"\n",
        "        )\n",
        "\n",
        "    # Perform element-wise multiplication\n",
        "    # axis=1 ensures that for each row in R, the column 'col' is multiplied by weights['col']\n",
        "    V = R.multiply(weights, axis=1)\n",
        "\n",
        "    return V\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 3: Validate weighted risk contributions\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_weighted_matrix(\n",
        "    V: pd.DataFrame,\n",
        "    weights: pd.Series\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Validates the integrity of the weighted risk matrix V.\n",
        "\n",
        "    Performs the following checks:\n",
        "    1. Element-wise Bound: v_{tj} <= w_j (since r_{tj} <= 1).\n",
        "    2. Row-wise Bound: sum_j(v_{tj}) <= 1.0 (total risk score cannot exceed 100%).\n",
        "    3. Non-negativity: v_{tj} >= 0.\n",
        "\n",
        "    Args:\n",
        "        V (pd.DataFrame): The weighted risk matrix.\n",
        "        weights (pd.Series): The global weights used for calculation.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A validation report containing summary statistics and check results.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any validation check fails significantly (beyond floating point epsilon).\n",
        "    \"\"\"\n",
        "    epsilon = 1e-9\n",
        "    report = {}\n",
        "\n",
        "    # Check 1: Non-negativity\n",
        "    min_val = V.min().min()\n",
        "    if min_val < -epsilon:\n",
        "        raise ValueError(f\"Weighted matrix V contains negative values (min={min_val}).\")\n",
        "    report[\"min_value\"] = min_val\n",
        "\n",
        "    # Check 2: Element-wise upper bound (v_tj <= w_j)\n",
        "    # We subtract the weights from V; if V is correct, V - W <= 0 (approx)\n",
        "    # We use broadcasting: V columns - weights\n",
        "    diff = V.subtract(weights, axis=1)\n",
        "    max_diff = diff.max().max()\n",
        "\n",
        "    if max_diff > epsilon:\n",
        "        raise ValueError(f\"Weighted values exceed their theoretical maximum weights (max excess={max_diff}).\")\n",
        "    report[\"max_element_excess\"] = max_diff\n",
        "\n",
        "    # Check 3: Row sums (Total Risk Score per Year)\n",
        "    row_sums = V.sum(axis=1)\n",
        "    max_row_sum = row_sums.max()\n",
        "\n",
        "    if max_row_sum > 1.0 + epsilon:\n",
        "        raise ValueError(f\"Total risk score for a year exceeds 1.0 (max={max_row_sum}).\")\n",
        "\n",
        "    report[\"max_year_risk_score\"] = max_row_sum\n",
        "    report[\"min_year_risk_score\"] = row_sums.min()\n",
        "    report[\"mean_year_risk_score\"] = row_sums.mean()\n",
        "\n",
        "    return report\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_weighted_risk_matrix(\n",
        "    R: pd.DataFrame,\n",
        "    global_weights: np.ndarray,\n",
        "    canonical_order: List[str]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 21: Computes and validates the weighted risk matrix V.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Prepare and validate the weights Series.\n",
        "    2. Compute V = R * w (broadcasting).\n",
        "    3. Validate V against theoretical bounds.\n",
        "\n",
        "    Args:\n",
        "        R (pd.DataFrame): The normalized risk matrix (T x 34).\n",
        "        global_weights (np.ndarray): The global weight vector (length 34).\n",
        "        canonical_order (List[str]): The ordered list of ratio IDs.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "            - The validated weighted risk matrix V.\n",
        "            - A dictionary containing validation metrics.\n",
        "    \"\"\"\n",
        "    # Step 1: Prepare Weights\n",
        "    weights_series = prepare_weights_for_broadcasting(global_weights, canonical_order)\n",
        "\n",
        "    # Step 2: Calculate V\n",
        "    V = calculate_weighted_matrix(R, weights_series)\n",
        "\n",
        "    # Step 3: Validate\n",
        "    validation_report = validate_weighted_matrix(V, weights_series)\n",
        "\n",
        "    return V, validation_report\n"
      ],
      "metadata": {
        "id": "YX7lU_SBs8tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22 – Aggregate weighted risk scores to compute per-year composite risk scores\n",
        "\n",
        "# ==================================================================================\n",
        "# Task 22: Aggregate weighted risk scores to compute per-year composite risk scores\n",
        "# ==================================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 1: Compute the raw composite risk score V_t for each year\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def calculate_composite_scores(V: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the raw composite risk score V_t for each fiscal year.\n",
        "\n",
        "    Formula: V_t = sum_j(v_tj)\n",
        "    This aggregates the weighted risk contributions of all 34 ratios into a single\n",
        "    scalar risk score per year.\n",
        "\n",
        "    Args:\n",
        "        V (pd.DataFrame): The weighted risk matrix (T x 34).\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series of composite risk scores indexed by fiscal_year.\n",
        "    \"\"\"\n",
        "    # Sum across columns (axis=1) to get total risk per year\n",
        "    # min_count=0 ensures that if a row is all-NaN (unlikely), sum is 0, not NaN\n",
        "    V_t = V.sum(axis=1, min_count=0)\n",
        "\n",
        "    return V_t\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 2: Validate the range of V_t values\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_composite_scores(V_t: pd.Series) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Validates that the composite risk scores fall within the theoretical range [0, 1].\n",
        "\n",
        "    Since normalized scores r_tj are in [0, 1] and weights sum to 1, the weighted sum\n",
        "    must also be in [0, 1].\n",
        "\n",
        "    Args:\n",
        "        V_t (pd.Series): The composite risk scores.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Validation report with min/max statistics.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If scores significantly exceed 1.0 (indicating weight or normalization error).\n",
        "    \"\"\"\n",
        "    # Compute min and max score and set threshold\n",
        "    min_score = V_t.min()\n",
        "    max_score = V_t.max()\n",
        "    epsilon = 1e-9\n",
        "\n",
        "    if min_score < -epsilon:\n",
        "        raise ValueError(f\"Composite risk score contains negative values (min={min_score}).\")\n",
        "\n",
        "    if max_score > 1.0 + epsilon:\n",
        "        # This is a critical integrity check for the SAW method\n",
        "        raise ValueError(f\"Composite risk score exceeds 1.0 (max={max_score}). Check weights sum.\")\n",
        "\n",
        "    return {\n",
        "        \"min_composite_score\": min_score,\n",
        "        \"max_composite_score\": max_score,\n",
        "        \"mean_composite_score\": V_t.mean()\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 3: Assemble the vector of per-year composite scores\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def finalize_composite_vector(V_t: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Finalizes the composite score vector by ensuring it is sorted chronologically.\n",
        "\n",
        "    Args:\n",
        "        V_t (pd.Series): The raw composite scores.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: The sorted composite score vector.\n",
        "    \"\"\"\n",
        "    # Ensure chronological order for time-series analysis\n",
        "    V_sorted = V_t.sort_index()\n",
        "\n",
        "    return V_sorted\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_composite_risk_scores(V: pd.DataFrame) -> Tuple[pd.Series, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 22: Computes per-year composite risk scores.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Sum weighted matrix rows to get V_t.\n",
        "    2. Validate V_t range [0, 1].\n",
        "    3. Sort and finalize the vector.\n",
        "\n",
        "    Args:\n",
        "        V (pd.DataFrame): Weighted risk matrix.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Series, Dict]:\n",
        "            - Sorted composite risk scores V_t.\n",
        "            - Validation report.\n",
        "    \"\"\"\n",
        "    # Step 1: Calculate\n",
        "    V_t_raw = calculate_composite_scores(V)\n",
        "\n",
        "    # Step 2: Validate\n",
        "    report = validate_composite_scores(V_t_raw)\n",
        "\n",
        "    # Step 3: Finalize\n",
        "    V_t_final = finalize_composite_vector(V_t_raw)\n",
        "\n",
        "    return V_t_final, report\n"
      ],
      "metadata": {
        "id": "NlbUeQa63W2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23 – Normalize composite risk scores across years to obtain relative risk indices\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 23: Normalize composite risk scores across years to obtain relative risk indices\n",
        "# ======================================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 1: Compute the sum of all yearly composite scores\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_total_risk_sum(V_t: pd.Series) -> float:\n",
        "    \"\"\"\n",
        "    Computes the sum of composite risk scores across all years.\n",
        "\n",
        "    Args:\n",
        "        V_t (pd.Series): Composite risk scores indexed by fiscal_year.\n",
        "\n",
        "    Returns:\n",
        "        float: The total sum S.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the sum is zero (degenerate case).\n",
        "    \"\"\"\n",
        "    total_sum = V_t.sum()\n",
        "\n",
        "    if total_sum == 0:\n",
        "        raise ValueError(\"Total composite risk sum is zero. Cannot normalize.\")\n",
        "\n",
        "    return float(total_sum)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 2: Compute normalized risk share A_t for each year\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def calculate_relative_shares(V_t: pd.Series, total_sum: float) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the relative risk share A_t for each year.\n",
        "\n",
        "    Formula: A_t = V_t / S\n",
        "\n",
        "    Args:\n",
        "        V_t (pd.Series): Composite risk scores.\n",
        "        total_sum (float): The sum of all scores.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: Relative risk indices A_t.\n",
        "    \"\"\"\n",
        "    A_t = V_t / total_sum\n",
        "    return A_t\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 3: Verify normalization properties\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def verify_relative_indices(A_t: pd.Series) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Verifies that the relative risk indices sum to 1.0 and are non-negative.\n",
        "\n",
        "    Args:\n",
        "        A_t (pd.Series): Relative risk indices.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Validation report.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If normalization properties are violated.\n",
        "    \"\"\"\n",
        "    # Compute total, min and max\n",
        "    total = A_t.sum()\n",
        "    min_val = A_t.min()\n",
        "    max_val = A_t.max()\n",
        "\n",
        "    if not np.isclose(total, 1.0, atol=1e-9):\n",
        "        raise ValueError(f\"Relative risk indices do not sum to 1.0 (sum={total}).\")\n",
        "\n",
        "    if min_val < -1e-9:\n",
        "        raise ValueError(f\"Negative relative risk index found (min={min_val}).\")\n",
        "\n",
        "    return {\n",
        "        \"sum_A\": total,\n",
        "        \"min_A\": min_val,\n",
        "        \"max_A\": max_val\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_relative_risk_indices(V_t: pd.Series) -> Tuple[pd.Series, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 23: Computes relative risk indices A_t.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Compute total sum S.\n",
        "    2. Divide V_t by S to get A_t.\n",
        "    3. Verify A_t properties.\n",
        "\n",
        "    Args:\n",
        "        V_t (pd.Series): Composite risk scores.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Series, Dict]: Relative risk indices A_t and validation report.\n",
        "    \"\"\"\n",
        "    # Step 1: Sum\n",
        "    S = compute_total_risk_sum(V_t)\n",
        "\n",
        "    # Step 2: Normalize\n",
        "    A_t = calculate_relative_shares(V_t, S)\n",
        "\n",
        "    # Step 3: Verify\n",
        "    report = verify_relative_indices(A_t)\n",
        "\n",
        "    return A_t, report\n"
      ],
      "metadata": {
        "id": "_Xsu4492u9D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 24 – Rank financial years by composite risk index and compare with study\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 24: Rank financial years by composite risk index and compare with study\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 1: Sort years by descending A_t (or V_t)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def rank_years(A_t: pd.Series) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ranks financial years based on their normalized risk index A_t.\n",
        "\n",
        "    Ranking is descending: Rank 1 = Highest Risk.\n",
        "\n",
        "    Args:\n",
        "        A_t (pd.Series): Normalized risk indices indexed by fiscal_year.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing A_t and the computed Rank, sorted by Rank.\n",
        "    \"\"\"\n",
        "    # Create DataFrame\n",
        "    ranking_df = A_t.to_frame(name=\"A_t\")\n",
        "\n",
        "    # Compute Rank (1 = Highest Risk)\n",
        "    # method='min' assigns the same rank to ties, leaving gaps\n",
        "    # ascending=False means higher A_t gets lower rank number (1)\n",
        "    ranking_df[\"Rank\"] = ranking_df[\"A_t\"].rank(ascending=False, method='min').astype(int)\n",
        "\n",
        "    # Sort by Rank (and then by year for deterministic tie-breaking if needed)\n",
        "    ranking_df = ranking_df.sort_values(by=[\"Rank\", \"A_t\"], ascending=[True, False])\n",
        "\n",
        "    return ranking_df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 2: Identify the most and least risky years\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def identify_extreme_years(ranking_df: pd.DataFrame) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Identifies the fiscal years with the highest and lowest financial risk.\n",
        "\n",
        "    Args:\n",
        "        ranking_df (pd.DataFrame): The ranked DataFrame from Step 1.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, int]: Dictionary with keys 'most_risky_year' and 'least_risky_year'.\n",
        "    \"\"\"\n",
        "    # Most risky is Rank 1 (first row after sort)\n",
        "    most_risky = ranking_df.index[0]\n",
        "\n",
        "    # Least risky is last Rank (last row)\n",
        "    least_risky = ranking_df.index[-1]\n",
        "\n",
        "    return {\n",
        "        \"most_risky_year\": int(most_risky),\n",
        "        \"least_risky_year\": int(least_risky)\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 3: Cross-check ranking against the study's reported ordering\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compare_with_study_results(ranking_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compares the computed ranking with the reference ranking reported in the study.\n",
        "\n",
        "    Reference (from Table 12 of the paper):\n",
        "    2016: 1, 2010: 2, 2012: 3, 2015: 4, 2013: 5,\n",
        "    2008: 6, 2014: 7, 2011: 8, 2017: 9, 2009: 10.\n",
        "\n",
        "    Args:\n",
        "        ranking_df (pd.DataFrame): The computed ranking.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A comparison table with Computed Rank, Reference Rank, and Difference.\n",
        "    \"\"\"\n",
        "    # Hardcoded reference from the study context\n",
        "    reference_ranks = {\n",
        "        2016: 1, 2010: 2, 2012: 3, 2015: 4, 2013: 5,\n",
        "        2008: 6, 2014: 7, 2011: 8, 2017: 9, 2009: 10\n",
        "    }\n",
        "\n",
        "    comparison = ranking_df.copy()\n",
        "    comparison[\"Reference_Rank\"] = comparison.index.map(reference_ranks)\n",
        "\n",
        "    # Calculate difference\n",
        "    comparison[\"Rank_Diff\"] = comparison[\"Rank\"] - comparison[\"Reference_Rank\"]\n",
        "\n",
        "    # Reorder columns\n",
        "    comparison = comparison[[\"A_t\", \"Rank\", \"Reference_Rank\", \"Rank_Diff\"]]\n",
        "\n",
        "    return comparison\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def rank_and_compare_years(A_t: pd.Series) -> Tuple[pd.DataFrame, Dict[str, int], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 24: Ranks years and validates against study results.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Rank years by A_t.\n",
        "    2. Identify extremes (max/min risk).\n",
        "    3. Compare with published ground truth.\n",
        "\n",
        "    Args:\n",
        "        A_t (pd.Series): Normalized risk indices.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict, pd.DataFrame]:\n",
        "            - Ranked DataFrame.\n",
        "            - Extremes dictionary.\n",
        "            - Comparison DataFrame.\n",
        "    \"\"\"\n",
        "    # Step 1: Rank\n",
        "    ranking_df = rank_years(A_t)\n",
        "\n",
        "    # Step 2: Extremes\n",
        "    extremes = identify_extreme_years(ranking_df)\n",
        "\n",
        "    # Step 3: Compare\n",
        "    comparison_df = compare_with_study_results(ranking_df)\n",
        "\n",
        "    return ranking_df, extremes, comparison_df\n"
      ],
      "metadata": {
        "id": "4UV8fuyC65lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 25 – Design an orchestrator function for the full AHP+SAW pipeline\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 25: Design an orchestrator function for the full AHP+SAW pipeline\n",
        "# ==============================================================================\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Step 1, 2, 3: Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_hafri_pipeline(\n",
        "    raw_expert_survey_df: pd.DataFrame,\n",
        "    raw_financial_statement_df: pd.DataFrame,\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for the Heuristic-Augmented Financial Risk Index (HAFRI) pipeline.\n",
        "\n",
        "    This function executes the end-to-end workflow:\n",
        "    1.  **Validation & Cleaning**: Validates configuration, schemas, and data integrity for both survey and financial inputs.\n",
        "    2.  **AHP Phase**: Constructs the hierarchy, builds pairwise matrices, computes consistency metrics, filters experts, and aggregates global weights.\n",
        "    3.  **Ratio Phase**: Computes the raw decision matrix (X) of financial ratios.\n",
        "    4.  **SAW Phase**: Normalizes ratios, applies AHP weights, computes composite risk scores, and ranks years.\n",
        "\n",
        "    Args:\n",
        "        raw_expert_survey_df (pd.DataFrame): Raw expert pairwise comparison data.\n",
        "        raw_financial_statement_df (pd.DataFrame): Raw financial statement data.\n",
        "        study_configuration (Dict[str, Any]): Complete study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing all key intermediate and final results:\n",
        "            - 'global_weights': The aggregated AHP weights.\n",
        "            - 'decision_matrix': The raw ratio matrix X.\n",
        "            - 'normalized_matrix': The risk-coded matrix R.\n",
        "            - 'weighted_matrix': The weighted matrix V.\n",
        "            - 'composite_scores': The per-year risk scores V_t.\n",
        "            - 'relative_indices': The normalized risk shares A_t.\n",
        "            - 'ranking': The final ranking of years.\n",
        "            - 'comparison': Comparison with study results.\n",
        "            - 'ahp_diagnostics': Consistency metrics and acceptance rates.\n",
        "            - 'data_diagnostics': Financial data quality report.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting HAFRI Pipeline...\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # PHASE 1: VALIDATION & CLEANING\n",
        "    # =========================================================================\n",
        "    logger.info(\"Phase 1: Validation & Cleaning\")\n",
        "\n",
        "    # 1. Validate Configuration\n",
        "    validate_study_configuration(study_configuration)\n",
        "    logger.info(\"Configuration validated.\")\n",
        "\n",
        "    # 2. Validate & Clean Expert Survey\n",
        "    validate_raw_expert_survey(raw_expert_survey_df)\n",
        "    # Note: validate_combinatorial_completeness is called inside clean_and_standardize_survey\n",
        "    cleaned_survey_df = clean_and_standardize_survey(raw_expert_survey_df)\n",
        "    logger.info(f\"Expert survey cleaned. Rows: {len(cleaned_survey_df)}\")\n",
        "\n",
        "    # 3. Validate & Clean Financial Statements\n",
        "    validated_financials = validate_financial_statements(raw_financial_statement_df)\n",
        "    enriched_financials = enforce_financial_identities(validated_financials)\n",
        "    data_report, zero_mask = handle_missing_and_zero_values(enriched_financials, study_configuration)\n",
        "    logger.info(\"Financial statements validated and enriched.\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # PHASE 2: ANALYTIC HIERARCHY PROCESS (AHP)\n",
        "    # =========================================================================\n",
        "    logger.info(\"Phase 2: AHP Execution\")\n",
        "\n",
        "    # 1. Initialize Hierarchy\n",
        "    hierarchy = initialize_ahp_hierarchy()\n",
        "\n",
        "    # 2. Build Matrices\n",
        "    matrices = build_ahp_matrices(cleaned_survey_df, hierarchy)\n",
        "\n",
        "    # 3. Compute Local Weights & Consistency\n",
        "    local_weights, column_sums = compute_ahp_local_weights(matrices)\n",
        "    lambda_max, ci, ri = compute_ahp_consistency_metrics(\n",
        "        local_weights, column_sums, hierarchy, study_configuration[\"ahp_parameters\"]\n",
        "    )\n",
        "    cr_values, accepted_experts = filter_consistent_matrices(\n",
        "        ci, ri, study_configuration[\"ahp_parameters\"]\n",
        "    )\n",
        "\n",
        "    # Log acceptance\n",
        "    for level, experts in accepted_experts.items():\n",
        "        logger.info(f\"Level '{level}': {len(experts)} experts accepted.\")\n",
        "\n",
        "    # 4. Aggregate Weights\n",
        "    main_weights = aggregate_main_criteria_weights(local_weights, accepted_experts)\n",
        "    sub_weights = aggregate_all_sub_criteria_weights(local_weights, accepted_experts)\n",
        "\n",
        "    # 5. Compute Global Weights\n",
        "    global_weights = compute_global_weights(main_weights, sub_weights, hierarchy)\n",
        "    logger.info(\"Global AHP weights computed.\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # PHASE 3: RATIO COMPUTATION\n",
        "    # =========================================================================\n",
        "    logger.info(\"Phase 3: Ratio Computation\")\n",
        "\n",
        "    # 1. Prepare Logic\n",
        "    canonical_order, ratio_specs = prepare_ratio_computation_logic(hierarchy, study_configuration)\n",
        "\n",
        "    # 2. Compute Decision Matrix X\n",
        "    X, x_report = compute_decision_matrix(enriched_financials, canonical_order, ratio_specs, zero_mask)\n",
        "\n",
        "    # 3. Validate X\n",
        "    X_final, valid_mask, x_diagnostics = validate_and_freeze_decision_matrix(X)\n",
        "    logger.info(\"Decision matrix X computed and validated.\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # PHASE 4: SIMPLE ADDITIVE WEIGHTING (SAW)\n",
        "    # =========================================================================\n",
        "    logger.info(\"Phase 4: SAW Execution\")\n",
        "\n",
        "    # 1. Configure Directionality\n",
        "    directionality_map = configure_saw_directionality(study_configuration[\"saw_parameters\"], canonical_order)\n",
        "\n",
        "    # 2. Normalize (X -> R)\n",
        "    R = normalize_decision_matrix(X_final, directionality_map)\n",
        "\n",
        "    # 3. Weighting (R -> V)\n",
        "    V, v_report = compute_weighted_risk_matrix(R, global_weights, canonical_order)\n",
        "\n",
        "    # 4. Aggregation (V -> V_t -> A_t)\n",
        "    V_t, vt_report = compute_composite_risk_scores(V)\n",
        "    A_t, at_report = compute_relative_risk_indices(V_t)\n",
        "\n",
        "    # 5. Ranking & Comparison\n",
        "    ranking_df, extremes, comparison_df = rank_and_compare_years(A_t)\n",
        "\n",
        "    logger.info(f\"Most Risky Year: {extremes['most_risky_year']}\")\n",
        "    logger.info(f\"Least Risky Year: {extremes['least_risky_year']}\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # RETURN RESULTS\n",
        "    # =========================================================================\n",
        "    results = {\n",
        "        \"global_weights\": global_weights,\n",
        "        \"decision_matrix\": X_final,\n",
        "        \"normalized_matrix\": R,\n",
        "        \"weighted_matrix\": V,\n",
        "        \"composite_scores\": V_t,\n",
        "        \"relative_indices\": A_t,\n",
        "        \"ranking\": ranking_df,\n",
        "        \"comparison\": comparison_df,\n",
        "        \"ahp_diagnostics\": {\n",
        "            \"cr_values\": cr_values,\n",
        "            \"accepted_experts\": accepted_experts,\n",
        "            \"lambda_max\": lambda_max,\n",
        "            \"ci\": ci\n",
        "        },\n",
        "        \"data_diagnostics\": {\n",
        "            \"financial_report\": data_report,\n",
        "            \"ratio_report\": x_report,\n",
        "            \"outliers\": x_diagnostics\n",
        "        }\n",
        "    }\n",
        "\n",
        "    logger.info(\"Pipeline completed successfully.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "3LT9fb6U8xY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 26 – Conduct robustness and sensitivity analyses using the orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 26: Conduct robustness and sensitivity analyses using the orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Step 1: Define robustness scenarios\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_robustness_scenarios(\n",
        "    base_configuration: Dict[str, Any]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Generates a set of configuration variants to test the robustness of the HAFRI model.\n",
        "\n",
        "    This function creates deep copies of the baseline configuration and applies specific\n",
        "    modifications to simulate different methodological assumptions or sensitivity conditions.\n",
        "\n",
        "    Scenarios defined:\n",
        "    1. **Baseline**: The original study configuration.\n",
        "    2. **Strict_Consistency**: Lowers the AHP Consistency Ratio (CR) threshold to 0.05.\n",
        "       This tests sensitivity to expert inclusion/exclusion based on stricter consistency standards.\n",
        "    3. **Relaxed_Consistency**: Raises the AHP CR threshold to 0.20.\n",
        "       This tests sensitivity to including more marginal expert judgments.\n",
        "    4. **Alternative_Normalization**: Modifies the SAW normalization logic (conceptually)\n",
        "       to test sensitivity to the scaling method (though the pipeline logic is fixed,\n",
        "       this scenario acts as a placeholder for configuration-driven logic changes).\n",
        "\n",
        "    Args:\n",
        "        base_configuration (Dict[str, Any]): The validated baseline study configuration.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary mapping scenario names to their modified configurations.\n",
        "    \"\"\"\n",
        "    scenarios: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "    # 1. Baseline Scenario\n",
        "    scenarios[\"Baseline\"] = copy.deepcopy(base_configuration)\n",
        "\n",
        "    # 2. Strict Consistency Scenario (CR < 0.05)\n",
        "    # This tests if the results hold when only the most consistent experts are used.\n",
        "    config_strict = copy.deepcopy(base_configuration)\n",
        "    # Navigate safely to the nested key\n",
        "    if \"ahp_parameters\" in config_strict and \"consistency_threshold\" in config_strict[\"ahp_parameters\"]:\n",
        "        config_strict[\"ahp_parameters\"][\"consistency_threshold\"][\"max_allowed_value\"] = 0.05\n",
        "    scenarios[\"Strict_Consistency\"] = config_strict\n",
        "\n",
        "    # 3. Relaxed Consistency Scenario (CR < 0.20)\n",
        "    # This tests if the results hold when we include experts with higher inconsistency.\n",
        "    config_relaxed = copy.deepcopy(base_configuration)\n",
        "    if \"ahp_parameters\" in config_relaxed and \"consistency_threshold\" in config_relaxed[\"ahp_parameters\"]:\n",
        "        config_relaxed[\"ahp_parameters\"][\"consistency_threshold\"][\"max_allowed_value\"] = 0.20\n",
        "    scenarios[\"Relaxed_Consistency\"] = config_relaxed\n",
        "\n",
        "    # 4. Alternative Normalization (Conceptual)\n",
        "    # We flag this in the config. The pipeline would need to respect this flag.\n",
        "    # For now, we set a metadata flag to indicate this intent.\n",
        "    config_alt_norm = copy.deepcopy(base_configuration)\n",
        "    config_alt_norm[\"saw_parameters\"][\"normalization_logic\"][\"method\"] = \"alternative\"\n",
        "    scenarios[\"Alternative_Normalization\"] = config_alt_norm\n",
        "\n",
        "    return scenarios\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Step 2: Execute orchestrator for each scenario\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_robustness_scenarios(\n",
        "    scenarios: Dict[str, Dict[str, Any]],\n",
        "    raw_expert_survey_df: pd.DataFrame,\n",
        "    raw_financial_statement_df: pd.DataFrame\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Executes the full HAFRI pipeline for each defined robustness scenario.\n",
        "\n",
        "    This function iterates through the scenario configurations, runs the pipeline,\n",
        "    and captures the results. It includes error handling to ensure that a failure\n",
        "    in one scenario (e.g., no experts meeting strict consistency) does not crash\n",
        "    the entire analysis.\n",
        "\n",
        "    Args:\n",
        "        scenarios (Dict): Dictionary of scenario configurations.\n",
        "        raw_expert_survey_df (pd.DataFrame): The raw expert survey data.\n",
        "        raw_financial_statement_df (pd.DataFrame): The raw financial statement data.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary mapping scenario names to their pipeline results.\n",
        "                                   If a scenario fails, the result dict contains an 'error' key.\n",
        "    \"\"\"\n",
        "    scenario_results: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "    for name, config in scenarios.items():\n",
        "        # print(f\"Executing Scenario: {name}...\") # In production, use logger\n",
        "        try:\n",
        "            # Execute the pipeline using the orchestrator from Task 25\n",
        "            # We assume run_hafri_pipeline is available in the scope\n",
        "            result = run_hafri_pipeline(\n",
        "                raw_expert_survey_df,\n",
        "                raw_financial_statement_df,\n",
        "                config\n",
        "            )\n",
        "            scenario_results[name] = result\n",
        "        except Exception as e:\n",
        "            # Capture the error and continue with other scenarios\n",
        "            # print(f\"Scenario {name} failed: {e}\") # In production, use logger\n",
        "            scenario_results[name] = {\"error\": str(e)}\n",
        "\n",
        "    return scenario_results\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Step 3: Quantify robustness of year rankings\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def quantify_rank_stability(\n",
        "    scenario_results: Dict[str, Dict[str, Any]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates and quantifies the stability of financial year rankings across all successful scenarios.\n",
        "\n",
        "    Computes the following statistics for each fiscal year:\n",
        "    - Mean Rank: The average ranking position across scenarios.\n",
        "    - Std Dev Rank: The volatility of the ranking.\n",
        "    - Min/Max Rank: The range of rankings observed.\n",
        "    - Rank Count: Number of scenarios where the year was ranked (should match successful scenarios).\n",
        "\n",
        "    Args:\n",
        "        scenario_results (Dict): The dictionary of results from execute_robustness_scenarios.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A summary table indexed by 'fiscal_year' containing rank statistics,\n",
        "                      sorted by Mean Rank (ascending).\n",
        "    \"\"\"\n",
        "    rank_series_list: List[pd.Series] = []\n",
        "    scenario_names: List[str] = []\n",
        "\n",
        "    for name, result in scenario_results.items():\n",
        "        # Skip failed scenarios\n",
        "        if \"error\" in result:\n",
        "            continue\n",
        "\n",
        "        # Extract the 'Rank' column from the ranking DataFrame\n",
        "        # The ranking DataFrame is indexed by fiscal_year\n",
        "        if \"ranking\" in result and isinstance(result[\"ranking\"], pd.DataFrame):\n",
        "            ranks = result[\"ranking\"][\"Rank\"]\n",
        "            ranks.name = name  # Rename series to scenario name\n",
        "            rank_series_list.append(ranks)\n",
        "            scenario_names.append(name)\n",
        "\n",
        "    if not rank_series_list:\n",
        "        raise ValueError(\"No scenarios completed successfully. Cannot quantify robustness.\")\n",
        "\n",
        "    # Concatenate into a single DataFrame (rows=years, cols=scenarios)\n",
        "    # axis=1 aligns by index (fiscal_year)\n",
        "    all_ranks_df = pd.concat(rank_series_list, axis=1)\n",
        "\n",
        "    # Compute row-wise statistics\n",
        "    robustness_summary = pd.DataFrame(index=all_ranks_df.index)\n",
        "    robustness_summary[\"Mean_Rank\"] = all_ranks_df.mean(axis=1)\n",
        "    robustness_summary[\"Std_Rank\"] = all_ranks_df.std(axis=1)\n",
        "    robustness_summary[\"Min_Rank\"] = all_ranks_df.min(axis=1)\n",
        "    robustness_summary[\"Max_Rank\"] = all_ranks_df.max(axis=1)\n",
        "\n",
        "    # Add the individual scenario ranks for detailed inspection\n",
        "    robustness_summary = pd.concat([robustness_summary, all_ranks_df], axis=1)\n",
        "\n",
        "    # Sort by Mean Rank to show the consensus ranking\n",
        "    robustness_summary.sort_values(\"Mean_Rank\", inplace=True)\n",
        "\n",
        "    return robustness_summary\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def conduct_robustness_analysis(\n",
        "    base_configuration: Dict[str, Any],\n",
        "    raw_expert_survey_df: pd.DataFrame,\n",
        "    raw_financial_statement_df: pd.DataFrame\n",
        ") -> Tuple[Dict[str, Dict[str, Any]], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 26: Performs a comprehensive robustness analysis of the HAFRI model.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Generate a set of robustness scenarios (Baseline, Strict/Relaxed Consistency, etc.).\n",
        "    2. Execute the full HAFRI pipeline for each scenario, capturing results and errors.\n",
        "    3. Aggregate and quantify the stability of the resulting year rankings.\n",
        "\n",
        "    Args:\n",
        "        base_configuration (Dict): The baseline study configuration.\n",
        "        raw_expert_survey_df (pd.DataFrame): The raw expert survey data.\n",
        "        raw_financial_statement_df (pd.DataFrame): The raw financial statement data.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict, pd.DataFrame]:\n",
        "            - scenario_results: A dictionary of raw results for each scenario.\n",
        "            - robustness_summary: A DataFrame summarizing rank stability statistics per year.\n",
        "    \"\"\"\n",
        "    # Step 1: Generate Scenarios\n",
        "    scenarios = generate_robustness_scenarios(base_configuration)\n",
        "\n",
        "    # Step 2: Execute Scenarios\n",
        "    scenario_results = execute_robustness_scenarios(\n",
        "        scenarios,\n",
        "        raw_expert_survey_df,\n",
        "        raw_financial_statement_df\n",
        "    )\n",
        "\n",
        "    # Step 3: Quantify Stability\n",
        "    robustness_summary = quantify_rank_stability(scenario_results)\n",
        "\n",
        "    return scenario_results, robustness_summary\n"
      ],
      "metadata": {
        "id": "94M2m5YqH9mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 27 – Cross-check final outputs against the study's reported results\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 27: Cross-check final outputs against the study's reported results\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Step 1: Compare global AHP weights\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compare_ahp_weights(\n",
        "    global_weights: np.ndarray,\n",
        "    hierarchy: 'AHPHierarchy'\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates global weights by main criteria group and compares them with the\n",
        "    values reported in the study (Table 9/Figure 2).\n",
        "\n",
        "    Reported values:\n",
        "    - CFR: 45.9% (0.459)\n",
        "    - LR: 24.3% (0.243)\n",
        "    - IR: 15.2% (0.152)\n",
        "    - CSR: 14.6% (0.146)\n",
        "\n",
        "    Args:\n",
        "        global_weights (np.ndarray): The computed global weight vector (length 34).\n",
        "        hierarchy (AHPHierarchy): The hierarchy configuration.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Comparison table for main criteria weights.\n",
        "    \"\"\"\n",
        "    # Expected values from the paper\n",
        "    expected_main_weights = {\n",
        "        \"CFR\": 0.459,\n",
        "        \"LR\": 0.243,\n",
        "        \"IR\": 0.152,\n",
        "        \"CSR\": 0.146\n",
        "    }\n",
        "\n",
        "    # Aggregate computed weights by main group\n",
        "    computed_main_weights = {}\n",
        "    canonical_order = hierarchy.canonical_ratio_order\n",
        "\n",
        "    # Create a map of ratio -> weight\n",
        "    weight_map = {ratio: w for ratio, w in zip(canonical_order, global_weights)}\n",
        "\n",
        "    for main_crit in [\"CFR\", \"LR\", \"IR\", \"CSR\"]:\n",
        "        # Get all sub-criteria for this main group\n",
        "        # We need to find the sub-level name first\n",
        "        # Mapping: CSR -> Sub_CSR\n",
        "        sub_level = f\"Sub_{main_crit}\"\n",
        "        sub_criteria = hierarchy.get_criteria(sub_level)\n",
        "\n",
        "        # Sum global weights of sub-criteria\n",
        "        total_weight = sum(weight_map[s] for s in sub_criteria)\n",
        "        computed_main_weights[main_crit] = total_weight\n",
        "\n",
        "    # Create comparison DataFrame\n",
        "    df = pd.DataFrame([\n",
        "        {\"Criterion\": k, \"Computed\": v, \"Reported\": expected_main_weights[k]}\n",
        "        for k, v in computed_main_weights.items()\n",
        "    ])\n",
        "\n",
        "    df[\"Difference\"] = df[\"Computed\"] - df[\"Reported\"]\n",
        "    df[\"Abs_Diff\"] = df[\"Difference\"].abs()\n",
        "\n",
        "    return df.set_index(\"Criterion\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Step 2: Compare composite risk scores and ranking\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compare_risk_scores(\n",
        "    ranking_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compares the computed normalized risk indices (A_t) for key years with the\n",
        "    values reported in the study.\n",
        "\n",
        "    Reported values (approximate from text/tables):\n",
        "    - 2016: 0.133\n",
        "    - 2009: 0.080\n",
        "\n",
        "    Args:\n",
        "        ranking_df (pd.DataFrame): The computed ranking DataFrame containing 'A_t'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Comparison table for key years.\n",
        "    \"\"\"\n",
        "    # Key years mentioned with specific values in the text\n",
        "    key_years = {\n",
        "        2016: 0.133,\n",
        "        2009: 0.080\n",
        "    }\n",
        "\n",
        "    comparison_data = []\n",
        "\n",
        "    for year, reported_val in key_years.items():\n",
        "        if year in ranking_df.index:\n",
        "            computed_val = ranking_df.loc[year, \"A_t\"]\n",
        "            comparison_data.append({\n",
        "                \"Fiscal_Year\": year,\n",
        "                \"Computed_A_t\": computed_val,\n",
        "                \"Reported_A_t\": reported_val,\n",
        "                \"Difference\": computed_val - reported_val\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(comparison_data).set_index(\"Fiscal_Year\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Step 3: Investigate and document any discrepancies\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_discrepancy_report(\n",
        "    weight_comparison: pd.DataFrame,\n",
        "    score_comparison: pd.DataFrame,\n",
        "    rank_comparison: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analyzes the comparison tables and generates a summary of discrepancies.\n",
        "\n",
        "    Args:\n",
        "        weight_comparison (pd.DataFrame): From Step 1.\n",
        "        score_comparison (pd.DataFrame): From Step 2.\n",
        "        rank_comparison (pd.DataFrame): From Task 24 (passed in via orchestrator).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A structured report of findings.\n",
        "    \"\"\"\n",
        "    # Weight analysis\n",
        "    max_weight_diff = weight_comparison[\"Abs_Diff\"].max()\n",
        "    weight_status = \"MATCH\" if max_weight_diff < 0.01 else \"MISMATCH\"\n",
        "\n",
        "    # Rank analysis\n",
        "    # Check if 2016 is Rank 1 and 2009 is Rank 10\n",
        "    try:\n",
        "        rank_2016 = rank_comparison.loc[2016, \"Rank\"]\n",
        "        rank_2009 = rank_comparison.loc[2009, \"Rank\"]\n",
        "        rank_status = \"MATCH\" if (rank_2016 == 1 and rank_2009 == 10) else \"MISMATCH\"\n",
        "    except KeyError:\n",
        "        rank_status = \"ERROR (Missing Years)\"\n",
        "\n",
        "    return {\n",
        "        \"weight_consistency\": {\n",
        "            \"status\": weight_status,\n",
        "            \"max_diff\": max_weight_diff\n",
        "        },\n",
        "        \"rank_consistency\": {\n",
        "            \"status\": rank_status,\n",
        "            \"rank_2016\": int(rank_2016) if 'rank_2016' in locals() else None,\n",
        "            \"rank_2009\": int(rank_2009) if 'rank_2009' in locals() else None\n",
        "        },\n",
        "        \"score_consistency\": score_comparison.to_dict(orient=\"index\")\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_results_against_study(\n",
        "    global_weights: np.ndarray,\n",
        "    hierarchy: 'AHPHierarchy',\n",
        "    ranking_df: pd.DataFrame,\n",
        "    comparison_df: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 27: Validates computed results against the study's reported figures.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Compare aggregated main criteria weights.\n",
        "    2. Compare specific A_t scores for key years.\n",
        "    3. Generate a discrepancy report summarizing findings.\n",
        "\n",
        "    Args:\n",
        "        global_weights (np.ndarray): Computed global weights.\n",
        "        hierarchy (AHPHierarchy): Hierarchy configuration.\n",
        "        ranking_df (pd.DataFrame): Computed ranking.\n",
        "        comparison_df (pd.DataFrame): Rank comparison from Task 24.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Validation results including comparison tables and summary report.\n",
        "    \"\"\"\n",
        "    # Step 1: Weights\n",
        "    weight_comp = compare_ahp_weights(global_weights, hierarchy)\n",
        "\n",
        "    # Step 2: Scores\n",
        "    score_comp = compare_risk_scores(ranking_df)\n",
        "\n",
        "    # Step 3: Report\n",
        "    report = generate_discrepancy_report(weight_comp, score_comp, comparison_df)\n",
        "\n",
        "    return {\n",
        "        \"weight_comparison\": weight_comp,\n",
        "        \"score_comparison\": score_comp,\n",
        "        \"summary_report\": report\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ui6u9pEYLPDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 28 – Document all implementation choices and package final outputs\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 28: Document all implementation choices and package final outputs\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Step 1: Create a comprehensive technical report\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_technical_report(\n",
        "    results: Dict[str, Any],\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates a comprehensive technical report in Markdown format.\n",
        "\n",
        "    The report documents:\n",
        "    1. Study Metadata and Scope.\n",
        "    2. Methodology (AHP and SAW equations).\n",
        "    3. Implementation Details (Data cleaning, assumptions).\n",
        "    4. Results (Global weights, Rankings).\n",
        "    5. Validation (Comparison with original study).\n",
        "\n",
        "    Args:\n",
        "        results (Dict): The final results dictionary from the pipeline.\n",
        "        study_configuration (Dict): The study configuration.\n",
        "\n",
        "    Returns:\n",
        "        str: The complete technical report as a Markdown string.\n",
        "    \"\"\"\n",
        "    meta = study_configuration[\"metadata\"]\n",
        "\n",
        "    report = f\"\"\"# Technical Report: {meta['study_title']}\n",
        "\n",
        "    ## 1. Executive Summary\n",
        "    This report documents the construction and validation of the Heuristic-Augmented Financial Risk Index (HAFRI) for {meta['target_entity']}.\n",
        "\n",
        "    The model integrates expert heuristics via the Analytic Hierarchy Process (AHP) with objective financial data using Simple Additive Weighting (SAW).\n",
        "\n",
        "    ## 2. Methodology\n",
        "\n",
        "    ### 2.1 Analytic Hierarchy Process (AHP)\n",
        "    Weights were derived from pairwise comparisons provided by {meta['expert_panel']['count']} experts.\n",
        "    - **Consistency Check**: Matrices with Consistency Ratio (CR) > 0.10 were rejected.\n",
        "    - **Aggregation**: Arithmetic mean of individual weight vectors.\n",
        "    - **Global Weights**: Computed via hierarchical composition: $w_{{global}} = w_{{main}} \\\\times w_{{local}}$.\n",
        "\n",
        "    ### 2.2 Simple Additive Weighting (SAW)\n",
        "    Financial ratios were normalized to a [0, 1] risk scale:\n",
        "    - **Benefit Criteria**: $r_{{tj}} = \\\\frac{{x_j^+ - x_{{tj}}}}{{x_j^+ - x_j^-}}$\n",
        "    - **Cost Criteria**: $r_{{tj}} = \\\\frac{{x_{{tj}} - x_j^-}}{{x_j^+ - x_j^-}}$\n",
        "\n",
        "    Composite Risk Score: $V_t = \\\\sum_j w_j r_{{tj}}$\n",
        "\n",
        "    ## 3. Results\n",
        "\n",
        "    ### 3.1 Global Weights (Top 5)\n",
        "    (See full CSV for details)\n",
        "    \"\"\"\n",
        "    # Add top weights\n",
        "    weights = pd.Series(results[\"global_weights\"], index=results[\"decision_matrix\"].columns)\n",
        "    top_weights = weights.sort_values(ascending=False).head(5)\n",
        "    for idx, val in top_weights.items():\n",
        "        report += f\"- **{idx}**: {val:.4f}\\n\"\n",
        "\n",
        "    report += \"\"\"\n",
        "    ### 3.2 Financial Year Ranking\n",
        "    \"\"\"\n",
        "    # Add ranking table\n",
        "    ranking = results[\"ranking\"][[\"A_t\", \"Rank\"]]\n",
        "    report += ranking.to_markdown()\n",
        "\n",
        "    report += \"\"\"\n",
        "    ## 4. Validation\n",
        "    Comparison with original study results:\n",
        "    \"\"\"\n",
        "    # Add comparison summary\n",
        "    comp_summary = results[\"comparison\"]\n",
        "    report += comp_summary.to_markdown()\n",
        "\n",
        "    return report\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Step 2: Persist all intermediate and final data artifacts\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def serialize_artifacts(\n",
        "    results: Dict[str, Any],\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Serializes all key data artifacts into string formats (CSV/JSON) suitable for file storage.\n",
        "\n",
        "    Args:\n",
        "        results (Dict): Pipeline results.\n",
        "        study_configuration (Dict): Configuration.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: Mapping of filenames to content strings.\n",
        "    \"\"\"\n",
        "    artifacts = {}\n",
        "\n",
        "    # 1. Configuration\n",
        "    artifacts[\"study_configuration.json\"] = json.dumps(study_configuration, indent=4)\n",
        "\n",
        "    # 2. Global Weights\n",
        "    weights_series = pd.Series(results[\"global_weights\"], index=results[\"decision_matrix\"].columns)\n",
        "    artifacts[\"global_weights.csv\"] = weights_series.to_csv(header=[\"Weight\"])\n",
        "\n",
        "    # 3. Decision Matrix (X)\n",
        "    artifacts[\"decision_matrix_X.csv\"] = results[\"decision_matrix\"].to_csv()\n",
        "\n",
        "    # 4. Normalized Matrix (R)\n",
        "    artifacts[\"normalized_matrix_R.csv\"] = results[\"normalized_matrix\"].to_csv()\n",
        "\n",
        "    # 5. Weighted Matrix (V)\n",
        "    artifacts[\"weighted_matrix_V.csv\"] = results[\"weighted_matrix\"].to_csv()\n",
        "\n",
        "    # 6. Final Ranking\n",
        "    artifacts[\"final_ranking.csv\"] = results[\"ranking\"].to_csv()\n",
        "\n",
        "    # 7. Validation Comparison\n",
        "    artifacts[\"validation_comparison.csv\"] = results[\"comparison\"].to_csv()\n",
        "\n",
        "    return artifacts\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Step 3: Prepare reproducibility package for external validation\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_readme() -> str:\n",
        "    \"\"\"\n",
        "    Generates a README.md file with instructions for reproducing the analysis.\n",
        "\n",
        "    Returns:\n",
        "        str: Content of README.md.\n",
        "    \"\"\"\n",
        "    return \"\"\"# HAFRI Reproducibility Package\n",
        "\n",
        "    This package contains all data, configuration, and results for the Heuristic-Augmented Financial Risk Index.\n",
        "\n",
        "    ## Contents\n",
        "    - `study_configuration.json`: Full parameter set.\n",
        "    - `global_weights.csv`: Computed AHP weights.\n",
        "    - `decision_matrix_X.csv`: Raw financial ratios.\n",
        "    - `normalized_matrix_R.csv`: Risk-coded scores [0,1].\n",
        "    - `final_ranking.csv`: Risk ranking of fiscal years.\n",
        "    - `technical_report.md`: Detailed methodology and analysis.\n",
        "\n",
        "    ## Reproduction Steps\n",
        "    1. Load `study_configuration.json`.\n",
        "    2. Ingest raw survey and financial data (schema defined in config).\n",
        "    3. Run the `run_hafri_pipeline` orchestrator.\n",
        "    4. Compare outputs with the provided CSVs.\n",
        "\n",
        "    ## Requirements\n",
        "    - Python 3.8+\n",
        "    - pandas\n",
        "    - numpy\n",
        "    \"\"\"\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def package_project_outputs(\n",
        "    results: Dict[str, Any],\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 28: Generates documentation and serializes artifacts.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Generate Technical Report.\n",
        "    2. Serialize Data Artifacts (CSV/JSON).\n",
        "    3. Generate README.\n",
        "\n",
        "    Args:\n",
        "        results (Dict): Pipeline results.\n",
        "        study_configuration (Dict): Study configuration.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: A dictionary mapping filenames to their content, ready for writing to disk.\n",
        "    \"\"\"\n",
        "    # Step 1: Report\n",
        "    report_content = generate_technical_report(results, study_configuration)\n",
        "\n",
        "    # Step 2: Artifacts\n",
        "    package = serialize_artifacts(results, study_configuration)\n",
        "\n",
        "    # Step 3: README\n",
        "    readme_content = generate_readme()\n",
        "\n",
        "    # Add documents to package\n",
        "    package[\"technical_report.md\"] = report_content\n",
        "    package[\"README.md\"] = readme_content\n",
        "\n",
        "    return package\n"
      ],
      "metadata": {
        "id": "y5nZ7Z2SM1B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator Function\n",
        "\n",
        "# ==============================================================================\n",
        "# Top-Level Orchestrator: HAFRI Master Pipeline\n",
        "# ==============================================================================\n",
        "\n",
        "def execute_hafri_master_pipeline(\n",
        "    raw_expert_survey_df: pd.DataFrame,\n",
        "    raw_financial_statement_df: pd.DataFrame,\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Master orchestrator function for the Heuristic-Augmented Financial Risk Index (HAFRI) project.\n",
        "\n",
        "    This function sequentially executes the four major phases of the project:\n",
        "    1.  **Baseline Analysis (Task 25)**: Runs the full AHP+SAW pipeline to generate risk indices and rankings.\n",
        "    2.  **Robustness Analysis (Task 26)**: Conducts sensitivity analysis across multiple scenarios.\n",
        "    3.  **Validation (Task 27)**: Cross-checks the baseline results against the original study's reported figures.\n",
        "    4.  **Packaging (Task 28)**: Generates a comprehensive technical report and serializes all data artifacts.\n",
        "\n",
        "    Args:\n",
        "        raw_expert_survey_df (pd.DataFrame): The raw expert pairwise comparison data.\n",
        "        raw_financial_statement_df (pd.DataFrame): The raw financial statement data.\n",
        "        study_configuration (Dict[str, Any]): The complete study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A master dictionary containing the artifacts from all phases:\n",
        "            - 'baseline_results': Output from Task 25 (weights, matrices, rankings).\n",
        "            - 'robustness_results': Output from Task 26 (scenario results, stability summary).\n",
        "            - 'validation_results': Output from Task 27 (comparison tables, discrepancy report).\n",
        "            - 'final_package': Output from Task 28 (serialized files, report text).\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Initializing HAFRI Master Pipeline...\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 1: Baseline Analysis (Task 25)\n",
        "    # -------------------------------------------------------------------------\n",
        "    logger.info(\">>> Executing Phase 1: Baseline Analysis (Task 25)\")\n",
        "    baseline_results = run_hafri_pipeline(\n",
        "        raw_expert_survey_df,\n",
        "        raw_financial_statement_df,\n",
        "        study_configuration\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 2: Robustness Analysis (Task 26)\n",
        "    # -------------------------------------------------------------------------\n",
        "    logger.info(\">>> Executing Phase 2: Robustness Analysis (Task 26)\")\n",
        "    scenario_results, robustness_summary = conduct_robustness_analysis(\n",
        "        study_configuration,\n",
        "        raw_expert_survey_df,\n",
        "        raw_financial_statement_df\n",
        "    )\n",
        "\n",
        "    robustness_results = {\n",
        "        \"scenario_details\": scenario_results,\n",
        "        \"stability_summary\": robustness_summary\n",
        "    }\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 3: Validation (Task 27)\n",
        "    # -------------------------------------------------------------------------\n",
        "    logger.info(\">>> Executing Phase 3: Validation against Study (Task 27)\")\n",
        "\n",
        "    # Reconstruct the hierarchy object to pass to the validator\n",
        "    hierarchy = initialize_ahp_hierarchy()\n",
        "\n",
        "    validation_results = validate_results_against_study(\n",
        "        baseline_results[\"global_weights\"],\n",
        "        hierarchy,\n",
        "        baseline_results[\"ranking\"],\n",
        "        baseline_results[\"comparison\"]\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 4: Packaging (Task 28)\n",
        "    # -------------------------------------------------------------------------\n",
        "    logger.info(\">>> Executing Phase 4: Packaging Outputs (Task 28)\")\n",
        "\n",
        "    # Combine relevant results for the report generator\n",
        "    # The report generator expects 'results' (baseline) and 'comparison' (from validation)\n",
        "    # We update baseline_results with the detailed validation info for the report\n",
        "    baseline_results[\"comparison_details\"] = validation_results\n",
        "\n",
        "    final_package = package_project_outputs(\n",
        "        baseline_results,\n",
        "        study_configuration\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Final Assembly\n",
        "    # -------------------------------------------------------------------------\n",
        "    master_artifacts = {\n",
        "        \"baseline_results\": baseline_results,\n",
        "        \"robustness_results\": robustness_results,\n",
        "        \"validation_results\": validation_results,\n",
        "        \"final_package\": final_package\n",
        "    }\n",
        "\n",
        "    logger.info(\"HAFRI Master Pipeline completed successfully.\")\n",
        "    return master_artifacts\n"
      ],
      "metadata": {
        "id": "2Od0J13ETHmn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}